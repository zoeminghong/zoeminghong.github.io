<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[阿里巴巴大数据实践之数据仓库读书笔记]]></title>
    <url>%2F2020%2F02%2F13%2F%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B20200213%2F</url>
    <content type="text"><![CDATA[名词说明业务板块定义数据仓库的名称和业务空间，以企业内一个相对独立的业务为分配单元。例如，如果业务涉及零售、文娱，且系统间相对独立，则需要构建两个业务板块，即零售、文娱。如果业务仅涉及零售，且业务内的系统间隔离较少，则只需要构建一个业务板块，即零售。 公共定义定义企业构建数据所需的全局概念对象或参数，以保证全局概念统一。当定义完成后，系统内其他指标（例如派生指标）可以按需统一、通用化引用这些对象，例如统计周期，年、月、日、每周、每日。 项目管理项目是一种物理空间上的划分。项目管理，即用户在数据中台建设过程中，对物理资源及开发人员进行隔离化管理。一个业务板块可以包含多个项目，每个系统成员可以加入多个不同的项目。 维度维度即进行统计的对象。通常情况下，维度是实际存在、不因事件发生就存在的实体。创建维度，即从顶层规范业务中的实体（主数据），并保证实体的唯一性。例如订单、商品。维度由多个属性组成的一个实体，例如买家维度，可以由买家支付金额、买家姓名、买家手机号之类的属性丰富维度这个实体。对于哪些使用较多却无法归属到相应的维度中的属性，归并到杂项维度中。 维度在事实表中用于描述业务过程所处的环境信息。 维度退化，是将一些维度信息直接存储到事实表中，冗余数据，方便读取数据。 业务过程业务过程即业务活动中的所有事件（它是一个事件集合）。创建业务过程，即从顶层规范业务中事务内容的类型及唯一性。因此业务过程是一个不可拆分的行为事件。例如下单、支付、退款都是业务过程。 淘宝交易订单的流转的业务过程有四个：创建订单、买家付款、卖家发货、买家确认收货。 指标指标分为原子指标和派生指标。原子指标：对指标统计口径（即计算逻辑）、具体算法的一个抽象，是业务定义中不可再拆分的指标，例如支付金额。一般都为数值（统计）。原子指标=业务过程（动作）+度量，如支付（事件）金额（度量）。派生指标：业务中常用的统计指标。派生指标=原子指标+业务限定+统计周期+统计粒度。例如，自然周、会员、采用优惠券支付的订单。 统计粒度统计的最小颗粒度，数据唯一性的保证，统计分析的对象或视角，定义数据需要汇总的程度，可以理解为聚合运算时的分组条件（类似于SQL中group by的对象）。粒度是维度的一个组合，指明您的统计范围。例如，某个指标是某个卖家在某个省份的成交额，则粒度就是卖家、省份这两个维度的组合。 事实表中的一条记录所表达的业务细节程度被称为粒度。通常粒度可以通过两种方式来表述：一种是维度属性组合所表示的细节程度；一种是所表示的具体业务含义。 事实度量业务过程，一般为整型或浮点型的十进制数值。有可加性、半加性和不可加性三种类型。可加性事实是指可以按照与事实表关联的任意维度进行汇总。半可加性事实只能只能按照特定的维度汇总，能对所有维度汇总，比如库存可以按照地点和商品进行汇总，而按时间维度把一年中每个月的库存累加起来则毫无意义。还有一种度量完全不具备可加性，比如比率型事实。对于不可加性事实可分解为可加的事实来实现聚集。 事实可以通过回答“过程的度量是什么”来确定。 事实的设计准则 事实完整性 尽可能多地获取所有的度量。 事务一致性 明确存储每一个事实以确保度量的一致性。 事实可加性 遇到不可加性度量，比如分摊比例、利润率等，需要进行转化为可加性。 不可加性，就是两条记录中的值相加，是没有统计价值的，就可以认为是不可加的。 事实表事实表紧紧围绕业务过程来设计，通过获取描述业务过程的度量来表达业务过程。 事实表有三种类型：事务事实表、周期快照事实表和累计快照事实表。 事务事实表用来描述业务过程，跟踪空间或时间上某个点的度量事件，保存的是最原子的数据。 周期快照事实表以具有规律性的、可预见的时间间隔记录事实，时间间隔如每天、每月、每年等。 累积快照事实表用来表述过程开始和结束之间的关键步骤事件，覆盖过程的整个生命周期，通常具有多个日期字段来记录关键时间点。 事务事实表 周期快照对照事实表 累积快照事实表 时期/时间 离散事务时间点 以有规律的、可预测的间隔产生快照 用于时间跨度不确定的不断变化的工作流 日期维度 事务日期 快照日期 相关业务过程涉及的多个日期 业务过程数 一个或多个 一个？？？ 多个 粒度 每行代表实体的一个事务 每行代表某时间周期的一个实体 每行代表一个实体的生命周期 事实 事务事实 累积事实 相关业务过程事实和时间间隔事实 事实表加载 插入 插入 插入与更新 事实表更新 不更新 不更新 业务过程变更时更新 事实表设计原则 尽可能包含所有与业务过程相关的事实 只选择与业务过程相关的事实 分解不可加性事实为可加的组件 在选择维度和事实之前必须先声明粒度 在同一个事实表中不能有多种不同粒度的事实 事实的单位要保持一致 对事实的 null 值要处理 使用退化维度提高事务表的易用性 事实表设计方法 选择业务过程及确定事实表类型 声明粒度 确定维度 确定事实 冗余维度 声明粒度应该尽量选择最细级别的原子粒度，以确保事实表的应用具有最大的灵活性。 确定了粒度，也就确定了主键。 确定事实尽可能包含所有与业务过程相关的事实。 冗余维度正常建模时对维度的处理应该是单独存放在一张表中，通过主键与事实表进行关联。其目的是为了减少事实表的维度冗余，但有的时候，为了提高效率，进行适当将高频的维度信息冗余，方便下游用户使用。 事务事实表设计过程任何类型的时间都可以被理解为一种事务。 事务事实表用以跟踪定义业务过程的个体行为，提供丰富的分析能力，作为数据仓库原子的明细数据。 选择业务过程 确定粒度 确定维度 确定事实 冗余维度 在确定维度的时候，对于无法归类的维度属性，可以合并到一个杂项维度进行存放。 在事实表中，除了维度信息外的其他属性都是事实。事实一般为整型或浮点型的十进制数值。 事务事实表主键为粒度。 Kimball 维度建模理论认为，为了便于进行独立的分析研究，应该为每个业务过程建立一个事实表。 Q：每一个业务过程是否只能建一个事实表呢？ A：当然不是，一般我们建设事务事实表时，选用最小粒度作为粒度，增强其灵活性。除了事务事实表，还有其他两种类型的事实表。 事务事实表分类事务事实表还分为两种：单事务事实表和多事务事实表。 单事务事实表：针对每一个业务过程设计一个事实表。 多事务事实表：将不同的事实放到同一个事实表中，即同一个事实表包含不同的业务过程。 多事务事实表多事务事实表使用场景一般为多个业务过程拥有相同的粒度。 多事务事实表设计时有两种方法进行事实的处理： 不同业务过程的事实使用不同的事实字段进行存放 不同业务过程的事实使用同一个事实字段进行存放 当两个或多个业务过程事实结构比较相似的时候，选用同一个事实字段进行存放。 不同的事实字段 如果不是当前业务过程的度量，则采取零值处理方式。 同一个事实字段 两种事务事实表如何选择 单事务事实表 多事务事实表 业务过程 一个 多个 粒度 相互间不相关 相同粒度 维度 相互间不相关 一致 事实 只取当前业务过程中的事实 保留多个业务过程中的事实，非当前业务过程中的事实需要置零处理 冗余维度 多个业务过程，则需要冗余多次 不同的业务过程只需冗余一次 理解程度 易于理解，不会混淆 难以理解，需要通过标签来限定 计算存储成本 较多，每个业务过程都需要计算存储一次 较少，不同业务过程融合到一起，降低了存储计算量，但是非当前业务过程的度量存在大量零值。 建议：为了理解性和存储资源相对不紧张的情况下，尽可能选用单事务事实表，但若满足粒度和维度的要求情况下，可以考虑多事务事实表。 周期快照事实表当需要一些 状态度量 时，比如账户余额、买卖家星级、商品库存等，则需要聚集于至相关的事务才能进行识别计算；或者聚集事务无法识别，比如温度等。对于这些状态度量，事务事实表是无效率的，而这些度量也和度量事务本身一样是有用的。 特性事务事实表的粒度能以多种方式表达，但快照事实表的粒度通常以维度形式声明；事务事实表是稀疏的，但快照事实表是稠密的；事务事实表中的事实是完全可加的，但快照模型将至少包含一个用来展示半可加性质的事实。 用快照采样状态快照事实表以预定的间隔采样状态度量。这种间隔联合一个或多个维度，将被用来定义快照事实表的粒度。 快照粒度快照事实表的粒度通常总是被多维声明，可以简单地理解为快照需要采样的周期以及什么将被采样。 密度与稀疏性快照事实表是稠密的，无论当天是否有业务过程发生，都会记录一行。 半可加性在快照事实表中收集到的状态都是半可加的。半可加性事实不能根据时间维度获得有意义的汇总结果。 快照事实表分类设计步骤 确定粒度 确定状态度量 单维度的每天快照事实表不同的采样粒度确定了不同的快照事实表。 混合维度的每天快照事实表混合维度相对于单维度，只是在每天的采样周期上针对多个维度进行采样。 以上两类快照事实表都有一个特点：都可以从事务事实表中进行汇总产出。除此之外，还有一种产出模式，即直接使用操作型系统的数据作为周期快照事实表的数据加工，比如淘宝卖家星级。 全量快照事实表全量快照事实表会增加冗余维度的操作。 快照事实表与事务事实表往往都是成对设计的，相互补充，一瞒住更多的下游统计分析需求。 累计快照事实表比如统计卖家下单到支付的时长、买家支付到卖家发货的时长等。对于类似于研究事件之间事件间隔的需求。 设计过程 选择业务过程 确定粒度 确定维度 确定事实 退化维度 业务过程存在横跨多个业务过程。 确定粒度一个完整的流程，累计快照事实表用于考察实体的唯一实例，所以只记录一行。 确定维度与事务事实表相同。多个维度进行描述。 确定事实需要将各业务过程对应的事实均放入事实表中。累计事实表解决掉的最重要的问题是统计不同业务过程之间的时间间隔，建议将每个过程的时间间隔作为事实放在事实表中。 特点数据不断更新事务事实表记录事务发生时的状态，对于实体的某一实例不再更新，而累积快照事实表则对实体的某一实例定期更新。 多业务过程日期累积快照事实表适用于具有较明确起始时间的段生命周期的实体，比如交易订单、物流订单等。 聚集型事实表又名为汇总表。聚集主要是通过汇总明细粒度数据来获得改进查询性能的效果。 基本原则一致性。聚集表必须提供与查询明细粒度数据一致的查询结果。最简单方法是确保聚集星型模型中的维度和度量与原始模型中的维度和度量保持一致。 避免单一表设计。不要在同一个表中存储不同层次的聚集数据。 聚集粒度可不同。聚集并不需要保持与原始米线粒度数据一样的粒度，聚集只关心锁需要查询的维度。 基本步骤 确定聚集维度 确定一致性上钻 确定聚集事实 聚集补充说明聚集不能跨越事实的，聚集的维度和度量必须与原始模型保持一致。 聚集会带来查询性能的提升，但聚集也会增加 ETL 维护的难度。当子类目对应的以及类目发生变更是，先前存在的、已经被汇总到聚集表中的数据需要被重新调整。 讨论Q：是否所有的业务流程都需要进行统计？ A: 不是，对于有些业务流程特别长的，需要考虑进行精简，对关键节点进行统计就行了。 Q: 汇总表与快照事实表的区别是什么？ A: 汇总表是基于事实表数据进行汇总，是纵向的，是基于已有的数据进行类似于 SUM 操作。而快照事实表更多是横向的，跨越多个事务事实表，进行横向的合并。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据治理--元数据]]></title>
    <url>%2F2020%2F02%2F08%2F%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E5%85%83%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[元数据是对某个潜在信息性对象做出的陈述。在浏览其他网页的时候会看到元数据被称之为 “数据的数据”。为了更好的描述元数据到底是什么东西，我以一本《Metadata》书作为例子进行说明。《Metadata》第二页记录着该书的 CIP 信息、作者、出版社、书号、定价、印次、字数等信息，而这些信息都是用于描述《Metadata》这本书的元数据。 一条元数据记录就是关于一个资源的主谓宾陈述集合。例如：达芬奇（宾语）是蒙娜丽莎（主语）的创作者（谓语）。 主数据、元数据与参考数据 元数据元数据（meta-data）是描述企业数据的相关数据，指在IT系统建设过程中所产生的有关数据定义，目标定义，转换规则等相关的关键数据，包括对数据的业务、结构、定义、存储、安全等各方面对数据的描述。地市表这个实体的数据模型如何进行定义正是元数据所关心的范畴。 元数据可以说是企业的数据地图，它直接反映了企业中有什么样的数据，数据是如何存放的，例如，数据结构是什么样子，数据与业务之间的关系是怎么样，数据与数据之间的关系是怎么样，数据有什么样的安全需求，数据有什么样的存储需求。 主数据主数据（main data）主要是指经实例化的企业关键数据。 我们在上面设计完成数据模型设计的“城市表”中填写了相应的城市数据，例如，北京、上海、广州、南宁等等。这些在城市表中填充的数据，正是组织中国地理协会的主数据，因为这些数据是中国地理协会这个组织的关键业务实体，它为组织的业务开展提供关联环境，而且它可能在企业业务开展过程中被反复引用。针对这些核心关键数据，组织和企业无论从数据的质量、一致性、可用性、管理规范等方面都应该有着最严格的数据要求。 主数据就是企业被不同运营场合反复引用关键的状态数据，它需要在企业范围内保持高度一致。它可以随着企业的经营活动而改变，例如，客户的增加，组织架构的调整，产品下线等；但是，主数据的变化频率应该是较低的。所以，企业运营过程产生过程数据，如生产过程产生各种如订购记录、消费记录等，一般不会纳入主数据的范围。当然，在不同行业，不同企业对主数据有不同的看法和做法，正如我们与国内大型航空企业的实施相关数据项目时，也在为航班动态是不是主数据而纠结不已。 个人觉得主数据具有跨行业也能存在能力，同时实例数据变更是低频的。 例如商品价格信息、会员信息是主数据，而订单信息不是主数据，它具有高频更新存在。因而怎么算高频就是一个仁者见仁智者见智的情况了，需要结合自己所在的行业进行判断。 参考数据——数据的字典在本文引用的假设案例中，我们将会注意到刚才填写的地市这类数据有些列，如省份、城市类型等。如果没有缺少上下文的环境，我们是无法理解其具体含义，这时候我们往往引入参考数据（reference data）加以解释和理解。 参考数据是增加数据可读性、可维护性以及后续应用的重要数据。例如，你看到“性别”的这个字段，很可能是1代表男性、2代表女性。在许多企业中有这样的约定俗成，而更多的参考数据可能记录在开发人员和运营人员的大脑当中。但问题是一旦这些人离开，您系统里面的数据就成了一堆没有注释的天书。 大家可能觉得，这所谓参考数据不就是数据字典吗？对，我们在很多系统里面都会有这样和那样的数据字典。但是正是由于这些数据字典仅局限于个别系统而没有统一标准，从一个侧面间接造就了大量的数据孤岛。企业为了进行更有效率的数据整合、数据共享和数据分析应用，开始尝试对参考数据进行企业或者部门层面的整合和管理，利用参考数据集记录系统尝试为范围内的IT系统中的数据库提供统一的参考数据。 小结主数据则是真实的企业业务数据，是企业的关键业务数据。 参考数据则是对数据的解释，针对一些数据范围和取值的数据解释，让人们容易读取相关的数据。 元数据是对数据的描述，用于描述企业数据的所有信息和数据，如结构、关系、安全需求等，除增加数据可读性外，也是后续数据管理的基础。 一般而言，企业中这三类数据与其它数据的数据量、质量需求，更新频率、数据生命周期的关系大致如下图： 中台与元数据中台强调的是复用性，利用提高服务的重复使用，降低开发时间和提高工作效率。但现实生活中，大量的数据孤岛和重复建设存在，从而存在大量的数据指标定义不同带来的歧义问题。由于不同的项目 PM 管理风格迥异，存在有些必要信息没有进行存档，而是通过口口相传的不靠谱形式，信息准确性大打折扣。 数据中台建设中需要寻找共性数据，通过数据建模，将其进行抽取形成一张公共的数据大宽表。如何快速了解掌握数据关系，定位表与表之间的共性部分，往往需要对业务有一定的了解，但现实情况大量的开发人员只了解自己负责的那一个模块内容，对于其他人或者项目的内容一无所知。 而元数据就是为了消除二义性，同时，对元数据的管理，形成结构化的权威数据，降低数据传递成本和提高准确性。 常见元数据分类 元数据包括业务元数据、技术元数据和管理元数据。 常见的业务元数据包括：业务定义、业务术语、业务规则、业务指标等。 常见的技术元数据包括：存储位置、数据模型、数据库表、字段长度、字段类型、ETL脚本、SQL脚本、接口程序、数据关系等。 常见的管理元数据包括：数据所有者、数据质量定责、数据安全等级等。 业务问题在没有元数据管理的情况下，从使用问题、管理问题、数据问题等三个方面进行说明： 使用问题 查看表结构信息不方便（查看、检索、表复用） 表依赖关系不清楚（血缘） 表信息缺少（表维护者，表状态） 管理问题 表权限管理 数据管理 数据质量监控 数据问题 建表规则混乱，没有统一标准 表结构变更之后，信息不同步 如何设计元数据管理 数据治理数仓构建名词说明业务板块定义数据仓库的名称和业务空间，以企业内一个相对独立的业务为分配单元。例如，如果业务涉及零售、文娱，且系统间相对独立，则需要构建两个业务板块，即零售、文娱。如果业务仅涉及零售，且业务内的系统间隔离较少，则只需要构建一个业务板块，即零售。 公共定义定义企业构建数据所需的全局概念对象或参数，以保证全局概念统一。当定义完成后，系统内其他指标（例如派生指标）可以按需统一、通用化引用这些对象，例如统计周期，年、月、日、每周、每日。 项目管理项目是一种物理空间上的划分。项目管理，即用户在数据中台建设过程中，对物理资源及开发人员进行隔离化管理。一个业务板块可以包含多个项目，每个系统成员可以加入多个不同的项目。 维度维度即进行统计的对象。通常情况下，维度是实际存在、不因事件发生就存在的实体。创建维度，即从顶层规范业务中的实体（主数据），并保证实体的唯一性。例如订单、商品。 业务过程业务过程即业务活动中的所有事件（它是一个事件集合）。创建业务过程，即从顶层规范业务中事务内容的类型及唯一性。因此业务过程是一个不可拆分的行为事件。例如下单、支付、退款都是业务过程。 指标指标分为原子指标和派生指标。原子指标：对指标统计口径（即计算逻辑）、具体算法的一个抽象，是业务定义中不可再拆分的指标，例如支付金额。一般都为数值（统计）。原子指标=业务过程（动作）+度量，如支付（事件）金额（度量）。派生指标：业务中常用的统计指标。派生指标=原子指标+业务限定+统计周期+统计粒度。例如，自然周、会员、采用优惠券支付的订单。 统计粒度统计的最小颗粒度，数据唯一性的保证，统计分析的对象或视角，定义数据需要汇总的程度，可以理解为聚合运算时的分组条件（类似于SQL中group by的对象）。粒度是维度的一个组合，指明您的统计范围。例如，某个指标是某个卖家在某个省份的成交额，则粒度就是卖家、省份这两个维度的组合。 这里需要注意粒度与维度的关系，通过一个或多个维度构建粒度。 流程 维度构建 DIM 维度模型。 数据域与项目是相互独立的不同统计指标，但同属于同一个业务板块下面。 以源表和维表构建得到事实明细表。 以原子指标、业务限定、统计周期、统计粒度构建得到派生指标，结合统计粒度构建 DWS 汇总表 示例 拓展阅读 Apache Atlas快速入门 元数据管理解析以及数据仓库和主数据介绍 Dataphin使用说明 数据治理平台工具前世今生 企业数据资产管理体系初探 一组图详解元数据、主数据与参考数据]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈 Spark 应用日志级别的坑]]></title>
    <url>%2F2019%2F06%2F24%2F%E8%B0%88%E8%B0%88%20Spark%20%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[环境说明：HDP 3.0 + Kerberos + Livy 根据 Spark 官方文档的指引，清楚的知道存在三种方式可以对应用的日志级别进行调整。 upload a custom log4j.properties using spark-submit, by adding it to the --files list of files to be uploaded with the application. add -Dlog4j.configuration=&lt;location of configuration file&gt; to spark.driver.extraJavaOptions (for the driver) or spark.executor.extraJavaOptions (for executors). Note that if using a file, the file: protocol should be explicitly provided, and the file needs to exist locally on all the nodes. update the $SPARK_CONF_DIR/log4j.properties file and it will be automatically uploaded along with the other configurations. Note that other 2 options has higher priority than this option if multiple options are specified. 第三种方案是我们最不希望看到的选择，因而选择了第一、二两种进行尝试。 第一种方案过程： 将编辑准备好的 log4j-error.properties 文件上传到 HDFS 授予接下来启动 Spark 应用的用户读取权限 启动参数中，添加 &quot;files&quot;:[&quot;/logfile/log4j-error.properties&quot;] 参数，来指定 log4j.properties 文件路径。 启动参数中，添加 &quot;spark.driver.extraJavaOptions&quot;: &quot;-Dlog4j.configuration=log4j-error.properties&quot;,&quot;spark.executor.extraJavaOptions&quot;: &quot;-Dlog4j.configuration=log4j-error.properties&quot; ，相对路径即可 使用 Livy 进行启动应用 这里的可能遇到的问题： -files 参数是数组，不是简单的 String spark.driver.extraJavaOptions 用于 Driver 的日志级别文件的指定，&quot;spark.executor.extraJavaOptions 用于 Executor 的日志级别文件的指定。可以单独分别指定，支持相对路径。 HDFS 下文件的权限一定要注意 第二种方案过程： 将编辑准备好的 log4j-error.properties 文件上传到 Spark Server 所在服务器的 $SPARK_CONF_DIR/ 目录下 授予 log4j-error.properties 文件读取权限，粗暴一点直接设置为 777 启动参数中，添加 &quot;spark.driver.extraJavaOptions&quot;: &quot;-Dlog4j.configuration=log4j-error.properties&quot;,&quot;spark.executor.extraJavaOptions&quot;: &quot;-Dlog4j.configuration=log4j-error.properties&quot; ，如果相对路径不生效，可以使用绝对路径( file:/spark/conf/log4j-error.properties ) 使用 Livy 进行启动应用 这里的可能遇到的问题： 读取的权限，一定要启动应用的用户拥有权限 所有 Spark 节点下都要有日志文件哦 第三种方案过程： 在 HDP Ambari页面 Spark 下，对日志配置进行相应的修改 重启 Spark 服务使其生效 这里的可能遇到的问题： 优先级该方案是三者中最低的，前两种都可以覆盖该种方案 谈谈 log4j 配置自定义某路径下的日志级别比如：希望 com.zerostech.demo 路径下日志级别为 ERROR 1log4j.logger.com.zerostech.demo=ERROR]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优秀的数据同步方案如何设计]]></title>
    <url>%2F2019%2F06%2F10%2F%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[应用开发中，为了提升查询性能或者做服务降级方案时，我们会使用缓存作为解决方案，像分布式缓存方案，比如 Redis、Memcache等；本地缓存方案，比如 Guava、Caffeine等。如果仅仅对当前服务的执行结果的缓存，用于下次相同查询时加快查询效率来说，还相对简单一点。只需要将查询条件作为key，返回的结果作为 value 即可实现，复杂一点会加上缓存失效机制等。 但还有一种可能缓存，可能是需要进行数据的同步的操作的。比如笔者之前做过的用户权限中心，由于对响应实时性方面有很大的要求，虽然使用了异步非阻塞编程方式以提高性能，但如果涉及到数据库的操作，其实性能并不能达到目标值。由于权限的相关配置项通过字节估算，对资源消耗并不算大，因而，笔者考虑使用本地缓存方案实现。 同步方案做数据同步需要考虑同步方案和数据格式。同步方案常见有主动同步（启动初始化、定时任务）和被动同步（消息通知、回调）两种模式。应用一般会在启动的时候初始化一份基准数据，之后的数据更新都基于这份基准数据进行修改。对数据实时性要求不高的场景，可以通过定时任务方式主动拉取数据，在这种方式中存在全量和增量两种模式。全量是最简单的方案，只需要对原先的缓存进行清空操作，填充最新的数据即可，适合数据量比较小的场景。增量方式相对来说比较复杂，需要依照不同的更新维度做相应的修改。还是拿权限例子来说，一般存在Tenant、AppId、 User、Role、Group、Resource等内容，这里存在层级关系，{User、Role、Group、Resource} 存在于 AppId 下，AppId 又同时存在于 Tenant 中，其中广义上来说 API、Tag、Menu 都是属于 Resource范畴，具体设计这里不进行展开，那么缓存格式可以是这样的： 1Tenant -&gt; Appid -&gt; Method -&gt; Path -&gt; UserId -&gt; RoleId 在用户登录的时候，会携带 tenant、appid、user、role 等信息，同时，当前请求的 Method 和 Path 也是可以知晓的。假设用户在配置请求路径 Path 的时候配置错误了，现在需要在后台进行修改，修改之后就会进行数据的同步，我们先不关心用哪种方式触发同步，我们去修改缓存的时候，需要从左到右一层层进行判断，进行修改，这样还不是最麻烦的，麻烦的是上面的每一层级都是一个可以变化的单元，都可能存在新增、修改和删除的情况，是不是想想就会觉得头大了呢。那么有哪些解决方案可以供参考： 全量同步，简单粗暴且高效，但不适合数据量大且获取更新数据比较复杂麻烦的场景 拆分多个缓存，例如 Tenant -&gt; Appid -&gt; Method -&gt; Path -&gt; RoleId，Tenant -&gt; Appid -&gt; Method -&gt; Path -&gt; UserId（这里只是举例说明，实际并非如此） 简化操作，一般缓存都是存在增删改的操作，这三者中改操作往往是最复杂的一种，如果只有增删会简单很多 再回过来讲一下消息通知的同步方式，消息通知存在 RabbitMQ、RocketMQ、Kafka 等消息中间件解决方案。在一致性方面要求高的场景，可以使用 RabbitMQ 和 RocketMQ，能确保数据量比较大的场景推荐使用 Kafka 方案，毕竟 Kafka 是为大数据而生的。使用消息通知的方式就需要引用消息中间，相对 API 方式来说比较笨重且引入了一个不稳定因素，对于小项目来说得不偿失，同时，如果是公司外部应用，不会提供消息中间件作为数据同步方案。 接着说说回调，这种方式被广泛用于对外业务中，HTTP 或者 HTTPS 方式比较轻量级、接受度高，当然回调这种概念不局限于通讯协议方式，RPC 方式也是可以的。回调方式与消息通知方式进行对比的话，回调需要自行实现幂等和重试机制，在编码方面需要投入更多，这也是大家为什么异步的场景青睐消息队列的原因。 数据格式数据格式需要结合同步方案和业务要求。如果是增量的方式，需要考虑修改前与修改之后，比如这样： 1234567891011121314151617181920[ &#123; &quot;id&quot;:&quot;UUID&quot;, &quot;op&quot;:&quot;U&quot;, // 操作，U、D &quot;t&quot;:1590730661263, // 时间戳 &quot;prev&quot;:&#123; &quot;id&quot;:&quot;XXX&quot;, // 更新前ID &quot;name&quot;:&quot;zhansan&quot;, // 更新前名称 &quot;time&quot;:&quot;1590730661124&quot; // 更新前更新时间 &#125;, &quot;cur&quot;:&#123; &quot;id&quot;:&quot;XXX&quot;, // 更新后ID &quot;name&quot;:&quot;lisi&quot;, // 更新后名称 &quot;time&quot;:&quot;1590730661263&quot; // 更新后时间 &#125; &#125;, &#123; ...... &#125;] 全量方式则不需要这么复杂，只要最新结果集即可。同步方案的不同也会存在字段的考量，一般会从幂等性、数据一致性、服务稳定性、可用性、实时性等方面出发。一般我建议： 字段尽可能短 必须有id和时间戳信息 Type 类型字段值，尽可能使用 Int 类型或者短字符串映射，例如上面的op字段使用短字符串方式 一些建议正向不行，可以试试反向。在设计缓存结构时候，由于人的大脑擅长正向思维，可能设计的结果并不特别的理想（在查询和更新性能方面），这个时候可以考虑反向试试，可能会豁然开朗。Tenant -&gt; Appid -&gt; Method -&gt; Path -&gt; UserId 数据格式，在某些场景不如 UserId -&gt; Method -&gt; Path -&gt; Appid -&gt; Tenant 。 稳定节点在前，多变的在后。数据量少在前，数据量多在后。 上面的例子中，Tenant 相对比较稳定，变更的比较少且数据量相对于 UserId 肯定比较少。这样在修改或者查找的时候，性能相对好。 空间与时间互换。 这个想必大家经常听到，时间换空间或者空间换时间。对于性能有要求的业务场景，通过冗余缓存方案可以提高查询性能；在资源紧张的场景但对时间有包容性，那适当在实时性方面进行取舍。 不要忽视数据提供方的性能问题。 实时性不仅仅依赖于需要数据的那方或者中间件，数据提供方也是可能存在性能瓶颈的。如果数据的数据格式要求特别变态，需要数据提供方联表查询 3 张表以上，性能可想而知，所以同步的数据要进行取舍，从而节省网络带宽和IO，提升性能。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase Thrift with Python]]></title>
    <url>%2F2019%2F05%2F19%2Fhbase_thrift%2F</url>
    <content type="text"><![CDATA[说在文前 本文内容是基于 Centos 7、HDP 3.0.0、HBase 2.0.0、Python 2.7 环境下，其他环境的童鞋选择性进行参考。 Thrift 安装在 HBase 服务节点上即可。 HBase 实现了两套Thrift Server服务，有两种 Thrift IDL 文件，提供了两套数据结构: 第一套有 TCell， ColumnDescriptor，TRegionInfo 等，它的 API 比较全，它不仅有读写 API，同时也有创建删除等 API； 第二套有 TTimeRange, TColumn, TColumnValue 等，它更加接近 HBase Java API 的调用方式，但是它的 API 比较少，只有读写表的API），它们最后都是通过 HBase Client 的 Java API 来完成操作。 安装 Thrift安装依赖包1yum install -y automake libtool flex bison pkgconfig gcc-c++ libevent-devel zlib-devel python-devel ruby-devel openssl-devel 安装 boost (CentOS 7 必做)12345wget https://dl.bintray.com/boostorg/release/1.64.0/source/boost_1_64_0.tar.gztar zxvf boost_1_64_0.tar.gzcd boost_1_64_0./bootstrap.sh./b2 install 下载 Thrift12345678wget https://archive.apache.org/dist/thrift/0.10.0/thrift-0.10.0.tar.gztar zxvf thrift-0.10.0.tar.gzcd thrift-0.10.0/./configure 或者 ./configure --with-boost=/usr/local --without-java --without-phpmakemake install# 进行确认安装成功thrift -help 0.10.0 版本之前的不支持 python 3.5 生成 hbase.thrift HDP 下 HBase 相应的安装目录下本身就已经存在 hbase.thrift 文件了，所以我们不需要自行创建了。 生成指定语言的代码 12345# hdp hbase.thrift 文件路径cd /usr/hdp/3.0.0.0-1634/hbase/include/thrift/# 生成 python# 该路径下存在 thrift1 和 thrift2 两种，可以自行选择thrift -gen py hbase1.thrift 或 thrift -gen py hbase2.thrift 执行完该命令之后，会生成一个 gen-py 目录，将该目录下 hbase 文件下载到本地项目中。(hbase-1 请忽略) 启动 Thrift 服务12cd /usr/hdp/3.0.0.0-1634/hbase/bin/./hbase-daemon.sh start thrift -p 9090 --infoport 8086 日志路径为 /var/log/hbase/ 使用 Thrift 2 模式 12./hbase-daemon.sh start thrift2 -p 9090 --infoport 8086./hbase-daemon.sh stop thrift Python 方式连接Thrift_1 模式123456789101112from thrift.transport.TSocket import TSocketfrom thrift.transport.TTransport import TBufferedTransportfrom thrift.protocol import TBinaryProtocolfrom hbase import Hbaseif __name__ == '__main__': transport = TBufferedTransport(TSocket('10.200.168.18', 9090)) transport.open() protocol = TBinaryProtocol.TBinaryProtocol(transport) client = Hbase.Client(protocol) client.get() print(client.getTableNames()) hbase 源于 上文中 hbase 目录文件包 Thrift_2 模式123456789101112131415161718from thrift.transport.TSocket import TSocketfrom thrift.transport.TTransport import TBufferedTransportfrom thrift.protocol import TBinaryProtocolfrom hbase import THBaseServicefrom hbase.ttypes import TGetimport loggingif __name__ == '__main__': logging.basicConfig(level=logging.DEBUG) transport = TBufferedTransport(TSocket('10.200.168.18', 9090)) transport.open() protocol = TBinaryProtocol.TBinaryProtocol(transport) client = THBaseService.Client(protocol) tget = TGet(row = '321ahah') tresult = client.get('shop', tget) for col in tresult.columnValues: print(col.qualifier, '=', col.value) print(client.send_get()) transport.close() Kerberos On Thrift服务配置core-site.xml 12hadoop.proxyuser.hbase.groups=*hadoop.proxyuser.hbase.hosts=* hbase-site.xml 1234567hbase.thrift.security.qop=authhbase.thrift.support.proxyuser=truehbase.regionserver.thrift.http=false # 使用http方式设为 true，binary 方式设为 falsehbase.thrift.keytab.file=/etc/security/keytabs/hbase.service.keytab hbase.thrift.kerberos.principal=hbase/_HOST@DEVDIP.COM hbase.security.authentication.spnego.kerberos.keytab=/etc/security/keytabs/spnego.service.keytab hbase.security.authentication.spnego.kerberos.principal=HTTP/_HOST@DEVDIP.COM 重启 HDFS 和 HBase 重启 Thrift 服务12345678# 停止./hbase-daemon.sh stop thrift# 启动kinit -kt /etc/security/keytabs/hbase.headless.keytab hbase-dev_dmp &amp; /usr/hdp/3.0.0.0-1634/hbase/bin/hbase-daemon.sh start thrift -p 9090 --infoport 8086# To test the thrift server in http mode the syntax is:hbase org.apache.hadoop.hbase.thrift.HttpDoAsClient DEVDIP.ORG 9090 hbase true# to test in binary mode the syntax is:hbase org.apache.hadoop.hbase.thrift.DemoClient DEVDIP.ORG 9090 true 示例 /var/log/hbase 参考文章 Python 方式连接Thrift_1 模式12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env pythonfrom thrift.transport import TSocketfrom thrift.protocol import TBinaryProtocolfrom thrift.transport import TTransportfrom hbase import Hbase# Apache HBase Thrift server coordinates (network location)thriftServer = "dev-dmp5.fengdai.org"thriftPort = 9090# The service name is the "primary" component of the Kerberos principal the# Thrift server uses.# See: http://web.mit.edu/kerberos/krb5-1.5/krb5-1.5.4/doc/krb5-user/What-is-a-Kerberos-Principal_003f.html# e.g. For a server principal of 'hbase/localhost@EXAMPLE.COM', the primary is "hbase"saslServiceName = "hbase"# HBase table and data informationtableName = 'demo_table'row = 'test2'colName = "cf:name"if __name__ == '__main__': # Open a socket to the server sock = TSocket.TSocket(thriftServer, thriftPort) # Set up a SASL transport. transport = TTransport.TSaslClientTransport(sock, thriftServer, saslServiceName) transport.open() # Use the Binary protocol (must match your Thrift server's expected protocol) protocol = TBinaryProtocol.TBinaryProtocol(transport) client = Hbase.Client(protocol) # Pass the above to the generated HBase clietn # Fetch a row from HBase print "Row=&gt;%s" % (client.getRow(tableName, row, &#123;&#125;)) # Cleanup transport.close() Thrift_2 模式12345678910111213141516171819202122232425262728293031#!/usr/bin/env pythonfrom thrift.protocol import TBinaryProtocolfrom thrift.transport import TSocketfrom hbase import THBaseServicefrom hbase.ttypes import *# Apache HBase Thrift server coordinates (network location)thriftServer = "dev-dmp5.fengdai.org"thriftPort = 9090# The service name is the "primary" component of the Kerberos principal the# Thrift server uses.# See: http://web.mit.edu/kerberos/krb5-1.5/krb5-1.5.4/doc/krb5-user/What-is-a-Kerberos-Principal_003f.html# e.g. For a server principal of 'hbase/localhost@EXAMPLE.COM', the primary is "hbase"saslServiceName = 'hbase'# HBase table and data informationtableName = 'DMP:demo'row = 'test2'coulumnValue1 = TColumnValue('cf', 'name')coulumnValues = [coulumnValue1]if __name__ == '__main__': socket = TSocket.TSocket(thriftServer, thriftPort) transport = TTransport.TSaslClientTransport(socket,host=thriftServer,service=saslServiceName,mechanism='GSSAPI') protocol = TBinaryProtocol.TBinaryProtocol(transport) client = THBaseService.Client(protocol) transport.open() #get get = TGet(row=row, columns=coulumnValues) result = client.get(tableName,get) print result 问题1、找不到libboost_unit_test_framework.a 使用源码本地编译 boos t安装；由于默认认为是 32 位，在 /usr/lib64/libboost_unit_test_framework.a 下是找不到的。可以通过 find libboost_unit_test_framework.a 定位文件真实路径，进行创建软连接。 12find / -name libboost_unit_test_framework.aln -s /usr/local/lib/libboost_unit_test_framework.a /usr/lib64/libboost_unit_test_framework.a 2、kerberos.GSSError: ((‘ Miscellaneous failure (see text)’, 851968), (‘Error from KDC: UNKNOWN_SERVER’, -1765328377)) 123# 日志信息2019-05-08 20:36:10,529 WARN [qtp176041373-47] http.HttpParser: Illegal character 0x1 in state=START for buffer HeapByteBuffer@28800d32[p=1,l=11,c=8192,r=10]=&#123;\x01&lt;&lt;&lt;\x00\x00\x00\x06GSSAPI&gt;&gt;&gt;\x02\x00\x00\x03P`\x82\x03L\x06\t*\x86H\x86\xF7\x12...\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00&#125;2019-05-08 20:36:10,530 WARN [qtp176041373-47] http.HttpParser: bad HTTP parsed: 400 Illegal character 0x1 for HttpChannelOverHttp@24c343ea&#123;r=0,c=false,a=IDLE,uri=null&#125; 1234thriftServer = &quot;10.200.168.7&quot;改为thriftServer = &quot;dev-dmp5.fengdai.org&quot;应该跟hbase/localhost@EXAMPLE.COM =&gt; hbase/dev-dmp5.fengdai.org@DEVDIP.ORG 3、thrift.transport.TTransport.TTransportException: TSocket read 0 bytes 访问其实已经是通了，一直以为是客户端的问题，由于代码是通过 binary 方式访问，hbase.regionserver.thrift.http=false 应该设置为 false。 4、out 日志中查看到错误信息是认证的问题？ hbase.thrift.kerberos.principal 肯定是配置错误了 高级拓展启动方式选择Thrift服务启动有两种方式： 每个节点上启动thrift服务 1./bin/hbase-daemon.sh start thrift 仅在Master上启动线程池服务 1./bin/hbase thrift start -threadpool 由于系统Hbase集群节点数很多，第二种方式更简单些 优化配置项Hbase-site.xml 12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;hbase.regionserver.handler.count&lt;/name&gt; &lt;value&gt;400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.thrift.minWorkerThreads&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.thrift.maxWorkerThreads&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.thrift.server.socket.read.timeout&lt;/name&gt; &lt;value&gt;6000000&lt;/value&gt; &lt;description&gt;eg:milisecond&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.thrift.maxreadlength&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;0:not check data length&lt;/description&gt; &lt;/property&gt; 相关文章 HBase ThriftServer Kerberos认证 Connecting HBase with Python Application using Thrift Server Python Access Secured Hadoop Cluster Through Thrift API - 程序园 使用 Python 和 Thrift 连接 HBase | 张吉的博客 hbase-thrift-server-in-a-kerberised.html Start the HBase Thrift and REST Servers - Hortonworks Data Platform Python HBase Kerberos Example 我的博客即将同步至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=u3ne6vwvzcx2]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次 Git 管理经历]]></title>
    <url>%2F2019%2F04%2F01%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%20Git%20%E7%AE%A1%E7%90%86%E7%BB%8F%E5%8E%8620190401%2F</url>
    <content type="text"><![CDATA[随着负责的项目越来越大，出现了专人维护一个模块的可能，业务与模块划分变得清晰可见，但出现了如下几个问题： Source 变的很重 模块版本管理变得不灵活 开发人员被迫接受很多不要关注的代码 以上问题，促使我寻找一种方案解决这个问题。先简单介绍一下我们的项目构成，由 DMP、DCP、DOP 三个主要业务模块构成，DCP 与其他两个模块之间不存在任何直接关系，DOP 依赖了 DMP 提供的相应基础服务包，DMP 也相对独立，三个模块存在各自发版计划，基于现状，会采用统一的版本号进行管理，这显然是不科学的，所以我提出了使用 GIt Submodule 来解决这个问题。 123456789101112|-docs|-apps|--dop|--ext|--toolkit|-sources|--basics|--components|--templates|--plugins|--terminals|-examples DMP、DCP、DOP 三个业务模块分别创建一个 Git 进行维护。同时，例如：Basic、Env 、Template 等公共模块都作为子模块进行管理。 DMP 1234567891011|-docs|-apps|--dop(与DOP共用子模块)|--ext|--toolkit|-sources|--basics(与DOP共用子模块)|--components|--templates(与DOP共用子模块)|--terminals|-examples(子模块，权限等级可以比较低，以供他人学习查看) DCP 123456|-docs|--toolkit|-sources|--components|--plugins|--terminals DOP 1234567|-docs|-apps(与DOP共用子模块)|-sources|--basics(与DMP共用子模块)|--components|--templates(与DMP共用子模块)|--terminals 看似好像模块变多了，但各个业务变得清晰，版本可控，公共部分进行 Git Submodule ，使开发者只要关注需要关注的就行了，模块之间的权限也变的灵活。 Git Submodule 使用 基于已有项目进行改造 首先设计需要被 Submodule 的模块，并相应的 Git Repository 创建。 将各个子模块的最新数据进行 git push 到相应的远程仓库中，这样子模块的代码已经被管理起来了 执行 git submodule add ，需要将相应的子模块目录删除才能执行 拓展初始化 submodule 项目1git clone --recursive [远程仓库地址] 或者 123git clone [远程父仓库地址]git submodule update --init --recursivegit submodule foreach 'git checkout master; git pull' 变更 submodule 项目路径12345Update .gitmodulesgit mv oldpath newpathgit rm oldpathgit add newpathgit submodule sync 如何优雅的修改项目版本号参照 submodule 其他命令参照参照]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 2.0 协处理器实现 ES 数据同步]]></title>
    <url>%2F2019%2F01%2F30%2FHBase_2.0_%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[在正式进行讲述实现之前，我觉得有必要说一下出发点。团队期初数据都是基于 HBase+Phoenix 这样架构进行持久化。随着业务的复杂性增加，对部分表的查询效率和查询条件多样性，提出了更高的要求。HBase+Phoenix 就会出现索引滥用。变更索引变的特别的频繁，同时一些数据客观的表，变更索引的代价是非常大的。 在海量数据的查询方面，Elasticsearch 具有出色的性能。如果 HBase+ES 是不是会是更好的解决方法呢？其实，这个时候会有一个思考点，Phoenix 是如何实现二级索引的？HBase 协处理器（Coprocessor） 。 我的实现过程比较曲折，后文中也会提到，以帮助大家避免这些坑。在过程中，还尝试了另一种实现方案。存放两份数据，一份 HBase，一份 ES。该方案需要解决的一个问题——数据一致性问题，但这个问题协处理器可以解决。在此过程中，由于不当操作，把 HBase 服务宕机了，现象是 REGION SERVERS 无法启动，只有通过硬删的方式解决。 出于不死心，在经历重装 HBase 之后。内心又开始蠢蠢欲动。首先要声明一下，我们团队的环境是 HDP 3.0、HBase 2.0 ，网上很多教程都是基于 1.X，2.X 与 1.X 区别还是挺大的。RegionObserver 从继承方式改为了面向接口编程。 协处理器没有选择协处理情况下，HBase 实现 RDBMS SQL 方式查询数据，大量的 Filter 需要在客户端进行编码完成，代码的臃肿，可维护性大大降低。如果这部分操作在服务器端完成，是否是更好的选择呢。协处理就能帮助实现该设想，由于在服务端完成，可以集中式优化查询，降低请求的带宽和提高查询效率。当然，对 HBase 性能产生了一定影响。 类型 Observer Endpoint ObserverObserver 协处理器类似于 RDBMS 中的触发器，当事件触发的时候该类协处理器会被 Server 端调用。 EndpointEndpoint 协处理器类似传统数据库中的存储过程，完成一些聚合操作。 实现基础尝试避免 ES 连接操作、代码复杂性导致的 Bug，在最初只通过打日志的方式来验证协处理方式。 代码实现概览HbaseDataSyncEsObserver.java 123456789101112131415161718192021222324252627282930313233343536373839package com.tairanchina.csp.dmp.examples;import org.apache.hadoop.hbase.CoprocessorEnvironment;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.Durability;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.coprocessor.RegionObserver;import org.apache.hadoop.hbase.wal.WALEdit;import org.apache.log4j.Logger;import java.io.IOException;import java.util.Optional;public class HbaseDataSyncEsObserver implements RegionObserver, RegionCoprocessor &#123; private static final Logger LOG = Logger.getLogger(HbaseDataSyncEsObserver.class); public Optional&lt;RegionObserver&gt; getRegionObserver() &#123; return Optional.of(this); &#125; public void start(CoprocessorEnvironment env) throws IOException &#123; LOG.info("====Test Start===="); &#125; public void stop(CoprocessorEnvironment env) throws IOException &#123; LOG.info("====Test End===="); &#125; public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; LOG.info("====Test postPut===="); &#125; public void postDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException &#123; LOG.info("====Test postDelete===="); &#125;&#125; pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tairanchina.csp.dmp&lt;/groupId&gt; &lt;artifactId&gt;hbase-observer-simple-example&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;HBase Observer Simple 用例&lt;/name&gt; &lt;properties&gt; &lt;hbase.version&gt;2.0.0&lt;/hbase.version&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.deploy.skip&gt;true&lt;/maven.deploy.skip&gt; &lt;maven.install.skip&gt;true&lt;/maven.install.skip&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;jetty-servlet&lt;/artifactId&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs-client&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;javax.servlet.jsp&lt;/artifactId&gt; &lt;groupId&gt;org.glassfish.web&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 包处理打包 1mvn clean assembly:assembly -Dmaven.test.skip=true 这里 package 得到的包必须是将依赖都包含在内的，否则，会报类找不到之类的错误。 上传包的时候，需要上传到 HDFS 下，同时，要给 hbase 用户授予权限，因而，我在测试的过程中，将其上传到 /apps/hbase 下（HDP 环境）。由于包名太长，这里对包名进行了重命名。 装载协处理器12345678910# 创建测试表create 'gejx_test','cf'# 停用测试表disable 'gejx_test'# 表与协处理器建立关系alter 'gejx_test' , METHOD =&gt;'table_att','coprocessor'=&gt;'hdfs://dev-dmp2.fengdai.org:8020/apps/hbase/hbase-observer-simple-example.jar|com.tairanchina.csp.dmp.examples.HbaseDataSyncEsObserver|1073741823'# 启用表enable 'gejx_test'# 查看表信息desc 'gejx_test' 测试12put 'gejx_test', '2','cf:name','gjx1'delete 'gejx_test', '2','cf:name' 查看日志要先在 HBase Master UI 界面下，确定数据存储在哪个节点上，再到相应的节点下面的 /var/log/hbase 下查看日志 1tail -100f hbase-hbase-regionserver-test.example.org.out 卸载协处理器123disable 'gejx_test'alter 'gejx_test', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1'enable 'gejx_test' 以上，已经完成最基础的协处理器实现。接下来进行讲述 ES 的一种实现方案。 HBase+ES 这里为了快速论证结果，在编码方面采用了硬编码方式，希望理解。 代码实现概览ElasticSearchBulkOperator.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package com.tairanchina.csp.dmp.examples;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import org.elasticsearch.action.bulk.BulkRequestBuilder;import org.elasticsearch.action.bulk.BulkResponse;import org.elasticsearch.action.delete.DeleteRequestBuilder;import org.elasticsearch.action.support.WriteRequest;import org.elasticsearch.action.update.UpdateRequestBuilder;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created on 2019/1/11. * * @author 迹_Jason */public class ElasticSearchBulkOperator &#123; private static final Log LOG = LogFactory.getLog(ElasticSearchBulkOperator.class); private static final int MAX_BULK_COUNT = 10000; private static BulkRequestBuilder bulkRequestBuilder = null; private static final Lock commitLock = new ReentrantLock(); private static ScheduledExecutorService scheduledExecutorService = null; static &#123; // init es bulkRequestBuilder bulkRequestBuilder = ESClient.client.prepareBulk(); bulkRequestBuilder.setRefreshPolicy(WriteRequest.RefreshPolicy.IMMEDIATE); // init thread pool and set size 1 scheduledExecutorService = Executors.newScheduledThreadPool(1); // create beeper thread( it will be sync data to ES cluster) // use a commitLock to protected bulk es as thread-save final Runnable beeper = () -&gt; &#123; commitLock.lock(); try &#123; bulkRequest(0); &#125; catch (Exception ex) &#123; System.out.println(ex.getMessage()); &#125; finally &#123; commitLock.unlock(); &#125; &#125;; // set time bulk task // set beeper thread(10 second to delay first execution , 30 second period between successive executions) scheduledExecutorService.scheduleAtFixedRate(beeper, 10, 30, TimeUnit.SECONDS); &#125; public static void shutdownScheduEx() &#123; if (null != scheduledExecutorService &amp;&amp; !scheduledExecutorService.isShutdown()) &#123; scheduledExecutorService.shutdown(); &#125; &#125; private static void bulkRequest(int threshold) &#123; if (bulkRequestBuilder.numberOfActions() &gt; threshold) &#123; BulkResponse bulkItemResponse = bulkRequestBuilder.execute().actionGet(); if (!bulkItemResponse.hasFailures()) &#123; bulkRequestBuilder = ESClient.client.prepareBulk(); &#125; &#125; &#125; /** * add update builder to bulk * use commitLock to protected bulk as thread-save * @param builder */ public static void addUpdateBuilderToBulk(UpdateRequestBuilder builder) &#123; commitLock.lock(); try &#123; bulkRequestBuilder.add(builder); bulkRequest(MAX_BULK_COUNT); &#125; catch (Exception ex) &#123; LOG.error(" update Bulk " + "gejx_test" + " index error : " + ex.getMessage()); &#125; finally &#123; commitLock.unlock(); &#125; &#125; /** * add delete builder to bulk * use commitLock to protected bulk as thread-save * * @param builder */ public static void addDeleteBuilderToBulk(DeleteRequestBuilder builder) &#123; commitLock.lock(); try &#123; bulkRequestBuilder.add(builder); bulkRequest(MAX_BULK_COUNT); &#125; catch (Exception ex) &#123; LOG.error(" delete Bulk " + "gejx_test" + " index error : " + ex.getMessage()); &#125; finally &#123; commitLock.unlock(); &#125; &#125;&#125; ESClient.java 12345678910111213141516171819202122232425262728293031323334353637383940package com.tairanchina.csp.dmp.examples;/** * Created on 2019/1/10. * * @author 迹_Jason */import org.elasticsearch.client.Client;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;import java.net.InetAddress;import java.net.UnknownHostException;/** * ES Cleint class */public class ESClient &#123; public static Client client; /** * init ES client */ public static void initEsClient() throws UnknownHostException &#123; System.setProperty("es.set.netty.runtime.available.processors", "false"); Settings esSettings = Settings.builder().put("cluster.name", "elasticsearch").build();//设置ES实例的名称 client = new PreBuiltTransportClient(esSettings).addTransportAddress(new TransportAddress(InetAddress.getByName("localhost"), 9300)); &#125; /** * Close ES client */ public static void closeEsClient() &#123; client.close(); &#125;&#125; HbaseDataSyncEsObserver.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.tairanchina.csp.dmp.examples;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.CoprocessorEnvironment;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.Durability;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.coprocessor.RegionObserver;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.hbase.wal.WALEdit;import org.apache.log4j.Logger;import java.io.IOException;import java.util.*;/** * Created on 2019/1/10. * * @author 迹_Jason */public class HbaseDataSyncEsObserver implements RegionObserver , RegionCoprocessor &#123; private static final Logger LOG = Logger.getLogger(HbaseDataSyncEsObserver.class); public Optional&lt;RegionObserver&gt; getRegionObserver() &#123; return Optional.of(this); &#125; @Override public void start(CoprocessorEnvironment env) throws IOException &#123; // init ES client ESClient.initEsClient(); LOG.info("****init start*****"); &#125; @Override public void stop(CoprocessorEnvironment env) throws IOException &#123; ESClient.closeEsClient(); // shutdown time task ElasticSearchBulkOperator.shutdownScheduEx(); LOG.info("****end*****"); &#125; @Override public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; String indexId = new String(put.getRow()); try &#123; NavigableMap&lt;byte[], List&lt;Cell&gt;&gt; familyMap = put.getFamilyCellMap(); Map&lt;String, Object&gt; infoJson = new HashMap&lt;&gt;(); Map&lt;String, Object&gt; json = new HashMap&lt;&gt;(); for (Map.Entry&lt;byte[], List&lt;Cell&gt;&gt; entry : familyMap.entrySet()) &#123; for (Cell cell : entry.getValue()) &#123; String key = Bytes.toString(CellUtil.cloneQualifier(cell)); String value = Bytes.toString(CellUtil.cloneValue(cell)); json.put(key, value); &#125; &#125; // set hbase family to es infoJson.put("info", json); LOG.info(json.toString()); ElasticSearchBulkOperator.addUpdateBuilderToBulk(ESClient.client.prepareUpdate("gejx_test","dmp_ods", indexId).setDocAsUpsert(true).setDoc(json)); LOG.info("**** postPut success*****"); &#125; catch (Exception ex) &#123; LOG.error("observer put a doc, index [ " + "gejx_test" + " ]" + "indexId [" + indexId + "] error : " + ex.getMessage()); &#125; &#125; @Override public void postDelete(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException &#123; String indexId = new String(delete.getRow()); try &#123; ElasticSearchBulkOperator.addDeleteBuilderToBulk(ESClient.client.prepareDelete("gejx_test", "dmp_ods", indexId)); LOG.info("**** postDelete success*****"); &#125; catch (Exception ex) &#123; LOG.error(ex); LOG.error("observer delete a doc, index [ " + "gejx_test" + " ]" + "indexId [" + indexId + "] error : " + ex.getMessage()); &#125; &#125;&#125; 其他方面的操作与上文操作类似，这里不再进行缀诉，直接看 Kibana 结果。 讲在最后上文中 HBase+ES 实现方案是在 HBase 和 ES 各自存放一份数据，使用协处理器达到数据一致性。这种方案存在数据冗余问题，在 ES 这边需要准备大量的存储空间。 还有一种方案也是比较流行的。使用 ES 作为二级索引的实现。使用协处理将需要查询的表查询字段与 RowKey 关系保存到 ES，查询数据的时候，先根据条件查询 ES 得到 RowKey，通过得到的 RowKey 查询 HBase 数据。以提高查询的效率。 Anyway，这两种方案都需要解决历史数据的问题和还有需要注意数据更新操作。 Q&amp;A 遇到 None of the configured nodes are available 错误信息？ 请检查一下 ES 的 cluster.name 配置是否错误。 为什么Hbase 2.0 Observer 未生效？ HBase 2.0 中 observer 接口有变化。你需要实现 RegionCoprocessor 的 getRegionObserver 接口。 发现已经更新包，协处理器还是在执行历史代码？ 当更新包的时候，要进行包名的变更，否则，可能会出现类似于缓存的现象问题。 待确认 未停用的情况下，更新 jar（已测试未操作表的时候，支持更新） 测试多张表公用同一个 jar 引文使用Hbase协作器(Coprocessor)同步数据到ElasticSearch 面向高稳定，高性能之-Hbase数据实时同步到ElasticSearch(之二) 使用HBase Coprocessor HBase 源码]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase ACL 实现]]></title>
    <url>%2F2018%2F08%2F01%2Fhbase20180801%2F</url>
    <content type="text"><![CDATA[HBase ACL 可以实现不同的用户、Group与Namespace、Table、ColumnFamily层级的数据权限控制 基本概念某个范围(Scope)的资源 范围 说明 Superuser 超级账号可以进行任何操作，运行HBase服务的账号默认是Superuser。也可以通过在hbase-site.xml中配置hbase.superuser的值可以添加超级账号 Global Global Scope拥有集群所有table的Admin权限 Namespace 在Namespace Scope进行相关权限控制 Table 在Table Scope进行相关权限控制 ColumnFamily 在ColumnFamily Scope进行相关权限控制 Cell 在Cell Scope进行相关权限控制 操作权限 操作 说明 Read ( R ) 读取某个Scope资源的数据 Write ( W ) 写数据到某个Scope的资源 Execute ( X ) 在某个Scope执行协处理器 Create ( C ) 在某个Scope创建/删除表等操作 Admin ( A ) 在某个Scope进行集群相关操作，如balance/assign等 实体 实体 说明 User 对某个用户授权 GROUP 对某个用户组授权 基本操作 HBase 123456789101112131415161718192021222324252627282930313233# hbase shell# 查看存在哪些表list# 创建表create '表名称', '列名称1','列名称2','列名称N'# 添加记录put '表名称', '行名称', '列名称:', '值'# 查看记录get '表名称', '行名称'# 查看表中的记录总数count '表名称'# 删除记录delete '表名' ,'行名称' , '列名称'# 删除一张表先要屏蔽该表，才能对该表进行删除，第一步 disable '表名称' 第二步 drop '表名称'# 查看所有记录scan "表名称"# 查看某个表某个列中所有数据scan "表名称" , ['列名称:']# 更新记录就是重写一遍进行覆# 退出 HBASE SHELL环境exit Namespace 123456789101112131415161718# hbase shell# 创建namespacecreate_namespace 'ns'# 删除namespacedrop_namespace 'ns'# 查看namespacedescribe_namespace 'ns'# 列出所有namespacelist_namespace# 在namespace下创建表create 'ns:test_table', 'fm1'# 查看namespace的表list_namespace_tables 'ns' Quick Start HBase服务配置 hbase-site.xml 12345678910111213141516&lt;property&gt; &lt;name&gt;hbase.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt; HBase的所有节点服务的配置都要进行添加 重启HBase服务 服务器用户配置 1234# 创建用户adduser &lt;userName&gt;# 复制hbase-config.sh文件cp bin/hbase-config.sh /home/&lt;userName&gt;/hbase-config.sh 下方使用test用户名进行讲解说明 设置权限 1234567891011121314# 进入hbase环境hbase shell# 创建namespacecreate_namespace 'ns'# 创建表create 'ns:test_table', 'fm1'# 给 test 用户授于ns:test_table表RW权限grant 'test','RW','ns:test_table' (1)# 查看权限user_permission 'ns:test_table' 单纯的namespace需要添加@前缀，user/group的授权方式一样，group需要加一个前缀@ 1234567grant &lt;user&gt; &lt;permissions&gt; [&lt;@namespace&gt; [&lt;table&gt; [&lt;column family&gt; [&lt;column qualifier&gt;]]]# 权限回收所有revoke 'test'# 权限回收指定内容revoke 'test','ns:test_table' 校验权限 12345678# 进入hbase环境# sudo -u test hbase shellsu - test hbase shell# 查看HBase表列表list# 或者使用test账号登录服务器进行查看 深入讲解HBase ACL是基于Linux环境的用户体系，进行对HBase环境权限的控制。上方提到的 test 用户其实就是Linux系统中的用户。HBase管理员为某个租户提供资源权限的时候， 需要先为其创建一个Linux系统的账号，再用管理员的权限为该账号进行权限操作。 可能会出现的问题 hbase-config.sh:No such file or directory 只需将 hbase-config.sh 文件copy一份到提示的目录下即可 sudo -u 命令无法执行 使用 su - 方式 配置 hbase-site.xml 不起作用 检查是否HBase服务没有重启，或者是否HBase所有节点都进行了配置 参考文献http://zhangxiong0301.iteye.com/blog/2244570 https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_sg_hbase_authorization.html#concept_enm_hhx_yp http://debugo.com/hbase-access-control/ https://community.pivotal.io/s/article/How-to-control-user-s-access-to-HBASE https://www.alibabacloud.com/help/zh/doc-detail/62705.htm]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Consumer重置Offset]]></title>
    <url>%2F2018%2F07%2F20%2FKafka%20ConsumerOffset20180720%2F</url>
    <content type="text"><![CDATA[在Kafka Version为0.11.0.0之后，Consumer的Offset信息不再默认保存在Zookeeper上，而是选择用Topic的形式保存下来。 在命令行中可以使用kafka-consumer-groups的脚本实现Offset的相关操作。 更新Offset由三个维度决定：Topic的作用域、重置策略、执行方案。 Topic的作用域 --all-topics：为consumer group下所有topic的所有分区调整位移） --topic t1 --topic t2：为指定的若干个topic的所有分区调整位移 --topic t1:0,1,2：为指定的topic分区调整位移 重置策略 --to-earliest：把位移调整到分区当前最小位移 --to-latest：把位移调整到分区当前最新位移 --to-current：把位移调整到分区当前位移 --to-offset &lt;offset&gt;： 把位移调整到指定位移处 --shift-by N： 把位移调整到当前位移 + N处，注意N可以是负数，表示向前移动 --to-datetime &lt;datetime&gt;：把位移调整到大于给定时间的最早位移处，datetime格式是yyyy-MM-ddTHH:mm:ss.xxx，比如2017-08-04T00:00:00.000 --by-duration &lt;duration&gt;：把位移调整到距离当前时间指定间隔的位移处，duration格式是PnDTnHnMnS，比如PT0H5M0S --from-file &lt;file&gt;：从CSV文件中读取调整策略 确定执行方案 什么参数都不加：只是打印出位移调整方案，不具体执行 --execute：执行真正的位移调整 --export：把位移调整方案按照CSV格式打印，方便用户成csv文件，供后续直接使用 注意事项 consumer group状态必须是inactive的，即不能是处于正在工作中的状态 不加执行方案，默认是只做打印操作 常用示例更新到当前group最初的offset位置 1bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-earliest --execute 更新到指定的offset位置 1bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-offset 500000 --execute 更新到当前offset位置（解决offset的异常） 1bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-current --execute offset位置按设置的值进行位移 1bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --shift-by -100000 --execute offset设置到指定时刻开始 1bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-datetime 2017-08-04T14:30:00.000]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Asciidoctor Maven插件使用]]></title>
    <url>%2F2018%2F07%2F05%2Fasciidoctor20180705%2F</url>
    <content type="text"><![CDATA[在项目应用中，我们会写很多文档去传递我们的设计思想、开发经验、采坑经历等等。使用Asciidoc的格式对非技术人员就不是那么的友好，或者说传递性、通用性与PDF和网页相比就差很多了。在JVM项目中可以使用Maven的插件方式将.adoc文件格式转化为PDF、HTML、EPUB等文件格式。 快速入门工程结构 12345678|doc-demo|-src|--main|---asciidoc|----.adoc文件|---resources|----images|pom.xml pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.demo&lt;/groupId&gt; &lt;artifactId&gt;docs&lt;/artifactId&gt; &lt;version&gt;1.1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;asciidoctorj.version&gt;1.5.6&lt;/asciidoctorj.version&gt; &lt;asciidoctorj.diagram.version&gt;1.5.4.1&lt;/asciidoctorj.diagram.version&gt; &lt;jruby.version&gt;1.7.26&lt;/jruby.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;!-- 默认命令，配置后可以直接使用mvn编译 --&gt; &lt;defaultGoal&gt;process-resources&lt;/defaultGoal&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;targetPath&gt;$&#123;project.build.directory&#125;/book&lt;/targetPath&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctor-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.5&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;output-html&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;process-asciidoc&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;backend&gt;html5&lt;/backend&gt; &lt;sourceHighlighter&gt;prettify&lt;/sourceHighlighter&gt; &lt;attributes&gt; &lt;toc&gt;left&lt;/toc&gt; &lt;icons&gt;font&lt;/icons&gt; &lt;sectanchors&gt;true&lt;/sectanchors&gt; &lt;!-- set the idprefix to blank --&gt; &lt;idprefix/&gt; &lt;/attributes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- Comment this section to use the default jruby artifact provided by the plugin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jruby&lt;/groupId&gt; &lt;artifactId&gt;jruby-complete&lt;/artifactId&gt; &lt;version&gt;$&#123;jruby.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Comment this section to use the default AsciidoctorJ artifact provided by the plugin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctorj&lt;/artifactId&gt; &lt;version&gt;$&#123;asciidoctorj.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctorj-diagram&lt;/artifactId&gt; &lt;version&gt;$&#123;asciidoctorj.diagram.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/book&lt;/outputDirectory&gt; &lt;sourceDocumentName&gt;book.adoc&lt;/sourceDocumentName&gt; &lt;imagesDir&gt;./&lt;/imagesDir&gt; &lt;preserveDirectories&gt;false&lt;/preserveDirectories&gt; &lt;requires&gt; &lt;require&gt;asciidoctor-diagram&lt;/require&gt; &lt;/requires&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 执行mvn命令 1mvn clean process-asciidoc 生成的HTML可以使用Http Server或者Nginx等服务进行部署，甚至可以使用Jenkins进行自动化部署。 生成PDF工程结构 1234567891011|doc-demo|-src|--main|---asciidoc|----data|-----fonts|-----themes|----.adoc文件|---resources|----images|pom.xml pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tairanchina.csp.dmp&lt;/groupId&gt; &lt;artifactId&gt;docs&lt;/artifactId&gt; &lt;version&gt;1.1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;asciidoctorj.version&gt;1.5.6&lt;/asciidoctorj.version&gt; &lt;asciidoctorj.diagram.version&gt;1.5.4.1&lt;/asciidoctorj.diagram.version&gt; &lt;jruby.version&gt;1.7.26&lt;/jruby.version&gt; &lt;asciidoctorj.pdf.version&gt;1.5.0-alpha-zh.16&lt;/asciidoctorj.pdf.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;!--https://github.com/asciidoctor/asciidoctor-maven-examples--&gt; &lt;!--https://github.com/asciidoctor/asciidoctor-maven-plugin/blob/master/README_zh-CN.adoc--&gt; &lt;!-- 默认命令，配置后可以直接使用mvn编译 --&gt; &lt;defaultGoal&gt;process-resources&lt;/defaultGoal&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;targetPath&gt;$&#123;project.build.directory&#125;/book&lt;/targetPath&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctor-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.5&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;output-html&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;process-asciidoc&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;backend&gt;html5&lt;/backend&gt; &lt;sourceHighlighter&gt;prettify&lt;/sourceHighlighter&gt; &lt;attributes&gt; &lt;toc&gt;left&lt;/toc&gt; &lt;icons&gt;font&lt;/icons&gt; &lt;sectanchors&gt;true&lt;/sectanchors&gt; &lt;!-- set the idprefix to blank --&gt; &lt;idprefix/&gt; &lt;/attributes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;output-pdf&lt;/id&gt; &lt;phase&gt;generate-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;process-asciidoc&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;backend&gt;pdf&lt;/backend&gt; &lt;sourceHighlighter&gt;coderay&lt;/sourceHighlighter&gt; &lt;doctype&gt;book&lt;/doctype&gt; &lt;attributes&gt; &lt;icons&gt;font&lt;/icons&gt; &lt;pagenums/&gt; &lt;toc/&gt; &lt;idprefix/&gt; &lt;idseparator&gt;-&lt;/idseparator&gt; &lt;pdf-fontsdir&gt;data/fonts&lt;/pdf-fontsdir&gt; &lt;pdf-stylesdir&gt;data/themes&lt;/pdf-stylesdir&gt; &lt;pdf-style&gt;cn&lt;/pdf-style&gt; &lt;/attributes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!-- Comment this section to use the default jruby artifact provided by the plugin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.jruby&lt;/groupId&gt; &lt;artifactId&gt;jruby-complete&lt;/artifactId&gt; &lt;version&gt;$&#123;jruby.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Comment this section to use the default AsciidoctorJ artifact provided by the plugin --&gt; &lt;dependency&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctorj&lt;/artifactId&gt; &lt;version&gt;$&#123;asciidoctorj.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctorj-diagram&lt;/artifactId&gt; &lt;version&gt;$&#123;asciidoctorj.diagram.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.asciidoctor&lt;/groupId&gt; &lt;artifactId&gt;asciidoctorj-pdf&lt;/artifactId&gt; &lt;version&gt;$&#123;asciidoctorj.pdf.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/book&lt;/outputDirectory&gt; &lt;sourceDocumentName&gt;book.adoc&lt;/sourceDocumentName&gt; &lt;imagesDir&gt;./&lt;/imagesDir&gt; &lt;preserveDirectories&gt;false&lt;/preserveDirectories&gt; &lt;requires&gt; &lt;require&gt;asciidoctor-diagram&lt;/require&gt; &lt;/requires&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 执行mvn命令 1mvn clean process-asciidoc 由于PDF格式插件没有安装中文字体，生成的PDF格式上会存在缺失，上方的fonts和themes可以对PDF的生成格式进行自定义，有时候为了方便，可以将其与asciidoctorj-pdf源码进行合并，手动打一个依赖包，放到自己的私服仓库中。 常见问题 在生成PDF的时候，可能code部分会存在很多空格的问题，一般产生这样的问题不是字体问题，而是编写格式有问题，可以选择将符号去掉。 参考资料Example Asciidoctor插件中文文档 Asciidoctor-PDF 中文乱码问题解决方案]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次大数据爬坑]]></title>
    <url>%2F2018%2F04%2F18%2Fbigdata20180418%2F</url>
    <content type="text"><![CDATA[前言VertxVertx是一个高效的异步框架，支持Java、Scala、JavaScript、Kotlin等多种语言。在非性能调优的场景下，TPS可以高达2-3万，同时，支持多种数据源也提供了异步支持。 Phoenix大数据的同学肯定对其很了解，是Apache基金会下的顶级工程，Phoenix帮助Hbase提供了SQL语法的支持，使难用的Hbase变得简单易用。 Hbase用于存储上百万的场景数据， Mysql用于存储Streaming处理和Batch之后数据量比较少，对SQL查询要求比较高的场景数据。 Redis用于存储统计数据，比如：PV、UV等类型数据。 爬坑日记Scala版本导致的冲突问题由于Vertx提供的Jar只支持Scala:2.12版本，而本地环境使用的是Scala:2.11，出现下方错误信息之后，猜想是由于Scala版本问题导致，摆在我们面前的有两条路，一条是换Scala版本号，由于种种原因无法更换版本；另一个方案是选用Vertx提供的Java Jar，选择放弃使用Scala版本，使用Java版本的Vertx的Jar来实现。 错误信息 1com.github.mauricio.async.db.SSLConfiguration.&lt;init&gt; scala.Product.$init$(Lscala/Product;)V Vertx包中Scala版本冲突在尝试完成Scala包换为Java之后，问题依旧，分析错误信息，猜想可能是com.github.mauricio相关的包导致的问题，在通过GitHub和官网文档中找到了蛛丝马迹，该包是由Scala编写的，就迅速想到了版本号的问题，果不其然，选用的是2.12，马上将Maven文件进行修改，解决了这个问题。 12345678910111213141516171819202122232425262728&lt;dependency&gt; &lt;groupId&gt;io.vertx&lt;/groupId&gt; &lt;artifactId&gt;vertx-redis-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.vertx&lt;/groupId&gt; &lt;artifactId&gt;vertx-mysql-postgresql-client&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;mysql-async_2.12&lt;/artifactId&gt; &lt;groupId&gt;com.github.mauricio&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;db-async-common_2.12&lt;/artifactId&gt; &lt;groupId&gt;com.github.mauricio&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;artifactId&gt;db-async-common_2.11&lt;/artifactId&gt; &lt;groupId&gt;com.github.mauricio&lt;/groupId&gt; &lt;version&gt;0.2.21&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;artifactId&gt;mysql-async_2.11&lt;/artifactId&gt; &lt;groupId&gt;com.github.mauricio&lt;/groupId&gt; &lt;version&gt;0.2.21&lt;/version&gt; &lt;/dependency&gt; Phoenix包问题项目中需要通过使用JDBC的方式连接Phoenix，在Spark项目中使用了如下的依赖实现 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-client&lt;/artifactId&gt; &lt;version&gt;$&#123;phoenix.version&#125;&lt;/version&gt; &lt;classifier&gt;client&lt;/classifier&gt;&lt;/dependency&gt; 但是出现了如下错误 1Caused by: java.lang.NoSuchMethodError: com.jayway.jsonpath.spi.mapper.JacksonMappingProvider.&lt;init&gt;(jackson-databind) 猜测可能原因是包冲突，但发现Maven中不存在jsonpath该相应的依赖，故猜想可能是jackson包版本导致的冲突，故将parent中的依赖配置移到当前pom文件中，因为Maven是就近查找依赖的，但发现还是没有效果。由于phoenix-client是一个独立的包，无法对其exclusion操作，在同事的提示下，采用的解压该Jar包，找到了jayway相关目录，将该目录删除后进行重新打包，神奇的事发生了，启动成功了。 Phoenix Driver问题程序启动成功，但在测试Vertx-JDBC连接Phoenix时，出现找不到Driver问题，原来phoenix-client中无法引用到org.apache.phoenix.jdbc.PhoenixDriver，在Google之后，使用了如下的Jar方案 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;$&#123;phoenix.version&#125;&lt;/version&gt;&lt;/dependency&gt; 问题就解决了。 jdbc:phoenix:host1,host2:2181:/hbase]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vertx-Phoenix实践]]></title>
    <url>%2F2018%2F04%2F18%2Fvertx-phoenix20180418%2F</url>
    <content type="text"><![CDATA[前言VertxVertx是一个高效的异步框架，支持Java、Scala、JavaScript、Kotlin等多种语言。在非性能调优的场景下，TPS可以高达2-3万，同时，支持多种数据源也提供了异步支持。 Phoenix大数据的同学肯定对其很了解，是Apache基金会下的顶级工程，Phoenix帮助Hbase提供了SQL语法的支持，让难用的Hbase变得简单易用。 场景出发点目标在项目应用中，为了达到简单、高效的接口化查询功能。 现状 使用HBase作为数据的持久化 场景对接口的TPS要求比较高 操作方式简单 问题与方案 Hbase是一种很好的大数据存储方案，但是其不支持SQL化操作，在开源解决方案中提供了Phoenix方案，文档和社区都比较活跃，故优先采用了 需要接口化和高TPS，使用单纯的Spring Boot无法实现目标，Vertx之前就在项目中使用，对其性能有所了解，同时支持Web应用，可以Spring Boot一起使用，故而选之 Vertx-Phoenix实现 只对涉及Phoenix方面进行讲解，通过Scala进行编写 依赖Pom12345678910111213&lt;dependency&gt; &lt;groupId&gt;io.vertx&lt;/groupId&gt; &lt;artifactId&gt;vertx-mysql-postgresql-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.vertx&lt;/groupId&gt; &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.13.1-HBase-1.3&lt;/version&gt;&lt;/dependency&gt; 实现123456789101112131415161718192021222324252627282930313233343536373839class HbaseDatabase(vertx: Vertx, dbConfig: DBInfoEntity, queryTimeout: Long) extends AbstractDatabase(vertx, dbConfig, queryTimeout) with LazyLogging &#123; var hbaseClient: JDBCClient = _ init(vertx, dbConfig, queryTimeout) override def init(vertx: Vertx, dbConfig: DBInfoEntity, queryTimeout: Long): Unit = &#123; val HbaseClientConfig: JsonObject = new JsonObject() .put("url", dbConfig.getHost) .put("user", "") .put("password", "") .put("max_idle_time", queryTimeout) .put("driver_class", "org.apache.phoenix.jdbc.PhoenixDriver") this.hbaseClient = JDBCClient.createShared(vertx, HbaseClientConfig) &#125; override def action(bodyInformation: BodyInformationVO, callback: Resp[Object] =&gt; Unit): Unit = &#123; hbaseClient.getConnection(new Handler[AsyncResult[SQLConnection]]() &#123; override def handle(res: AsyncResult[SQLConnection]): Unit = &#123; if (res.succeeded()) &#123; val connection = res.result() connection.query(bodyInformation.command, new Handler[AsyncResult[ResultSet]]() &#123; override def handle(result: AsyncResult[ResultSet]): Unit = &#123; if (result.succeeded()) &#123; setMetaData(result.result().getColumnNames, CacheContainer.getMetadatas) callback(Resp.success(result.result().getRows())) &#125; else &#123; logger.error("Get HBase value is error", result.cause()) callback("Get HBase value is error") &#125; &#125; &#125;) &#125; else &#123; logger.error("Get HBase connect is error", res.cause()) callback("Get HBase connect is error") &#125; &#125; &#125;) &#125; 相关配置说明URL格式1jdbc:phoenix:host1,host2:2181:/hbase]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网关调优指导书]]></title>
    <url>%2F2017%2F12%2F21%2Fspringcloud%E7%BD%91%E5%85%B3%E8%B0%83%E4%BC%9820171221%2F</url>
    <content type="text"><![CDATA[由于最近在使用Spring Cloud的Zuul网关的过程中，发现超时的可能性很多，出于性能的调优，所有想通过测试，了解一些参数的作用。在文章最后贴上推荐方案。 先看一个问题： execution.isolation.thread.timeoutInMilliseconds 到达当前时间后，会触发熔断，调用fallback方法，如果不存在fallback方法，会报错误 12345&#123; "code": "500", "message": "[Internal Server Error]getStores timed-out and fallback failed.", "body": null&#125; 结果： 123com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.client.ClientException: nullCaused by: java.net.SocketTimeoutException: Read timed out 说明：下面zuul是网关配置，service是网关代理下的一个服务 Case1zuul，延时3s 1234ribbon: ConnectTimeout: 2000 # 请按实际情况配置 ReadTimeout: 1000 service，延时2s 第一次访问的时候 12com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.client.ClientException: Load balancer does not have available server for client: hystrix-example 第二次访问的时候 12345678910com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.hystrix.exception.HystrixRuntimeException: hystrix-example timed-out and no fallback available.Caused by: java.util.concurrent.TimeoutException: null###result&#123; &quot;code&quot;: &quot;500&quot;, &quot;message&quot;: &quot;[Internal Server Error]TIMEOUT&quot;, &quot;body&quot;: null&#125; Case2zuul，延时3s 1234ribbon: ConnectTimeout: 4000 # 请按实际情况配置 ReadTimeout: 1000 service，延时2s 12345678910com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.hystrix.exception.HystrixRuntimeException: hystrix-example timed-out and no fallback available.Caused by: java.util.concurrent.TimeoutException: null###result&#123; &quot;code&quot;: &quot;500&quot;, &quot;message&quot;: &quot;[Internal Server Error]TIMEOUT&quot;, &quot;body&quot;: null&#125; Case3zuul，延时3s 1234ribbon: ConnectTimeout: 4000 # 请按实际情况配置 ReadTimeout: 1000 service，延时0.5s 正常 Case4zuul，延时3s 1234ribbon: ConnectTimeout: 4000 # 请按实际情况配置 ReadTimeout: 1000 service，延时2s 123456789hystrix: command: default: execution: timeout: enabled: true isolation: thread: timeoutInMilliseconds: 60000 12345678910com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.hystrix.exception.HystrixRuntimeException: hystrix-example timed-out and no fallback available.Caused by: java.util.concurrent.TimeoutException: null###result&#123; &quot;code&quot;: &quot;500&quot;, &quot;message&quot;: &quot;[Internal Server Error]TIMEOUT&quot;, &quot;body&quot;: null&#125; 结论： hystrix超时时间在配置文件中配置时无效的 123456789hystrix: command: default: execution: timeout: enabled: true isolation: thread: timeoutInMilliseconds: 60000 hystrix默认超时时间四1s，如果服务执行时间超过1s就会进行熔断，如果没有fallback，就会导致TIMEOUT错误 Case5zuul，延时3s 1234ribbon: ConnectTimeout: 500 # 请按实际情况配置 ReadTimeout: 2000 service，延时0.5s 正常 Case6zuul，延时3s 1234ribbon: ConnectTimeout: 400 # 请按实际情况配置 ReadTimeout: 400 service，延时0.5s 12345678910com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.client.ClientException: nullCaused by: java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed outCaused by: java.net.SocketTimeoutException: Read timed out####reuslt&#123; &quot;code&quot;: &quot;500&quot;, &quot;message&quot;: &quot;[Internal Server Error]GENERAL&quot;, &quot;body&quot;: null&#125; Case7zuul，延时3s 1234ribbon: ConnectTimeout: 400 # 请按实际情况配置 ReadTimeout: 600 service，延时0.5s 正常 结论： ribbon.readtimeout超时会导致SocketTimeoutException: Read timed out问题。 Case8zuul，延时3s 12345678910ribbon: ConnectTimeout: 1000 # 请按实际情况配置 ReadTimeout: 20000zuul: host: # 连接超时时间 connect-timeout-millis: 3000 # 响应超时时间 socket-timeout-millis: 100 service，延时0.5s 正常 结论：socket-timeout-millis对请求时间没有影响 Case9zuul，延时3s 12345678910111213hystrix: command: default: execution: isolation: thread: # 请按实际情况设置配置 timeoutInMilliseconds: 600ribbon: ConnectTimeout: 1000 # 请按实际情况配置 ReadTimeout: 20000 service，延时0.5s 正常 Case10zuul，延时3s 12345678910111213hystrix: command: default: execution: isolation: thread: # 请按实际情况设置配置 timeoutInMilliseconds: 400ribbon: ConnectTimeout: 1000 # 请按实际情况配置 ReadTimeout: 20000 service，延时0.5s 12345678910com.netflix.zuul.exception.ZuulException: Forwarding errorCaused by: com.netflix.hystrix.exception.HystrixRuntimeException: hystrix-example timed-out and no fallback available.Caused by: java.util.concurrent.TimeoutException: null###result&#123; &quot;code&quot;: &quot;500&quot;, &quot;message&quot;: &quot;[Internal Server Error]TIMEOUT&quot;, &quot;body&quot;: null&#125; 结论： Hystrix的超时时间是对次节点的请求时间的进行熔断 zuul，延时3s service，延时0.5s 12Caused by: com.netflix.hystrix.exception.HystrixRuntimeException: hystrix-example could not be queued for execution and no fallback available.error 500 : REJECTED_THREAD_EXECUTION 总结： Hystrix的超时配置是服务调用其他服务时候，有效，对服务自身是没有作用。e.g. A-&gt;B，是对A请求B这个请求有效，对访问A本身的请求是没有作用的。 网关在启动初期，会存在不稳定，甚至存在马上熔断的可能，但在之后，会恢复正常水平。 com.netflix.client.ClientException: Load balancer does not have available server for client: hystrix-example是服务启动还没完全，或者，如果使用了ribbon.eureka.eabled=false也会出现这个问题。 服务启动时候，推荐网关最后启动 推荐使用方案： 123456789101112131415161718192021222324252627zuul: okhttp: enabled: true # 使用okhttp方式请求，正常来说okhttp比较速度快一点 semaphore: max-semaphores: 500 # 并发处理数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 host: socket-timeout-millis: 30000 # socket超时时间，如果使用service-id方式是不用配置的 connect-timeout-millis: 30000 # 连接时间semaphores max-total-connections: 5000 # 最大连接数，值越大越好，但到到达一个临界点之后，就不会提高响应速度了 max-per-route-connections: 5 # 每个router最大连接数，降低请求时间，越小越好，但达到一定层级就没用了hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 30000 # Hystrix超时时间 strategy: THREADribbon: ReadTimeout: 20000 # 处理时间 ConnectTimeout: 20000 # 连接时间 MaxAutoRetries: 0 #最大自动重试次数 MaxAutoRetriesNextServer: 1 # 换实例重试次数 MaxTotalHttpConnections: 2000 # 最大http连接数，越大越好，但到到达一个临界点之后，就不会提高响应速度了 MaxConnectionsPerHost: 1000 # 每个host连接数 注意说明： timeout-in-milliseconds这样编写是不会生效的，需要改为timeoutInMilliseconds，起初认为是Spring的BUG，之后，发现由于default是一个key，是一个Map类型，依照源码中使用的是timeoutInMilliseconds，所以必须timeoutInMilliseconds。 拓展学习：http://www.spring4all.com/article/351 http://m635674608.iteye.com/blog/2389666 http://cloud.spring.io/spring-cloud-netflix/single/spring-cloud-netflix.html#how-to-configure-hystrix-thread-pools http://tietang.wang/2016/02/25/hystrix/Hystrix%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/ https://github.com/spring-cloud/spring-cloud-netflix/issues/327 zuul性能测试]]></content>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐书籍]]></title>
    <url>%2F2017%2F12%2F21%2Fbooks%2F</url>
    <content type="text"><![CDATA[只想给你最好的 AI智能时代：大数据与智能革命重新定义未来 深度学习]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bookmark]]></title>
    <url>%2F2017%2F12%2F14%2Fbookmark%2F</url>
    <content type="text"><![CDATA[高质量网站整合到这里~~ 前端ES6ES6 核心特性 ES6在WebStrom中的相关配置 前端基础进阶（十三）：透彻掌握Promise的使用，读这篇就够了 NodeNodeJS入门 WebpackWebpack傻瓜式指南（一） Webpack傻瓜指南（二）开发和部署技巧 入门Webpack，看这篇就够了 webpack-dev-server介绍及配置 webpack 插件： html-webpack-plugin 使用webpack构建多页面应用 gulp + webpack 构建多页面前端项目 webpack入坑之旅（一）不是开始的开始 webpack入坑之旅（二）loader入门 webpack入坑之旅（三）webpack.config入门 webpack入坑之旅（四）扬帆起航 webpack入坑之旅（五）加载vue单文件组件 webpack入坑之旅（六）配合vue-router实现SPA webpack使用优化 手把手深入理解 webpack dev middleware 原理與相關 plugins 入门Webpack，看这篇就够了 请练完这16个webpack小例子 FullPagejQuery全屏滚动插件fullPage.js FullPage.js全屏滚动插件学习总结 Vue延迟加载(Lazyload)三种实现方式 图解 Flexbox 2 - 深入理解 Vue中router-link介绍 手把手教你封装一个vue component 其他你不知道的 XMLHttpRequest JAVAJDKJava8_Stream 10种简单的Java性能优化 Java中Exception的种类 谁在关心toString的性能？ 深入理解Java：内部类 《深入理解Java集合框架》系列文章（已全部更新完毕） 原码, 反码, 补码 详解 SpringframeWork Spring缓存注解@Cache,@CachePut , @CacheEvict，@CacheConfig使用 深入理解分布式事务 springMVC中文文档 Microservice微服务（Microservice）那点事 Dubbo 博客 Spring Boot[Spring Boot 系列] 集成maven和Spring boot的profile 专题 SpringBoot之redis Springboot最全配置文件 Springboot 入门 Spring-Boot-Reference-Guide中文 Spring CloudSpring Cloud构建微服务架构（一）服务注册与发现 Spring Cloud构建微服务架构（二）服务消费者 Spring Cloud构建微服务架构（三）断路器 Spring Cloud构建微服务架构（四）分布式配置中心 Spring Cloud构建微服务架构（四）分布式配置中心（续） Spring Cloud构建微服务架构（五）服务网关 Spring Cloud构建微服务架构（六）高可用服务注册中心 Spring Cloud构建微服务架构（七）消息总线 Spring Cloud构建微服务架构（七）消息总线（续：Kafka） 使用Spring Cloud与Docker实战微服务 史上最简单的SpringCloud教程: docker部署spring cloud项目 spring cloud config 详解 RestTemplate的逆袭之路，从发送请求到负载均衡 Spring RestTemplate中几种常见的请求方式 Spring Cloud源码分析（二）Ribbon MybatisSpringBoot 快速整合Mybatis（去XML化+注解进阶） 其他OAuth 2.0 认证的原理与实践 优秀程序员不得不知道的20个位运算技巧 数据库数据库读写分离和负载均衡策略 Mysql主从Java端实现 H2数据库函数及数据类型概述 MySQL入门教程 大数据大数据入门 机器学习吴恩达斯坦福机器学习 推荐些 AI 学习的书籍和资料 性能测试JVM性能火焰图 PPT优质内容Open-Source License iconfont 博客社区SpringCloud中国社区 ImportNew Spring Boot系列文章 微服务等架构分享 阿里巴巴中间价团队博客 掘金高质量技术社区 热链 it资讯，聚合 技术公众号聚合 csdn 极客头条技术 阿里巴巴技术沙龙 https://wanqu.co/ 唯品会技术大牛 小米程序员（小米发展里程技术） java jvm技术等 微信聚合，学习.工具栏目 美团技术博客，大批量订单相关 ibm中国java，高质量博客 张开涛博客，spring等京东技术 阿里mysql技术专家 阿里数据库内核月报 计算机书籍经典实战系列和动物书 黑客大会资料 纯洁的微笑 优秀项目SpringBoot&amp;&amp;Cloud SpringBoot和SpringCloud学习 go-library 30分钟入门 hsweb 一个用于快速搭建企业后台管理系统的基础项目,集成一揽子便捷功能如:通用增删改查，在线代码生成，权限管理(可控制到列和行)，动态多数据源分布式事务，动态脚本，动态定时任务，在线数据库维护等等. 基于 spring-boot,mybaits handbook 放置我的笔记、搜集、摘录、实践，保持好奇心。这里就是个随记，涉猎技术知识点广而不精，不能保证正确，看文需谨慎，后果很严重。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feign 如何支持进行文件上传]]></title>
    <url>%2F2017%2F12%2F01%2Fspringcloudfegin20171201%2F</url>
    <content type="text"><![CDATA[最近，别的项目组提出需要SDK，就利用Feign做了一个，在此期间发现上传文件是一个坑，正常的实现是无法支持文件上传，需要进行对资源有一个Convert。为了避免大家像我一样，继续掉坑里，就出现了这篇文章的初衷。 入门 在SDK工程处，添加包依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign.form&lt;/groupId&gt; &lt;artifactId&gt;feign-form&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign.form&lt;/groupId&gt; &lt;artifactId&gt;feign-form-spring&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt; 在SDK工程处，创建一个Configuration 123456789101112131415161718192021import feign.codec.Encoder;import feign.form.spring.SpringFormEncoder;import org.springframework.beans.factory.ObjectFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.autoconfigure.web.HttpMessageConverters;import org.springframework.cloud.netflix.feign.support.SpringEncoder;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class MultipartSupportConfig &#123; @Autowired private ObjectFactory&lt;HttpMessageConverters&gt; messageConverters; @Bean public Encoder feignFormEncoder() &#123; return new SpringFormEncoder(new SpringEncoder(messageConverters)); &#125;&#125; 期初在网上看到是使用下方的注入方式，一直不成功，在走头部路下，尝试了👆者方案成功了。 1234@FeignClient(name = "demo",configuration=MultipartSupportConfig.class)public interface SignBaseCommonClient &#123; &#125; 修改接口 1234567@FeignClient(name = "demo")public interface FeginExample &#123; @PostMapping(value = "images", consumes = MULTIPART_FORM_DATA_VALUE) Resp&lt;String&gt; uploadImage( @RequestParam MultipartFile image, @RequestParam("id") String id);&#125; @RequestPart与@RequestParam效果是一样的，大家就不用花时间在这上面了。 修改服务器接口 12345678910@RestControllerpublic class FeginServiceExample &#123; @PostMapping(value = "images", consumes = MULTIPART_FORM_DATA_VALUE) public Resp&lt;String&gt; uploadImage( @RequestParam("image") MultipartFile image, @RequestParam("id") String id, HttpServletRequest request) &#123; return Resp.success(null); &#125;&#125; 在启动类添加@EnableFeignClients 这个就不用多说了吧，😆 常见问题： HTTP Status 400 - Required request part ‘file’ is not present 请求文件参数的名称与实际接口接受名称不一致 feign.codec.EncodeException: Could not write request: no suitable HttpMessageConverter found for request type [org.springframework.mock.web.MockMultipartFile] and content type [multipart/form-data] 转换器没有生效，检查一下MultipartSupportConfig]]></content>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot Admin最佳实践]]></title>
    <url>%2F2017%2F11%2F29%2Fspringbootadmin20171128%2F</url>
    <content type="text"><![CDATA[本文不进行Spring Boot Admin入门知识点说明 在Spring Boot Actuator中提供很多像health、metrics等实时监控接口，可以方便我们随时跟踪服务的性能指标。Spring Boot默认是开放这些接口提供调用的，那么就问题来了，如果这些接口公开在外网中，很容易被不法分子所利用，这肯定不是我们想要的结果。在这里我们提供一种比较好的解决方案。 被监控的服务配置 为被保护的http请求添加请求前缀 123456management: context-path: /example-context #&lt;1&gt;eureka: instance: status-page-url-path: $&#123;management.context-path&#125;/info #&lt;2&gt; health-check-url-path: $&#123;management.context-path&#125;/health 添加请求前缀 Spring Boot Admin在启动的时候会去eureka拉去服务信息，其中health与info需要特殊处理，这两者的地址是根据status-page-url-path和health-check-url-path的值。 zuul网关配置 zuul保护内部服务http接口 12zuul: ignoredPatterns: /*/example-context/** #&lt;1&gt; 这里之所以不是/example-context/**，由于网关存在项目前缀，需要往前一级，大家可以具体场景具体配置 Spring Boot Admin配置 配置监控的指标参数 12345678910111213141516171819spring: application: name: monitor boot: admin: discovery: converter: management-context-path: /example-context # The endpoints URL prefix #&lt;1&gt; routes: endpoints: env,metrics,dump,jolokia,info,configprops,trace,logfile,refresh,flyway,liquibase,heapdump,loggers,auditevents,hystrix.stream turbine: clusters: default location: monitorturbine: aggregator: clusterConfig: default appConfig: monitor-example #&lt;2&gt; clusterNameExpression: metadata['cluster'] 与应用配置的management.context-path相同 添加需要被监控的应用Service-Id，以逗号分隔 讲解一下，通过创建一个请求前缀，可以在网关处使用前缀的方式将其排除，也就是外网将无法访问这些监控API，同时，内网还是可以进行加前缀的方式进行访问，为Spring Boot Admin提供了支持条件。management还支持port和ip的方式，但这两种方式有局限性，如果在同一台机器上部署多个服务，就会存在端口占用或者其他问题。这种方案还有一个好处，以上配置一旦确定以后，所有服务都不需要进行特殊化处理，可以直接使用。 问答： 问题：Full authentication is required to access this resource 在被监控的服务中添加management.security.enabled=false 拓展阅读：spring-boot-admin-samples issue jolokia]]></content>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot启动方式与部署]]></title>
    <url>%2F2017%2F10%2F16%2Fspringboot_deloy20171016%2F</url>
    <content type="text"><![CDATA[Spring Boot为我们提供很多便捷的启动和配置方式。本文就来好好说一下这两方面。 启动方式12345678import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GirlApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GirlApplication.class, args); &#125;&#125; 方法一：直接在程序中run 方法二：在命令行中切换到工程所在目录，mvn spring-boot:run 方法三：先mvn install编译工程，之后切换到target路径下，使用java -jar jar包名 1java -jar target/first-project-1.0.0.jar --spring.profile.active=prod 在Ctrl+C之后，服务即停止 方法四： 12345nohup -Dspring.profiles.active=dev -jar XXX.jar &gt;/dev/null &amp;# 指定logjava -jar spring-boot01-1.0-SNAPSHOT.jar &gt; log.file 2&gt;&amp;1 &amp;# 配置服务内存信息java -server -Xms8000m -Xmx8000m -jar luckydrawall-0.1.1.jar --spring.profiles.active=rel-Xmx8000m -jar luckydrawall-0.1.1.jar --spring.profiles.active=rel 方法五：外部Tomcat部署（不推荐） 1、将项目的启动类Application.java继承SpringBootServletInitializer并重写configure方法 12345678910@SpringBootApplicationpublic class Application extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; return application.sources(Application.class); &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2、在pom.xml文件中，project下面增加package标签1&lt;packaging&gt;war&lt;/packaging&gt; 3、还是在pom.xml文件中，dependencies下面添加12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;complied&lt;/scope&gt;&lt;/dependency&gt; 这样，只需要以上3步就可以打包成war包，并且部署到tomcat中了。需要注意的是这样部署的request url需要在端口后加上项目的名字才能正常访问。spring-boot更加强大的一点就是：即便项目是以上配置，依然可以用内嵌的tomcat来调试，启动命令和以前没变，还是：mvn spring-boot:run。如果需要在springboot中加上request前缀，需要在application.properties中添加server.contextPath=/prefix/即可。其中prefix为前缀名。这个前缀会在war包中失效，取而代之的是war包名称，如果war包名称和prefix相同的话，那么调试环境和正式部署环境就是一个request地址了。 部署由于Spring Boot内置了Tomcat，从而可以直接使用jar的方式进行部署。启动命令在上方进行了说明。部署这一环节重要就是配置文件。 Spring Boot 所提供的配置优先级顺序比较复杂。按照优先级从高到低的顺序，具体的列表如下所示。 命令行参数。 通过 System.getProperties() 获取的 Java 系统参数。 操作系统环境变量。 从 java:comp/env 得到的 JNDI 属性。 通过 RandomValuePropertySource 生成的“random.*”属性。 应用jar 文件之外的属性文件。(通过spring.config.location参数) 应用jar 文件内部的属性文件。 在应用配置 Java 类（包含“@Configuration”注解的 Java 类）中通过“@PropertySource”注解声明的属性文件。 通过“SpringApplication.setDefaultProperties”声明的默认属性。 说明： 1）Spring Boot应用在启动命令中使用--开头的命令行参数，可修改应用的配置。 1java -server -Xms8000m -Xmx8000m -jar luckydrawall-0.1.1.jar --spring.profiles.active=rel-Xmx8000m 使用如下代码进行关闭 1SpringApplication.setAddCommandLineProperties(false) 6）属性文件是比较推荐的配置方式。Spring Boot在启动时会对如下目录进行搜查，读取相应配置文件。优先级从高到低。 当前jar目录的“/config”子目录 当前jar目录 classpath 中的“/config”包 classpath 可以通过“spring.config.name”配置属性来指定不同的属性文件名称。也可以通过“spring.config.location”来添加额外的属性文件的搜索路径。如果应用中包含多个 profile，可以为每个 profile 定义各自的属性文件，按照“application-{profile}”来命名。 1java -jar demo.jar --spring.config.location=/path/test_evn.properties 使用Profile区分环境在Spring Boot中可以使用application.yml，application-default.yml，application-dev.yml，application-test.yml进行不同环境的配置。默认时，会读取application.yml，application-default.yml这两个文件中的配置，优先级高的会覆盖优先级低的配置。无论切换到哪个环境，指定的环境的配置的优先级是最高的。 可以使用spring.profiles.active=dev指定环境。 推荐阅读：Spring Boot 配置优先级顺序]]></content>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口动态实现解决方案]]></title>
    <url>%2F2017%2F07%2F26%2Fmybatits20170726%2F</url>
    <content type="text"><![CDATA[声明解决方案是基于Mybatis源码，进行二次开发实现。 问题领导最近跟我提了一个需求，是有关于实现类Mybatis的@Select、@Insert注解的功能。其是基于interface层面，不存在任何的接口实现类。因而在实现的过程中，首先要解决的是如何动态实现接口的实例化。其次是如何将使接口根据注解实现相应的功能。 我们先来看看Mybatis是如何实现Dao类的扫描的。MapperScannerConfigurer.java 123456789101112131415161718public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.registerFilters(); scanner.scan(StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS)); &#125; ClassPathMapperScanner是Mybatis继承ClassPathBeanDefinitionScanner类而来的。这里对于ClassPathMapperScanner的配置参数来源于我们在使用Mybatis时的配置而来，是不是还记得在使用Mybatis的时候要配置basePackage的参数呢？ 接着我们就顺着scanner.scan()方法，进入查看一下里面的实现。 ClassPathBeanDefinitionScanner.java 123456789101112public int scan(String... basePackages) &#123; int beanCountAtScanStart = this.registry.getBeanDefinitionCount(); doScan(basePackages); // Register annotation config processors, if necessary. if (this.includeAnnotationConfig) &#123; AnnotationConfigUtils.registerAnnotationConfigProcessors(this.registry); &#125; return (this.registry.getBeanDefinitionCount() - beanCountAtScanStart);&#125; 这里关键的代码是doScan(basePackages);，那么我们在进去看一下。可能你会看到的是Spring源码的实现方法，但这里Mybatis也实现了自己的一套，我们看一下Mybatis的实现。 ClassPathMapperScanner.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefinitions = super.doScan(basePackages); if (beanDefinitions.isEmpty()) &#123; logger.warn("No MyBatis mapper was found in '" + Arrays.toString(basePackages) + "' package. Please check your configuration."); &#125; else &#123; for (BeanDefinitionHolder holder : beanDefinitions) &#123; GenericBeanDefinition definition = (GenericBeanDefinition) holder.getBeanDefinition(); if (logger.isDebugEnabled()) &#123; logger.debug("Creating MapperFactoryBean with name '" + holder.getBeanName() + "' and '" + definition.getBeanClassName() + "' mapperInterface"); &#125; // the mapper interface is the original class of the bean // but, the actual class of the bean is MapperFactoryBean definition.getPropertyValues().add("mapperInterface", definition.getBeanClassName()); definition.setBeanClass(MapperFactoryBean.class); definition.getPropertyValues().add("addToConfig", this.addToConfig); boolean explicitFactoryUsed = false; if (StringUtils.hasText(this.sqlSessionFactoryBeanName)) &#123; definition.getPropertyValues().add("sqlSessionFactory", new RuntimeBeanReference(this.sqlSessionFactoryBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionFactory != null) &#123; definition.getPropertyValues().add("sqlSessionFactory", this.sqlSessionFactory); explicitFactoryUsed = true; &#125; if (StringUtils.hasText(this.sqlSessionTemplateBeanName)) &#123; if (explicitFactoryUsed) &#123; logger.warn("Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored."); &#125; definition.getPropertyValues().add("sqlSessionTemplate", new RuntimeBeanReference(this.sqlSessionTemplateBeanName)); explicitFactoryUsed = true; &#125; else if (this.sqlSessionTemplate != null) &#123; if (explicitFactoryUsed) &#123; logger.warn("Cannot use both: sqlSessionTemplate and sqlSessionFactory together. sqlSessionFactory is ignored."); &#125; definition.getPropertyValues().add("sqlSessionTemplate", this.sqlSessionTemplate); explicitFactoryUsed = true; &#125; if (!explicitFactoryUsed) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Enabling autowire by type for MapperFactoryBean with name '" + holder.getBeanName() + "'."); &#125; definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); &#125; &#125; &#125; return beanDefinitions;&#125; definition.setBeanClass(MapperFactoryBean.class);这行代码是非常关键的一句，由于在Spring中存在两种自动实例化的方式，一种是我们常用的本身的接口实例化类进行接口实例化，还有一种就是这里的自定义实例化。而这里的setBeanClass方法就是在BeanDefinitionHolder中进行配置。在Spring进行实例化的时候进行处理。 那么我们在看一下MapperFactoryBean.class MapperFactoryBean.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class MapperFactoryBean&lt;T&gt; extends SqlSessionDaoSupport implements FactoryBean&lt;T&gt; &#123; private Class&lt;T&gt; mapperInterface; private boolean addToConfig = true; /** * Sets the mapper interface of the MyBatis mapper * * @param mapperInterface class of the interface */ public void setMapperInterface(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; /** * If addToConfig is false the mapper will not be added to MyBatis. This means * it must have been included in mybatis-config.xml. * &lt;p&gt; * If it is true, the mapper will be added to MyBatis in the case it is not already * registered. * &lt;p&gt; * By default addToCofig is true. * * @param addToConfig */ public void setAddToConfig(boolean addToConfig) &#123; this.addToConfig = addToConfig; &#125; /** * &#123;@inheritDoc&#125; */ @Override protected void checkDaoConfig() &#123; super.checkDaoConfig(); notNull(this.mapperInterface, "Property 'mapperInterface' is required"); Configuration configuration = getSqlSession().getConfiguration(); if (this.addToConfig &amp;&amp; !configuration.hasMapper(this.mapperInterface)) &#123; try &#123; configuration.addMapper(this.mapperInterface); &#125; catch (Throwable t) &#123; logger.error("Error while adding the mapper '" + this.mapperInterface + "' to configuration.", t); throw new IllegalArgumentException(t); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; &#125; /** * &#123;@inheritDoc&#125; */ public T getObject() throws Exception &#123; return getSqlSession().getMapper(this.mapperInterface); &#125; /** * &#123;@inheritDoc&#125; */ public Class&lt;T&gt; getObjectType() &#123; return this.mapperInterface; &#125; /** * &#123;@inheritDoc&#125; */ public boolean isSingleton() &#123; return true; &#125; 在该类中其实现了FactoryBean接口，看过Spring源码的人，我相信对其都有很深的印象，其在Bean的实例化中起着很重要的作用。在该类中我们要关注的是getObject方法，我们之后将动态实例化的接口对象放到Spring实例化列表中，这里就是入口，也是我们的起点。不过要特别说明的是mapperInterface的值是如何被赋值的，可能会有疑问，我们再来看看上面的ClassPathMapperScanner.java我们在配置MapperFactoryBean.class的上面存在一行 definition.getPropertyValues().add(&quot;mapperInterface&quot;, definition.getBeanClassName());其在之后在Spring的PostProcessorRegistrationDelegate类的populateBean方法中进行属性配置，会将其依靠反射的方式将其注入到MapperFactoryBean.class中。 而且definition.getPropertyValues().add中添加的值是注入到MapperFactoryBean对象中去的。这一点需要说明一下。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端错误信息站]]></title>
    <url>%2F2017%2F07%2F04%2Ffrontenderror%2F</url>
    <content type="text"><![CDATA[nodejs/webpack项目提示Invalid Host header 新版的webpack-dev-server出于安全考虑，默认检查hostname，如果hostname不是配置内的，将中断访问。 ———— disableHostCheck: true 123456devServer: &#123; contentBase: path.resolve(__dirname, 'build'), host: '0.0.0.0', port: process.env.PORT || 8601, disableHostCheck: true &#125;]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Config]]></title>
    <url>%2F2017%2F07%2F02%2Fspringcloud_config20170702%2F</url>
    <content type="text"><![CDATA[config是Spring Cloud中的配置中心，在正式场景中，存在修改配置的情况，每次配置的修改都要进行重新打包，这是非常麻烦的一件事，可能还伴随着其他问题的引发。而config就可以将一些与启动无关的配置进行动态修改，并生效。以前要数据库进行配置的，现在也可以在config中完成。 快速入门Config服务端123456789 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; bootstrap.yml 1234567891011121314151617181920212223242526272829303132333435# Spring propertiesspring: application: name: config-service # Service registers under this name cloud: config: server: git: uri: https://github.com/zoeminghong/spring-cloud-demo.git search-paths: config-repo #文件搜索路径 username: username #账号 password: password #密码 name: appzone #application name label: master #分支# profiles:# active: native #当不使用git时，可以设置为native，获取config服务下main/java/resources本地配置# Map the error path to error template (for Thymeleaf)error: path=/error# Discovery Server Accesseureka: instance: hostname: localhost client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://root:123456@$&#123;eureka.instance.hostname&#125;:8761/eureka/# 注册到eureka服务，账号：密码@地址# HTTP Serverserver: port: 7001 # HTTP (Tomcat) port Main.java 12345678@EnableConfigServer@SpringBootApplication@EnableDiscoveryClientpublic class ConfigApplication &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(ConfigApplication.class).web(true).run(args); &#125;&#125; Config客户端pom配置是与服务端是一样的 1234567891011121314151617181920212223242526272829303132333435spring: cloud:# config的相关配置 config: label: master profile: dev uri: http://localhost:7001/ #config Service address name: appzone #application name discovery: enabled: true #whether enable service-id service-id: CONFIG-SERVICE #config service application name on the eureka username: root # regisit centre's username password: 123456 # regisit centre's password# fail-fast: true # 没有连接配置服务端时直接启动失败# Spring properties application: name: order-service # Service registers under this name freemarker: enabled: false # Ignore Eureka dashboard FreeMarker templates thymeleaf: cache: false# Discovery Server Accesseureka: instance: hostname: localhost client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://root:123456@$&#123;eureka.instance.hostname&#125;:8761/eureka/# HTTP Serverserver: port: 2222 # HTTP (Tomcat) port main.java 12345678@SpringBootApplication@EnableDiscoveryClientpublic class OrderApplication &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(OrderApplication.class).web(true).run(args); &#125;&#125; 深入学习刷新配置git配置目录下的文件发生更改时，需要更新通知到服务，使用@RefreshScope可以帮助实现配置的刷新。 实现方式： 在指定的配置类下使用@RefreshScope，如若git配置发生变化，使用http://相应服务地址/refresh ,(POST)。 也可以： 重启服务在application.yml中启用endpoints.restart.enabled=true，调用http://相应服务地址/restart ,(POST)服务。 注-往往存在一些场景，refresh是不会生效的，因而，使用restart时比较保险的操作，但restart耗时比较长。故建议，在没有特殊的处理的配置类中使用@RefreshScope来实现refresh，存在比较复杂，且要求比较高的配置项，还是使用restart比较靠谱。 模式的匹配1234567891011121314spring:cloud: config: server: git: uri: https://github.com/spring-cloud-samples/config-repo repos: simple: https://github.com/simple/config-repo special: pattern: special*/dev*,*special*/dev* uri: https://github.com/special/config-repo local: pattern: local* uri: file:/home/configsvc/config-repo 当不存在pattern时，{application}/{profile}则根据key来决定，例如simple中，匹配的是simple/*，如local中，匹配的是local*/* 本地存储路径控制： 在使用的config服务的时候，其会clone一份缓存到本地，如果你要指定路径可以使用spring.cloud.config.server.git.basedir 使用本地加载配置文件： 需要配置：spring.cloud.config.server.native.searchLocations跟spring.profiles.active=native。路径配置格式：classpath:/, classpath:/config,file:./, file:./config。 基于文件的资源库： 在基于文件的资源库中(i.e. git, svn and native)，这样的文件名application*命名的资源在所有的客户端都是共享的(如 application.properties, application.yml, application-*.properties,etc.)。 加密与解密如果远程属性包含加密内容(以{cipher}开头),这些值将在通过HTTP传递到客户端之前被解密。 实现方式下载解压JCE，并复制至JDK/jre/lib/security文件夹下，Maven依赖”org.springframework.security:spring-security-rsa”。 环境配置config相关配置需要在bootstrap.yml中进行配置，在实际开发中存在调试环境，开发环境，测试环境，线上环境等场景，因而，对bootstrap.yml进行配置环境化配置是很必须的。 可以bootstrap-[environment].yml，默认是会读取bootstrap.yml和bootstrap-default.yml中的配置。若需要读取其他环境的配置，可在bootstrap.yml中设置 1234#spring环境和config中的配置都会使用该环境的配置spring: profile: active: environment 只是想更改config中的环境： 1234spring: cloud: config: profile: environment Tips如果config的客户端需要使用service-id这种负载均衡的方式获取config服务端的配置信息，需要注意将注册中心的信息和config服务的信息都写于bootstrap.yml下， 否则可能存在找不到config服务。 config中文文档]]></content>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Eureka]]></title>
    <url>%2F2017%2F06%2F29%2Fspringcloud_eureka20170629%2F</url>
    <content type="text"><![CDATA[Eureka是一个服务的注册中心，当然，其默认也是一个客户端 快速入门Eureka服务端pom.xml123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; application.yml12345678910111213141516spring: application: name: eureka-registerserver: port: $&#123;vcap.application.port:8761&#125; # HTTP porteureka: instance: hostname: localhost #与本地的hosts文件配置有关 client: registerWithEureka: false #表示是否将自己注册到Eureka Server，默认为true。 fetchRegistry: false #表示是否从Eureka Server获取注册信息，默认为true。 serviceUrl: defaultZone: http://localhost:$&#123;server.port&#125;/eureka/ #设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址。默认是http://localhost:8761/eureka ；多个地址可使用 , 分隔。 server: waitTimeInMsWhenSyncEmpty: 0 enableSelfPreservation: false Application.java 12345678@SpringBootApplication@EnableEurekaServer//启动使用EurekaServerpublic class SpringCloudEurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudEurekaApplication.class, args); &#125;&#125; Eureka客户端pom.xml 同上 bootstrap.xml 12345678eureka: instance: hostname: localhost client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:8761/eureka/ Application.java 12345678@SpringBootApplication@EnableEurekaClint//启动使用EurekaClientpublic class SpringCloudEurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudEurekaApplication.class, args); &#125;&#125; 深入学习高可用实现方案在eureka服务端yml配置文件 1234567891011121314151617spring: profiles: peer1eureka: instance: hostname: peer1 client: serviceUrl: defaultZone: http://peer2/eureka/---spring: profiles: peer2eureka: instance: hostname: peer2 client: serviceUrl: defaultZone: http://peer1/eureka/ 在hosts文件中进行host配置 12127.0.0.1 peer1127.0.0.1 peer2 启动程序 12java -jar microservice-eureka-1.0-SNAPSHOT.jar --spring.profiles.active=peer1java -jar microservice-eureka-1.0-SNAPSHOT.jar --spring.profiles.active=peer2 查看eureka服务地址，是否存在两个注册地址信息 注-第一个服务启动的时候存在java.net.ConnectException: Connection refused (Connection refused)的错误信息，不用理睬，只要最后服务端口显示就表示服务起来了。出现这个错误的原因是另一个服务还没起来，无法去对方服务进行注册，如果另一个服务起来之后，不存在问题了。 注-hostname必须是不一致的。 服务提供者注册到高可用注册中心123456eureka: client: serviceUrl: defaultZone: http://eureka1:8001/eureka/,http://eureka2:8002/eureka/ instance: preferIpAddress: true 服务提供者高可用第一种方式将服务部署在不同的两台服务器上，使用相同的端口 第二种方式将服务部署在相同的服务器上，使用不同的端口 消费服务调用123456eureka: client: serviceUrl: defaultZone: http://eureka1:8001/eureka/,http://eureka2:8002/eureka/ instance: preferIpAddress: true 如果使用了负载均衡配置情况下，会将请求分摊到两个服务提供者上。 健康检查和状态的路径123eureka: instance: instanceId: $&#123;spring.application.name&#125;:$&#123;vcap.application.instance_id:$&#123;spring.application.instance_id:$&#123;random.value&#125;&#125;&#125; 由于存在集群和同一台服务器部署多套服务的情况，所以需要一个唯一的身份ID，默认Spring Cloud也是有一套规则${spring.cloud.client.hostname}:${spring.application.name}:${spring.application.instance_id:${server.port}}}。例如 Myhost：myappname：8080 。但如果想进行自行定规则，就可以使用上面的属性配置。 其默认是client在注册成功之后，就认为status是UP的，不会频繁的去获取心跳，当设置了该属性为true时，才会去听取心跳。并且该配置必须只能在application.yml下面配置，不能使用bootstrap.yml。 12345eureka: instance: statusPageUrl: https://$&#123;eureka.hostname&#125;/info healthCheckUrl: https://$&#123;eureka.hostname&#125;/health homePageUrl: https://$&#123;eureka.hostname&#125;/ 添加密码服务端： pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; application.yml 1234security: user: name: root password: 123456 客户端： application.yml 12345678eureka: instance: hostname: localhost client: registerWithEureka: true fetchRegistry: true serviceUrl: defaultZone: http://$&#123;username&#125;：$&#123;password&#125;@$&#123;eureka.instance.hostname&#125;:8761/eureka/ Eureka.instance.prefer-ip-addresseureka.instance.prefer-ip-address=true以IP的形式注册register service，false是用所在机器名（hostname）。 eureka.instance.ip-address自定义IP地址，其优先级比eureka.instance.prefer-ip-address高。 ignored-interfaces场景问题： 服务器上分别配置了eth0, eth1和eth2三块网卡，只有eth1的地址可供其它机器访问，eth0和eth2的 IP 无效。在这种情况下，服务注册时Eureka Client会自动选择eth0作为服务ip, 导致其它服务无法调用。 忽略指定网卡 通过上面源码分析可以得知，spring cloud肯定能配置一个网卡忽略列表。通过查文档资料得知确实存在该属性： 1spring.cloud.inetutils.ignored-interfaces[0]=eth0 # 忽略eth0, 支持正则表达式11 因此，第一种方案就是通过配置application.properties让应用忽略无效的网卡。 配置host 当网查遍历逻辑都没有找到合适ip时会走JDK的InetAddress.getLocalHost()。该方法会返回当前主机的hostname, 然后会根据hostname解析出对应的ip。因此第二种方案就是配置本机的hostname和/etc/hosts文件，直接将本机的主机名映射到有效IP地址。 手工指定IP(推荐) 添加以下配置： 1234# 指定此实例的ipeureka.instance.ip-address=# 注册时使用ip而不是主机名eureka.instance.prefer-ip-address=true 服务续约在注册完毕之后，会在每过一个间隔期，就会向注册中心续约，表示服务还活着。 1234eureka: instance: lease-renewal-interval-in-secondes: 30 #服务续约任务的调用间隔时间 lease-expiration-duration-in-seconds: 90 #定义服务失效的时间 也就是说，默认是每隔30秒应用向注册中心进行续约操作，当服务续约时间间隔超过90秒，注册中心就任务其服务已经失效。 服务消费者获取服务清单 服务消费者会在启动的使用通过rest请求获取一份服务清单（feth-registy=true时），并缓存30秒，之后再次向注册中心获取。 123eureka: client: registry-fetch-interval-seconds: 30 当Ribbon与eureka连用时，Ribbon的服务清单RibbonServiceList会被DiscoveryEnableNIWSServiceList重写，拓展成了Eureka注册中心获取服务列表。同时，NIWSDiscoveryPing来取代IPing，其职责为确认服务是否启动。 服务调用 服务在获取服务列表中包含了其实例名和该实例的元数据。默认Ribbon是通过轮询的方式进行调用，以达到负载均衡的效果。 服务下线 当eureka的客户端服务进行正常下线时，其会向注册中心发一个rest请求，告知注册中心服务下线，同时，注册中心也会通知其他服务该服务下线消息。 服务注册中心失效剔除 如果是非正常服务停止的情况，注册中心会每60秒定时去清除清单中超时续约（90s）的服务。 自我保护 在注册中心运行期间，会统计心跳失败的比例在15分钟内是否低于85%，如果出现低于的情况，会将当前的实例注册信息保护起来，让这些实例不会过期。从而会导致一个问题就是客户端调用已经失效的服务，会出现错误，所以需要容错机制。 所以在本地开发的时候可以关闭保护机制， 123eureka: server: enable-self-preservation: false 随机数12345$&#123;random.int[10000,19999]&#125;$&#123;random.int(100)&#125; #限制生成的数字小于10$&#123;random.int[0,100]&#125; #指定范围的数字$&#123;random.value&#125; #随机字符串$&#123;random.long&#125; #随机long 实例名配置在使用服务高可用的时候，同一个服务的不同实例是根据主机名进行区分的，这样就会导致一台机子无法部署多台服务，解决这个问题的一种方式是通过更改服务的端口号，另一种是通过使用不同的实例名。 123eureka: instance: instanceId: $&#123;spring.application.name&#125;:$&#123;random.int&#125; 健康检查在eureka中的健康情况是根据服务提供者的心跳情况，只要续约正常，health一直是up状态。 使用需要spring-boot-actuator模块的/health端点。 1234eureka: client: healthcheck: enabled: true 自定义Instance ID在Spring Cloud中，服务的Instance ID的默认值是${spring.cloud.client.hostname}:${spring.application.name}:${spring.application.instance_id:${server.port}} 。如果想要自定义这部分的内容，只需在微服务中配置eureka.instance.instance-id 属性即可，例如： 123456spring: application: name: service-usereureka: instance: instance-id: $&#123;spring.cloud.client.ipAddress&#125;:$&#123;server.port&#125; # 将Instance ID设置成IP:端口的形式 instanceID初始化代码 123org.springframework.cloud.netflix.eureka.EurekaClientAutoConfigurationorg.springframework.cloud.commons.util.IdUtils.getDefaultInstanceId(PropertyResolver)org.springframework.cloud.netflix.eureka.EurekaInstanceConfigBean.getInstanceId() Tips服务在启动时，会通过rest请求方式注册到eureka。注册信息是通过双层Map进行存储，第一层key为服务名，第二层的key为具体的实例名称。 other settings123eureka: instance: preferIpAddress: true #实例名称显示IP 1234eureka: client: healthcheck: enabled: true http://www.cnblogs.com/yangfei-beijing/p/6379258.html 源码学习EndpointUtils 获取serviceUrl 123456// 获取配置方式和DNS方式获取ServiceUrl地址getDiscoveryServiceUrls();// 遍历所有zone，获取所有ServiceUrl地址，将preferSameZone为true时，将instanceZone下的ServiceUrl放在最前面，也就是同一个与客户端同一个zone的ServiceUrlgetServiceUrlsFromConfig();// 获取ServiceUrl地址，其实现是EurekaClientConfigBean类getEurekaServerServiceUrls()； EurekaClientConfigBean 该类包含了eureka.client的配置。包括一些定时任务配置。 12// 获取ServiceUrl地址getEurekaServerServiceUrls()； com.netflix.discovery.DiscoveryClient 最重要的类，定时任务和注册到注册中心等操作都是在这里完成的。 123456// 这里启用服务获取，服务续约，服务注册等定时任务，同时InstanceInfoReplicator对象中run方法中的discoveryClient.register();进行了服务的注册initScheduledTasks();// 进行服务的获取，存在全量和非全量两种fetchRegistry();// 进行心跳的维持HeartbeatThread.renew(); EurekaInstanceConfigBean 服务实例配置]]></content>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——策略模式]]></title>
    <url>%2F2017%2F06%2F24%2Fstrategy201706024%2F</url>
    <content type="text"><![CDATA[策略模式是为了解决在做同一件事的时候，存在多种可能发生情况问题。 什么是策略模式 什么时候使用 如何使用 优缺点是什么 使用场景一个商场中，针对不同的消费者，进行不同的消费打折促销，普通消费者打9.8折，VIP用户打8折，SVIP用户打7.5折，针对打折这件事，存在三种情况需要考虑，针对不同的人，使用不同的计算方式。这里就要使用策略模式去解决。 要素 针对问题的一个接口 接口的多种策略实现 一个接口的调用方 使用12345678/** * Created by 迹_Jason on 2017/6/24. * 策略模式接口 */public interface Discount &#123; Double discountMoney(Double total);&#125; 123456789/** * Created by 迹_Jason on 2017/6/24. * 普通消费者打折力度 */public class CommonConsumerDiscount implements Discount &#123; public Double discountMoney(Double total) &#123; return total * 0.98; &#125;&#125; 123456789/** * Created by 迹_Jason on 2017/6/24. * svip打折力度 */public class SvipConsumerDiscount implements Discount &#123; public Double discountMoney(Double total) &#123; return total * 0.75; &#125;&#125; 123456789/** * Created by 迹_Jason on 2017/6/24. * vip打折力度 */public class VipConsumerDiscount implements Discount &#123; public Double discountMoney(Double total) &#123; return total * 0.8; &#125;&#125; 1234567891011121314151617/** * Created by 迹_Jason on 2017/6/24. * 调用策略接口 */public class SumMoney &#123; Double sum(Discount consumerType) &#123; Double total = 7 * 2 + 5.3 * 8; return consumerType.discountMoney(total); &#125; public static void main(String[] args) &#123; Discount consumerType = new CommonConsumerDiscount(); SumMoney sumMoney = new SumMoney(); System.out.println(sumMoney.sum(consumerType)); &#125;&#125; 优点程序更加灵活，拓展性更好，可读性也更好 可以替换if-else语句的使用 缺点必须知道有哪些策略类，每个策略类的功能 如果策略很多，产生的策略实现类也非常多]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git实战]]></title>
    <url>%2F2017%2F06%2F20%2Fgit20170303%2F</url>
    <content type="text"><![CDATA[学习Git有一段时间了，一路上也一直在写有关于Git方面的文章，但总觉得不是我想要的，就是感觉有点肉肉的，不够直接，不够马上出效果，所以才有了这篇文章，当然这文章可能会不断更新和修正，希望读者可以作为一个工具文章使用，我也会努力将其优化的更加的符合工作场景 说明 1、在[…]中的内容，需要根据实际情况进行修改 如何本地与远程建立信任联系？要解决这个问题，需要使用SSH秘钥的方式，接下来我就讲一下怎么进行配置。 1.在本地生成私钥和公钥 123git config --global user.name "username"//用户昵称git config --global user.email "emailAddress"//github或者gitlib的邮箱地址ssh-keygen -t rsa -C "emailAddress"//github或者gitlib邮箱账户地址 2.将电脑上的公钥与远程仓库进行绑定 本地会在上面的步骤中会生成一个id_rsa.pub（默认情况是这个名称），将该文件中的内容copy到远程仓库github或gitlib的settings的SSH配置选项中。 Tittle名称自由发挥 克隆工程 将远程的数据复制一份到本地 12#【仓库copy地址】git clone [git@rep.xx.com:zoeminghong/hello.git] [工程别名] 本地新建Git工程 现在打算将本地的工程，放到Git仓库进行托管了，并且远程Git仓库已经创建了该项目的工程 123456789101112131415161718#本地初始化工程，会生成一个.git文件git init #将本地的工程与远程仓库中的项目进行关联（不用关心项目名不一致的问题）#此时本地工程与远程仓库已经建立了联系git remote add origin [git@rep.xx.com:zoeminghong/hello.git]#去除远程仓库绑定git remote remove origin #将本地所有文件添加到Git中，进行监管git add . #将内容提交 【提交注释】git commit -m "[...]"#将本地的内容同步到远程仓库中git push -u origin master 显示某一个特定的提交的日志1git show [十六进制码] 查看提交图1git log --graph --pretty=oneline --abbrev-commit 查看冲突未处理的文件列表1git ls-files -u 本地代码与远程代码冲突问题 本地代码未commit的前提下，解决与远程代码冲突问题 123456789101112131415161718192021git pull #失败#将当前修改进行暂存起来git stash#或git stash save "[注释]" #获取最新的远程仓库代码git pull #恢复暂存的内容git stash pop#stash其他操作#恢复最近一次save的原工作区内容,，并删除stash记录git stash pop#恢复最近一次save的原工作区内容,但不删除stash记录git stash apply [指定版本]#删除stash记录git stash drop#获取暂存列表git stash list 但，上面的也可能存在问题，由于本地存在未被追踪的文件，并且远程仓库pull的时候也存在同名的文件，就会存在pull失败，在这种情况下，在git stash后面追加 --include-untracked，会将远程的文件与本地的文件融合 stash只会保存当前索引和工作目录的状态，其保存的是add和commit的中间状态，如果还没有被git追踪的文件，是不会被记录的 stash只保存被修改的文件内容，未被修改的文件内容不会被记录，在apply恢复的时候，也只会更新 stash 时被保存的内容 如果我对某文件进行了修改，但我不想要push到远程仓库，同时我又想获取最新的修改记录12git stash savegit pull --rebase 如果暂存内容现在不想在当前分支恢复了，而是想单独起一个分支1git stash branch [newBranchName] 想要查看当前工作区与暂存状态内容区别1git stash show -p stash&#123;0&#125; 本地代码已经commit后，解决与远程代码冲突问题 12345678# 获取远端库最新信息 【分支名称】git fetch origin [master] # 做比较git diff [本地分支名] origin/[远程分支名] # 拉取最新代码，同时会让你merge冲突git pull 方法2 123456789101112131415161718192021# 获取最新代码到tmp分支上 [远程的分支:本地分支]git fetch origin [master:tmp] # 当前分支与tmp进行比较git diff tmp # 修改冲突部分，进行本地commit操作git add . git commit -m "[...]" # 将tmp中内容合并到当前分支中git merge tmp # 删除分支git branch -d tmp 删除文件 保留副本操作 1git rm --cache [文件名] 直接文件删除 1git rm [文件名] 后悔药 还原到最近的版本，废弃本地做的修改（当前文件修改没有进行add操作的时候） 1git checkout -- [文件名] 取消已经暂存的文件(撤销先前”git add”的操作) 12#当前HEAD，返回到上一次commit点，不会有任何日志记录git reset HEAD --hard 1git reset HEAD [文件名] 回退所有内容到上一个提交点 12#最近内容已经commit的情况下git reset HEAD^ --hard 回退这个文件的版本到上一个版本 12#最近内容已经commit的情况下git reset HEAD^ [文件名] 将本地的状态回退到和远程的一样 1git reset –-hard origin/[分支名] 回退到某个版本 123# 获取所有的HEAD更改信息的sha1值git refloggit reset [SHA1] 回退到上一次提交的状态，按照某一次的commit完全反向的进行一次commit.(代码回滚到上个版本，并提交git) 1git revert HEAD 使用reset是不会有日志记录的，revert则会要提交一个记录点 修改最新的提交信息(修改提交的注释信息) 1git commit --amend reset与revert的使用说明 reset一般用于本地还没有push到远端的时候，revert则用于想要远端也进行记录回退操作的时候，也就是说在push之后。 1、如果你已经push到远程仓库，reset删除指定commit以后,你git push可能导致一大堆冲突.但是revert 并不会 2、如果现有分支和历史分支需要合并的时候,reset 恢复部分的代码依然会出现在历史分支里.但是revert 方向提交的commit 并不会出现在历史分支里。 本地分支与远程分支相连 本地创建了一个分支，远程也有一个分支，进行两者关联 1git checkout -b [本地分支名] origin/[远程分支名] Tag使用 我们在开发的时候，可能存在线上发布了一个版本，需要给这个版本代码打上一个标签，到时候可以方便回退到这个版本 12345678910# 创建tag 【tag名】git tag v1.0(git tag -a [v1.4] -m ['my version 1.4']) # 查看存在的taggit tag(git tag -l ['v1.4.2.*']) # 将tag更新到远程，直接的push是不会将tag同步上去git push origin --tags 接下来就讲解回退到具体的tag 123456789101112# 保存当前编程环境git stash # 切换回某个tag（v1.0）git show v1.0 #【sha1】git reset --hard [2da7ef1] # 创建分支来保存tag的数据，tag只是一个节点的标记，无法承载数据的修改记录,【分支名】git checkout -b [branchName] [tagName] # 接着你就可以在这里改啊改了 切换回主干或其他分支 1234567891011121314# 切换分支git checkout master # 日志记录git reflog # 显示stash列表git stash list # 恢复之前的工作环境代码git stash apply # 删除stashgit stash drop 分支与主干合并 12345678git add .git commit -m "v1.1"git checkout [bill]# master分支最新的代码合并到bill分支上git rebase [master]git checkout master# bill分支合并到当前分支【分支名】git merge [bill] 关于代码的比较12345678910111213141516171819202122232425262728# 显示暂存区和工作区的差异git diffgit diff [filename] # 显示暂存区和上一个commit的差异【文件名】git diff --cached [hello.txt]git diff --cached [HEAD或者SHA1] [filename] # 显示工作区与当前分支最新commit之间的差异git diff HEADgit diff [HEAD或分支名] [filename] # 显示两次提交之间的差异【分支名】git diff [first-branch]...[second-branch]git diff [SHA1] [SHA1] [filename]#分支之间的差异#分支之间的差异git diff [分支1] [分支2]git diff [分支1]..[分支2]#指定文件git diff [分支1]:[file1] [分支2]:[file2]#查看指定提交范围内的所有变更文件情况git diff --stat master~[范围值] [分支名]git diff --stat master~5 tmp//还可以值查看具体某一个文件git diff --stat master~5 tmp test.txt 定位哪个提交点导致文件出现问题12345678910111213141516#先确定范围git bisect startgit bisect bad #一般都是当前HEAD是坏提交【sha1】git bisect good a794f9bd96f06b57b4c10433e4d6abb3f0855749 #上面的步骤就是确定范围的，接下来就是回答git的问题，他指定的提交点是好的还是坏的git bisect good//如果是坏的，就bad，直到你找到哪个提交点导致出现问题#查看维护日志git bisect loggit branch#完成操作后，要回切到工作分支上git bisect resetgit branchgit reset --hard fb47ddb2db... 检查文件中每一行代码是谁提交的记录1git blame -L [起始行数],[文件名] 创建分支123456#以当前节点作为分支的开始起点git branch [分支名]#以SHA1作为分支开始起点git branch [分支名] [SHA1]#创建并切换分支,sha1以哪个节点作为分支的起点git checkout -b [分支名] [SHA1] 拉去远程仓库分支代码1234git checkout 1.0git pull#或者git checkout 1.0 origin/1.0 开发的过程中生成新分支12345#因可能存在未被git监管和未提交的内容，需要将未提交的内容进行监管和暂存git add .git stash#包含[SHA1]及之前的代码会被copy盗分支上git branch [分支名] [SHA1] 重命名分支 在git中重命名远程分支，其实就是先删除远程分支，然后重命名本地分支，再重新提交一个远程分支。 12345678//显示现在分支git branch -av//删除远程要删除的分支develgit push --delete origin devel//重命名本地分支devel为developgit branch -m devel develop//推送到远程git push origin develop 这是由于在 github 中，devel 是项目的默认分支。要解决此问题，这样操作： 进入 github 中该项目的 Settings 页面； 设置 Default Branch 为其他的分支（例如 master）； 重新执行删除远程分支命令。 查看远程仓库分支1git branch -a 根据远程仓库分支代码创建分支1git fetch origin [远程仓库分支名:远程仓库分支名] [本地仓库分支名:本地仓库分支名] 删除本地分支1git branch -d [分支名] 删除远程仓库分支1git remote remove [分支名] 查看分支123git show-branch#或git branch 分支前面都存在*或者! *表示当前分支 在–之后的是记录分支的提交信息 像*+ [tmp] 远程2就表示该提交存在于两个分支中 显示某分支中某文件内容1git show [分支名]:[文件名] 显示某个节点某文件的内容1git show [SHA1] [文件名] 查看本地Git绑定的远程仓库信息1git remote -v 关于切换分支的逻辑 如果存在未被git追踪的文件，git是会将其忽略的 如果存在已追踪且被修改或删除，必须commit之后，才能切换 如果要不计后果的情况，强切，加-f 将当前的分支修改的内容同步到其他的分支上假如你希望变更作用于另一个分支上，但由于当前分支如果不提交，是无法切换到另一个分支上的 1git checkout -m [另一个分支名] 将一个区间的提交，移植到另一个分支12#当前分支，得到dev分支中dev~2之前的所有提交内容git cherry-pick dev~2 cherry-pick会生成一条新的提交记录 代码行数统计统计某人的代码提交量，包括增加，删除： 1git log --author="$(git config --get user.name)" --pretty=tformat: --numstat | gawk '&#123; add += $1 ; subs += $2 ; loc += $1 - $2 &#125; END &#123; printf "added lines: %s removed lines : %s total lines: %s\n",add,subs,loc &#125;' - 统计某人一个月内的代码提交量，包括增加，删除： 1git log --since=1.month.ago --author="$(git config --get user.name)" --pretty=tformat: --numstat | gawk '&#123; add += $1 ; subs += $2 ; loc += $1 - $2 &#125; END &#123; printf "added lines: %s removed lines : %s total lines: %s\n",add,subs,loc &#125;' - 仓库提交者排名前 5（如果看全部，去掉 head 管道即可）： 1git log --pretty='%aN' | sort | uniq -c | sort -k1 -n -r | head -n 5 仓库提交者（邮箱）排名前 5：这个统计可能不会太准，因为很多人有不同的邮箱，但会使用相同的名字 1git log --pretty=format:%ae | gawk -- '&#123; ++c[$0]; &#125; END &#123; for(cc in c) printf "%5d %s\n",c[cc],cc; &#125;' | sort -u -n -r | head -n 5 贡献者统计： 1git log --pretty='%aN' | sort -u | wc -l 提交数统计： 1git log --oneline | wc -l git log 参数说明： –author 指定作者–stat 显示每次更新的文件修改统计信息，会列出具体文件列表–shortstat 统计每个commit 的文件修改行数，包括增加，删除，但不列出文件列表：–numstat 统计每个commit 的文件修改行数，包括增加，删除，并列出文件列表： -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新​ 例如：git log -p -2–name-only 仅在提交信息后显示已修改的文件清单–name-status 显示新增、修改、删除的文件清单–abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符–relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）–graph 显示 ASCII 图形表示的分支合并历史–pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）​ 例如： git log –pretty=oneline ; git log –pretty=short ; git log –pretty=full ; git log –pretty=fuller–pretty=tformat: 可以定制要显示的记录格式，这样的输出便于后期编程提取分析​ 例如：git log –pretty=format:””%h - %an, %ar : %s””​ 下面列出了常用的格式占位符写法及其代表的意义。​ 选项 说明​ %H 提交对象（commit）的完整哈希字串​ %h 提交对象的简短哈希字串​ %T 树对象（tree）的完整哈希字串​ %t 树对象的简短哈希字串​ %P 父对象（parent）的完整哈希字串​ %p 父对象的简短哈希字串​ %an 作者（author）的名字​ %ae 作者的电子邮件地址​ %ad 作者修订日期（可以用 -date= 选项定制格式）​ %ar 作者修订日期，按多久以前的方式显示​ %cn 提交者(committer)的名字​ %ce 提交者的电子邮件地址​ %cd 提交日期​ %cr 提交日期，按多久以前的方式显示​ %s 提交说明–since 限制显示输出的范围，​ 例如： git log –since=2.weeks 显示最近两周的提交​ 选项 说明​ -(n) 仅显示最近的 n 条提交​ –since, –after 仅显示指定时间之后的提交。​ –until, –before 仅显示指定时间之前的提交。​ –author 仅显示指定作者相关的提交。​ –committer 仅显示指定提交者相关的提交。​​ 一些例子： git log –until=1.minute.ago // 一分钟之前的所有 log git log –since=1.day.ago //一天之内的log git log –since=1.hour.ago //一个小时之内的 log git log –since=`.month.ago –until=2.weeks.ago //一个月之前到半个月之前的log gitlog –since ==2013-08.01 –until=2013-09-07 //某个时间段的 log git blame看看某一个文件的相关历史记录​ 例如：git blame index.html –date short 子模块保留子组件的现有目录结构的完整性，故而git创造了类似于maven中的module一样的功能，来实现子模块的管理 打个比方：现在我有一个父工程A，其工程路径下面有五个子工程BCDEF，按照往常是要git clone 6个工程才可以完全下载成功，而且工程与工程之间的路径关系也不知道。现在有了gitmodules就可以直接下载A工程就可以了，其他的5个子工程都会自动下载。 文件.gitmodules存放在父工程根目录下 12345678910[submodule "others/B"] path = others/B url = https://rep.XXX.com/crm/B.git[submodule "sources/C"] path = sources/C url = https://rep.XXX.com/crm/C.git[submodule "sources/D"] path = sources/D url = https://rep.XXX.com/crm/D.git..... 添加子模块 1git submodule add [远程仓库地址] [相对于父模块的相对路径] 注： 直接手动更改gitmodule文件是没有用的哦 远程仓库地址要先于子模块之前准备好 子模块的名称是可以与[相对于父模块的相对路径]不一致的 创建完成以后会生成.gitmodules与.gitattributes这两个文件 .gitmodules和.git/config保存着子模块的信息 从远程仓库获取所有模块数据 12345678910111213#方式一git clone --recursive [远程仓库地址]#方式二git clone [远程父仓库地址]cd [父模块路径]git submodule initgit submodule update或者git submodule update --init --recursivecd [目标submodule]git branch -agit fetch origin [remote branch:remote branch] [new local branch:new local branch]git checkout [new local branch] 删除子模块功能 先清空.gitmodules中的内容 再执行文件的删除 ———— 其他命令 12# .gitmodules中子模块的内容更新到.git/config中git submodule init gitmodules参考 关闭issue格式Fixes #45，45是issue的ID，在相应的issue的链接地址就有。 可以关闭issue的关键字： close closes closed fix fixes fixed resolve resolves resolved 不同的仓库中关闭issue 格式close username/repository#issue_number 关闭多个issues 格式closes #34, closes #23, and closes example_user/example_repo#42 Fork&amp;Pull Request12345678910# 1、目标仓库A 页面 Fork# 2、git clone 在自己仓库的Fork代码到本地# 3、绑定数据源git remote add upstream [fork源的远程仓库地址]# 4、更新分支代码git pull upstream master# 5、commit&amp;push到自己的仓库git push origin master# 6、pull request# 7、等待对方同意 git停留在writing objects1git config --global http.postBuffer 524288000 http.postBuffer默认上限为1M,上面的命令是把git的配置里http.postBuffer的变量改大为500M 文件大,上传慢1git remote set-url origin [你的ssh地址] 把远程仓库的上传由原来的HTTPS改为SSH上传，github在国内本身就慢，还可能被dns污染 , 因此推荐使用SSH上传]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wagon插件使用]]></title>
    <url>%2F2017%2F06%2F20%2Fwagon20170620%2F</url>
    <content type="text"><![CDATA[wagon是maven插件中的一种，其作用是去除我们部署时繁复的步骤，不用再手动上传jar包或者war包到指定服务器路径下面。 问题有什么用？ 怎么用？ 有什么优势？ 使用pom.xml 12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;wagon-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;configuration&gt; &lt;!--这里的id与之后的setting.xml配置项中的要一致 --&gt; &lt;serverId&gt;demo-test&lt;/serverId&gt; &lt;!--本地包路径 --&gt; &lt;fromDir&gt;target/&lt;/fromDir&gt; &lt;!--上传哪些包 --&gt; &lt;includes&gt;$&#123;project.artifactId&#125;.jar&lt;/includes&gt; &lt;!--包保存到服务器哪个路径下面，注意是scp，要支持这个命令才行 --&gt; &lt;url&gt;scp://172.168.1.11:22/opt/micservice&lt;/url&gt; &lt;!--服务器目标路径，与url配合使用 --&gt; &lt;toDir&gt;apps/$&#123;project.artifactId&#125;&lt;/toDir&gt; &lt;!--因为存在服务器的重启之类的操作，wagon也支持使用shell命令，可以有多个command标签哦，根据先后顺序执行--&gt; &lt;commands&gt; &lt;command&gt;cd /opt/micservice/ ; ./dev restart $&#123;project.artifactId&#125; test&lt;/command&gt; &lt;/commands&gt; &lt;!-- 显示运行命令的输出结果 --&gt; &lt;displayCommandOutputs&gt;true&lt;/displayCommandOutputs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt; &lt;artifactId&gt;wagon-ssh&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt;&lt;/build&gt; setting.xml 12345678&lt;servers&gt; &lt;server&gt; 与上面的serverId一致 &lt;id&gt;demo-test&lt;/id&gt; &lt;username&gt;username&lt;/username&gt; &lt;password&gt;password&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 配置完成后，运行命令： 1mvn clean package wagon:upload wagon:sshexec wagon:upload-single是上传jar或者war包 wagon:sshexec是执行配置中的shell命令 如果不想执行上面的这么长串命令，也可以使用execution 1234567891011121314151617181920212223242526272829303132333435363738&lt;build&gt; &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt; &lt;artifactId&gt;wagon-ssh&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;wagon-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;upload-deploy&lt;/id&gt; &lt;!-- 运行package打包的同时运行upload-single和sshexec--&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;upload-single&lt;/goal&gt; &lt;goal&gt;sshexec&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;serverId&gt;mylinuxserver&lt;/serverId&gt; &lt;fromFile&gt;target/javawebdeploy.war&lt;/fromFile&gt; &lt;url&gt;scp://192.168.20.128/coder/tomcat/apache-tomcat-7.0.55/webapps&lt;/url&gt; &lt;commands&gt; &lt;command&gt;sh /coder/tomcat/apache-tomcat-7.0.55/bin/shutdown.sh&lt;/command&gt; &lt;command&gt;rm -rf /coder/tomcat/apache-tomcat-7.0.55/webapps/javawebdeploy&lt;/command&gt; &lt;command&gt;sh /coder/tomcat/apache-tomcat-7.0.55/bin/startup.sh&lt;/command&gt; &lt;/commands&gt; &lt;displayCommandOutputs&gt;true&lt;/displayCommandOutputs&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 以上mvn clean package 来代替 mvn clean package wagon:upload-single wagon:sshexec，但个人觉得不是特别适合开发时期，我觉得还是使用命令的方式执行，或者通过自己写一个shell文件，需要的时候执行一下即可。 Tips关于一些goals wagon:upload-single uploads the specified file to a remote location. wagon:upload uploads the specified set of files to a remote location. wagon:download-single downloads the specified file from a remote location. wagon:download downloads the specified set of files from a remote location. wagon:list lists the content of a specified location in a remote repository. wagon:copy copies a set of files under a Wagon repository to another. wagon:merge-maven-repos merges , including metadata, a Maven repository to another. wagon:sshexec Executes a set of commands at remote SSH host. 其他的标签 skip属性，boolean类型，默认是true，作用是忽略execution 优势 去除了发布的重复动作，直接一个命令就可以完成 将人为的错误降低了，工作更加高效 链接 官方文档]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在场景中使用Git]]></title>
    <url>%2F2016%2F11%2F08%2Fgit20161108%2F</url>
    <content type="text"><![CDATA[说明 1、在[…]中的内容，需要根据实际情况进行修改 1、克隆工程 将远程的数据复制一份到本地 12#【仓库copy地址】git clone [git@rep.xx.com:zoeminghong/hello.git] 2、本地新建Git工程 现在打算将本地的工程，放到Git仓库进行托管了，并且远程Git仓库已经创建了该项目的工程 123456789101112131415#本地初始化工程，会生成一个.git文件git init#将本地的工程与远程仓库中的项目进行关联（不用关心项目名不一致的问题）#此时本地工程与远程仓库已经建立了联系git remote add origin [git@rep.xx.com:zoeminghong/hello.git]#将本地所有文件添加到Git中，进行监管git add .#将内容提交 【提交注释】git commit -m "[...]"#将本地的内容同步到远程仓库中git push -u origin master 3、本地代码与远程代码冲突问题 本地代码未commit的前提下，解决与远程代码冲突问题 12345678#将当前修改进行暂存起来git stash#获取最新的远程仓库代码git pull#恢复暂存的内容git stash pop 本地代码已经commit后，解决与远程代码冲突问题 12345678# 获取远端库最新信息 【分支名称】git fetch origin [master]# 做比较git diff [本地分支名] origin/[远程分支名]# 拉取最新代码，同时会让你merge冲突git pull 方法2 12345678910111213141516# 获取最新代码到tmp分支上 [远程的分支:本地分支]git fetch origin [master:tmp]# 当前分支与tmp进行比较git diff tmp# 修改冲突部分，进行本地commit操作git add .git commit -m "[...]"# 将tmp中内容合并到当前分支中git merge tmp# 删除分支git branch -d tmp 4、回退到上一个commit节点，无log记录 当前内容没有commit的情况下 12# 当前HEAD，返回到上一次commit点，不会有任何日志记录git reset HEAD --hard 最近内容已经commit的情况下 1git reset HEAD^ --hard 5、回退到上一个commit节点，存在log记录 当前内容没有commit的情况下 12# 当前HEAD，返回到上一次commit点git revert HEAD 最近内容已经commit的情况下 1git revert HEAD^ 6、切换到指定commit节点 不存在log记录 12345# 获取所有的HEAD更改信息的sha1值git reflog# 切换至指定的sha1节点git reset --hard [sha1值] 7、删除文件 保留副本操作 1git rm --cache [文件名] 还原操作 1git reset HEAD [文件名] 直接文件删除 1git rm [文件名] 还原操作 123git reset HEAD [文件名]git checkout -- [文件名] 8、本地分支与远程分支相连 本地创建了一个分支，远程也有一个分支，进行两者关联 1git checkout -b [本地分支名] origin/[远程分支名] 9、Tag使用 我们在开发的时候，可能存在线上发布了一个版本，需要给这个版本代码打上一个标签，到时候可以方便回退到这个版本 12345678# 创建tag 【tag名】git tag v1.0# 查看存在的taggit tag# 将tag更新到远程git push origin --tags 接下来就讲解回退到具体的tag 12345678910111213# 保存当前编程环境git stash# 切换回某个tag（v1.0）git show v1.0 #【sha1】git reset --hard [2da7ef1]# 创建分支来保存tag的数据，tag只是一个节点的标记，无法承载数据的修改记录,【分支名】git checkout -b [bill]# 接着你就可以在这里改啊改了 切换回主干或其他分支 1234567891011121314# 切换分支git checkout master# 日志记录git reflog# 显示stash列表git stash list# 恢复之前的工作环境代码git stash apply# 删除stashgit stash drop 分支与主干合并 123456git add .git commit -m "v1.1"# bill分支合并到当前分支【分支名】git merge [bill] 10、关于代码的比较1234567891011# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异【文件名】$ git diff --cached [hell.txt]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异【分支名】$ git diff [first-branch]...[second-branch]]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手叫你一台电脑配置两个Git账户]]></title>
    <url>%2F2016%2F09%2F28%2Fgit20160928%2F</url>
    <content type="text"><![CDATA[假设环境 帐号一：github 帐号二：gitlab 配置帐号一生成ssh密钥1234561、安装Git软件2、在桌面打开Git，进行设置git config --global user.name "userName"//github的帐号名(也可以自定义)git config --global user.email "emailAdress"//github的邮箱地址(也可以自定义)3、生成ssh密钥ssh-keygen -t rsa -C "githubEmailAdress"//github邮箱地址 一路的回车键 github网站配置在github上将id_rsa.pub文件内容添加上（怎么添加请百度） 配置帐号二生成ssh密钥123ssh-keygen -t rsa -C "gitlabEmailAdress" # 设置名称为id_rsa_gitlabEnter file in which to save the key (/c/Users/Administrator/.ssh/id_rsa): id_rsa_gitlab 在.ssh 路径下面会生成一个id_rsa_gitlab文件 新密钥添加到SSH agent中12ssh-agent bashssh-add ~/.ssh/id_rsa_work 创建一个config文件（没有文件后缀名）123456789101112# 该文件用于配置私钥对应的服务器# Default github userHost githubHostName github.comUser gitIdentityFile C:/Users/Administrator/.ssh/id_rsa# second userHost gitlabHostName your gitlab host addressUser gitIdentityFile C:/Users/Administrator/.ssh/id_rsa_gitlab gitlab网站配置将id_rsa_gitlab.pub中的内容添加到gitlab帐号下的SSH Key中 测试1234567$ ssh -T githubHi jj! You've successfully authenticated, but GitHub does not provide shell access.$ ssh -T gitlabHi jj! You've successfully authenticated, but GitHub does not provide shell access. 这里的github与gitlab就是config文件中的Host值 运用正常情况我们要clone一个github工程是这样的 1git@github.com:jj/JForm.git 如今在github工程是这样的 1git clone github:jj/JForm.git 原因就是config 还有一个问题就是提交的邮箱地址和用户名是根据下面配置 12git config --global user.name "userName"//github的帐号名(也可以自定义)git config --global user.email "emailAdress"//github的邮箱地址(也可以自定义)]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用NodeJS调用Dubbo工程]]></title>
    <url>%2F2016%2F07%2F27%2Fnode20160727%2F</url>
    <content type="text"><![CDATA[使用node-zookeeper-dubbo模块进行处理，官方的文档还是有些简陋，我进行进一步的讲解 1、安装node-zookeeper-dubbo12npm install node-zookeeper-dubbo --savenpm install request --save 2、示例1234567891011121314151617181920212223var request = require('request');var Service=require('node-zookeeper-dubbo'); var opt=&#123; env:'1.0.1', // dubbo服务端版本号 group:'test', // dubbo group（Product，Dev，Test） conn:'10.10.13.11:2181', // zookeeper url path:'com.che.app.service.IAppFinanceService' // service接口地址 //version:'1.0.1' // dubbo的版本号，可以省略 &#125;; var method="getAssetInfoByPhone";//方法名 var arg1=&#123;$class:'java.lang.String',$:'13100000001'&#125;//参数1 var arg2=&#123;$class:'int',$:12208&#125;//参数2 var args=[arg1,arg2];//参数数组 var service = new Service(opt);//连接zk service.excute(method,args,function(err,data)&#123; if(err)&#123; console.log(err); return; &#125; console.log(data) &#125;); 参数讲解 对于基本数据类型直接填写：int，short，long，boolean，float，double。 对于非基本类型需要完整的类路径名：java.lang.String等。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Canvas基础积累]]></title>
    <url>%2F2016%2F07%2F12%2Fcanvas20160712%2F</url>
    <content type="text"><![CDATA[创建canvas1&lt;canvas id="canvas"&gt;&lt;/canvas&gt; 一般都要存在ID width和height建议直接在&lt;canvas&gt;中直接设定，不要使用css的方式去设定长和高，并且其是没有px单位的，因其也表示精度 width和height还可以使用JS的方式进行设定，在Element对象中存在width和height属性，可以进行设定 获取Canvas对象12var canvas=document.getElementById("canvas");var context= canvas.getContext(); context就是canvas对象，接下来都是对其进行操作 设置width和height 12canvas.width = 1024;canvas.height = 768; 直线123456context.strokeStyle="#005588";//设置颜色，red这种方式也是可以的context.lineWidth=5;//线宽context.moveTo(100,100);//表示画笔的起点坐标context.lineTo(700,700);//表示下一笔的坐标context.lineTo(1000,700);//表示下一笔的坐标context.stroke();//绘制线条 多边形12345context.fillStyle="rgb(2,100,30)";//颜色context.moveTo(100,100);//表示画笔的起点坐标context.lineTo(700,700);//表示下一笔的坐标context.lineTo(1000,700);//表示下一笔的坐标context.fill(); 123context.fillStyle="rgb(2,100,30)";//颜色context.arc( x , y , RADIUS , 0 , 2*Math.PI,false )//画圆，参数依次：横坐标，纵坐标，半径，画圆起点，画圆终点，是否为顺时针方向画圆context.fill(); fill()会将颜色填充 开始和结束1234567891011context.fillStyle="rgb(2,100,30)";//颜色context.moveTo(100,100);//表示画笔的起点坐标context.lineTo(700,700);//表示下一笔的坐标context.lineTo(1000,700);//表示下一笔的坐标context.fill();context.strokeStyle="#005588";//设置颜色，red这种方式也是可以的context.lineWidth=5;//线宽context.moveTo(100,100);//表示画笔的起点坐标context.lineTo(700,700);//表示下一笔的坐标context.lineTo(1000,700);//表示下一笔的坐标context.stroke();//绘制线条 这代码显示的结果，其实不是我们想的那样，其实还是存在一些问题，因fill()上方的路径状态还是存在有效的，所以为了解决这个问题，引入了beginPath()和closePath() beginPath():开始路径，表示画笔状态的开始 closePath():结束路径，结束一个路径，如果路径不是封闭的，会将其变为封闭 清除1context.clearRect(0,0,1024, 768); 参数表示清除的坐标范围]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring的Redis配置与使用]]></title>
    <url>%2F2016%2F07%2F07%2Fredis20160707%2F</url>
    <content type="text"><![CDATA[Redis是一种特殊类型的数据库，他被称之为key-value存储 本文覆盖缓存和存储两方面进行说明，使用的是Spring 4.0和Java配置方式 代码地址下载地址：https://github.com/zoeminghong/springmvc-javaconfig 存储Redis的配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546package springmvc.rootconfig;import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;@Configuration@EnableCachingpublic class CachingConfig &#123; /** * 连接Redis * * @return */ @Bean public JedisConnectionFactory redisConnectionFactory() &#123; JedisConnectionFactory jedisConnectionFactory = new JedisConnectionFactory(); // host地址 jedisConnectionFactory.setHostName("10.10.13.12"); // 端口号 jedisConnectionFactory.setPort(6379); jedisConnectionFactory.afterPropertiesSet(); return jedisConnectionFactory; &#125; /** * RedisTemplate配置 * * @param redisCF * @return */ @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate( RedisConnectionFactory redisCF) &#123; RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;String, Object&gt;(); redisTemplate.setConnectionFactory(redisCF); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125;&#125; Redis连接工厂 JedisConnectionFactory JredisConnectionFactory LettuceConnectionFactory SrpConnectionFactory 建议自行测试选用合适自己的连接工厂 如果使用的是localhost和默认端口，则这两项的配置可以省略 RedisTemplate RedisTemplate StringRedisTemplate RedisTemplate能够让我们持久化各种类型的key和value，并不仅限于字节数组 StringRedisTemplate扩展了RedisTemplate，只能使用String类型 StringRedisTemplate有一个接受RedisConnectionFactory的构造器，因此没有必要在构建后在调用setConnectionFactory() 使用RedisTemplateAPI 方法 子API接口 描述 opsForValue() ValueOperations&lt;K,V&gt; 描述具有简单值的条目 opsForList() ListOperations&lt;K,V&gt; 操作具有list值的条目 opsForSet() SetOperations&lt;K,V&gt; 操作具有set值的条目 opsForZSet() ZSetOperations&lt;K,V&gt; 操作具有ZSet值（排序的set）的条目 opsForHash() HashOperations&lt;K,HK,VH&gt; 操作具有hash值的条目 boundValueOps(K) BoundValueOperations&lt;K,V&gt; 以绑定指定key的方式，操作具有简单值的条目 boundListOps(K) BoundListOperations&lt;K,V&gt; 以绑定指定key的方式，操作具有list的条目 boundSetOps(K) BoundSetOperations&lt;K,V&gt; 以绑定指定key的方式，操作具有set的条目 boundZSet(K) BoundZSetOperations&lt;K,V&gt; 以绑定指定key的方式，操作具有ZSet（排序的set）的条目 boundHashOps(K) BoundHashOperations&lt;K,V&gt; 以绑定指定key的方式，操作具有hash值的条目 操作 12345678910111213141516171819202122232425package springmvc.web;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import springmvc.bean.Order;import springmvc.orders.db.OrderRepository;@Controllerpublic class HomeController &#123; @Autowired RedisTemplate&lt;String, Object&gt; redisTemplate; @RequestMapping(value = &#123; "/", "index" &#125;, method = RequestMethod.GET) public String index() &#123; redisTemplate.opsForValue().set("gege", 11); System.out.print(redisTemplate.opsForValue().get("gege")); return "index"; &#125;&#125; 123456//创建List条目，key是cartBoundListOperations&lt;String, Object&gt;cart=redisTemplate.boundListOps("cart");//删除最后的一条数据cart.rightPop();//在最后，添加一条数据cart.rightPush("我笑了"); Key和Value序列化如果要使用到JavaBean，需要其实现Serializable接口，将其序列化 或者使用Spring Data Redis提供的序列化器 GenericToStringSerializer：使用Spring转换服务进行序列化 JacksonJsonRedisSerializer：使用Jackson1，将对象序列化为JSON Jackson2JsonRedisSerializer：使用Jackson2，将对象序列化为JSON JdkSerializationRedisSerializer：使用Java序列化 OxmSerializer：使用Spring O/X映射的编排器和解排器实现序列化，用于XML序列化 StringRedisSerializer：序列化String类型的key和value 12redisTemplate.setKeySerializer(new StringRedisSerializer());redisTemplate.setValueSerializer(new Jackson2JsonRedisSerializer&lt;Order&gt;(Order.class)); 缓存配置在配置文件中追加如下代码 12345678910111213/** * 缓存管理器 * @param redisTemplate * @return */@Beanpublic CacheManager cacheManager(RedisTemplate&lt;String, Object&gt; redisTemplate) &#123; RedisCacheManager cacheManager =new RedisCacheManager(redisTemplate); //设置过期时间 cacheManager.setDefaultExpiration(10); return cacheManager;&#125; 使用注解进行缓存数据 注解 描述 @Cacheable 表明Spring在调用方法之前，首先应该在缓存中查找方法的返回值，如果这个值能够找到，就会返回缓存的值。否则，这个方法就会被调用，返回值会放到缓存之中 @CachePut 表名Spring应该将方法的返回值放到缓存中。在方法的调用前并不会检查缓存，方法始终都会被调用 @CacheEvict 表明Spring应该在缓存中清除一个或多个条目 @Caching 这是一个分组的注解，能够同时应用多个其他的缓存注解 @Cacheable与@CachePut的一些共有属性 属性 类型 描述 value String[] 要使用的缓存名称 condition String SpEL表达式，如果得到的值是false的话，不会将缓存应用到方法调用上 key String SpEL表达式，用来计算自定义的缓存key unless String SpEL表达式，如果得到的值是true的话，返回值不会放到缓存之中 12345678910111213package springmvc.orders.db;import java.util.List;import org.springframework.cache.annotation.Cacheable;import springmvc.bean.Order;public interface OrderOperations &#123; @Cacheable("spittle") List&lt;Order&gt; findOrdersByType(String t);&#125; 缓存切面会拦截调用并在缓存中查找之前以名spittle存储的返回值。缓存的key是传递到findOrdersByType()方法中的t参数。如果按照这个key能够找到值的话，就会返回找到的值，方法就不会被调用。如果没有找到值的话，那么就会调用这个方法 当在接口方法添加注解后，被注解的方法，在所有的实现继承中都会有相同的缓存规则 @CacheEvict 12@CacheEvict("spittle")void remove(String Id); @CacheEvict能够应用在返回值为void的方法上， 而@Cacheable和@CachePut需要非void的返回值，他将会作为放在缓存中的条目 属性 类型 描述 value String[] 要使用的缓存名称 key String SpEL表达式，用来计算自定义的缓存key condition String SpEL表达式，如果得到的值是false的话，缓存不会应用到方法调用上 allEntries boolean 如果为true的话，特定缓存的所有条目都会被移除 beforeInvocation boolean 如果为true的话，在方法调用之前移除条目，如果为false的话，在方法成功调用之后在移除条目]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JFormJS说明文档]]></title>
    <url>%2F2016%2F07%2F07%2Fjform20160707%2F</url>
    <content type="text"><![CDATA[之前使用了很多Jquery的表单插件，一直用的很不舒服，不能满足我现在的工作，所以就萌生了自己写一个插件的想法，于是就有了JFormJS Github下载地址：https://github.com/zoeminghong/JForm 在开发使用的时候只要导入jForm.values.min.js文件就可以了 用于HTML的表单初始化赋值和表单数据提交的轻量级Jquery插件 类型 配置 text、textarea data-text=”text” checkbox data-text=”checkbox” select data-text=”select” radio data-text=”radio” 表单赋值目前只支持Json格式数据进行表单初始化 调用方式： 1forms.resetValues(formID,jsonData); Demo 1234567891011121314151617181920212223242526&lt;body&gt;&lt;div&gt; &lt;form method="post" id="formDemo"&gt; &lt;input name="first" type="text" data-text="text"&gt; &lt;textarea name="firstArea" cols="20" rows="5" data-text="text"&gt;&lt;/textarea&gt; &lt;input type="checkbox" name="checkboxFirst" value="1" data-text="checkbox"&gt;1 &lt;input type="checkbox" name="checkboxFirst" value="2" data-text="checkbox"&gt;2 &lt;select name="selectFirst" data-text="select"&gt; &lt;option value="1"&gt;1&lt;/option&gt; &lt;option value="2"&gt;2&lt;/option&gt; &lt;option value="3"&gt;3&lt;/option&gt; &lt;/select&gt; &lt;input type="radio" name="radioFisrt" value="1" data-text="radio"&gt;1 &lt;input type="radio" name="radioFisrt" value="2" data-text="radio"&gt;2 &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;script&gt; $(function()&#123; setValue(); function setValue()&#123; var json=&#123;first:"11",firstArea:"22",checkboxFirst:[1],checkboxSecond:[4],selectFirst:3,radioFisrt:2,radioSecond:2&#125;; forms.resetValues("formDemo",json); &#125; &#125;);&lt;/script&gt; 获取表单值最后获取得到的是Json对象 调用方式 1forms.values(formID); Demo 123456789101112131415161718192021222324&lt;body&gt;&lt;div&gt; &lt;form method="post" id="formDemo"&gt; &lt;input name="first" type="text" data-text="text"&gt; &lt;textarea name="firstArea" cols="20" rows="5" data-text="text"&gt;&lt;/textarea&gt; &lt;input type="checkbox" name="checkboxFirst" value="1" data-text="checkbox"&gt;1 &lt;input type="checkbox" name="checkboxFirst" value="2" data-text="checkbox"&gt;2 &lt;select name="selectFirst" data-text="select"&gt; &lt;option value="1"&gt;1&lt;/option&gt; &lt;option value="2"&gt;2&lt;/option&gt; &lt;option value="3"&gt;3&lt;/option&gt; &lt;/select&gt; &lt;input type="radio" name="radioFisrt" value="1" data-text="radio"&gt;1 &lt;input type="radio" name="radioFisrt" value="2" data-text="radio"&gt;2 &lt;button onclick="getValue();"&gt;确定&lt;/button&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;script&gt; function getValue()&#123; forms.values("formDemo"); &#125; &lt;/script&gt; 以上 作者：@迹_Jason 更新日期：2016/07/07 微信号：zoeminghong 微信公众号：APPZone]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的Spring配置使用]]></title>
    <url>%2F2016%2F07%2F05%2Fmongo20160705%2F</url>
    <content type="text"><![CDATA[Spring-data对MongoDB进行了很好的支持，接下来就讲解一下关于Spring对MongoDB的配置和一些正常的使用 我下面的工程使用的是Spring的Java配置的方式和Maven构建 具体的工程代码大家可以访问我的Github地址:https://github.com/zoeminghong/springmvc-javaconfig ①MongoDB的必要配置123456789101112131415161718192021222324252627282930313233package springmvc.rootconfig;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.core.MongoClientFactoryBean;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.repository.config.EnableMongoRepositories;import com.mongodb.Mongo;@Configuration// 启用MongoDB的Repository功能，会对其Repositories自动扫描@EnableMongoRepositories(basePackages = "springmvc.orders.db") public class MongoConfig &#123; // MongoClient配置 @Bean public MongoClientFactoryBean mongo() &#123; MongoClientFactoryBean mongo = new MongoClientFactoryBean(); mongo.setHost("localhost"); //MongoCredential credential=MongoCredential.createCredential(env.getProperty("mongo.username"), "OrdersDB",env.getProperty("mongo.password").toCharArray());// mongo.setCredentials(new MongoCredential[]&#123;credential&#125;); //还可以对端口进行配置 return mongo; &#125; // Mongo Template配置 @Bean public MongoOperations mongoTemplate(Mongo mongo) &#123; //OrdersDB就是Mongo的数据库 return new MongoTemplate(mongo, "OrdersDB"); &#125;&#125; 为了访问数据库的时候，我们可能还需要帐号密码 12MongoCredential credential=MongoCredential.createCredential(env.getProperty("mongo.username"), "OrdersDB",env.getProperty("mongo.password").toCharArray());mongo.setCredentials(new MongoCredential[]&#123;credential&#125;); ②为模型添加注解123456789101112131415161718192021222324252627282930313233343536373839404142434445package springmvc.bean;import java.util.Collection;import java.util.LinkedHashSet;import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.mapping.Document;import org.springframework.data.mongodb.core.mapping.Field;//这是文档@Documentpublic class Order &#123; //指定ID @Id private String id; //为域重命名 @Field("client") private String customer; private String type; private Collection&lt;Item&gt; items=new LinkedHashSet&lt;Item&gt;(); public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getCustomer() &#123; return customer; &#125; public void setCustomer(String customer) &#123; this.customer = customer; &#125; public String getType() &#123; return type; &#125; public void setType(String type) &#123; this.type = type; &#125; public Collection&lt;Item&gt; getItems() &#123; return items; &#125; public void setItems(Collection&lt;Item&gt; items) &#123; this.items = items; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041package springmvc.bean;public class Item &#123; private Long id; private Order order; private String product; private double price; private int quantity; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Order getOrder() &#123; return order; &#125; public void setOrder(Order order) &#123; this.order = order; &#125; public String getProduct() &#123; return product; &#125; public void setProduct(String product) &#123; this.product = product; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; public int getQuantity() &#123; return quantity; &#125; public void setQuantity(int quantity) &#123; this.quantity = quantity; &#125; &#125; 注解 描述 @Document 标示映射到mongoDB文档上的领域对象 @ID 标示某个为ID域 @DbRef 标示某个域要引用其他的文档，这个文档有可能位于另外一个数据库中 @Field 为文档域指定自定义的元数据 @Version 标示某个属性用作版本域 若不使用@Field注解，域名就与Java属性相同 上面之所以Item的Java类为什么没有@Document注解，是因为我们不会单独想Item持久化为文档 ③使用MongoTemplate访问MongoDB12345678910111213141516171819202122232425262728293031323334package springmvc.web;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import springmvc.bean.Order;import springmvc.orders.db.OrderRepository;@Controllerpublic class HomeController &#123; @Autowired MongoOperations mongo; @RequestMapping(value = &#123; "/", "index" &#125;, method = RequestMethod.GET) public String index() &#123; long orderCount=mongo.getCollection("order").count(); System.out.println(orderCount);// Order order = new Order();// order.setId("1");// order.setCustomer("gg");// order.setType("2"); //第二个参数是文档存储的名称// mongo.save(order,"order");// String orderId="1";// Order order=mongo.findById(orderId, Order.class);// System.out.println(order.getCustomer()); return "index"; &#125;&#125; 在这里我们将MongoTemplate注入到一个类型为MongoOperations的属性中。MongoOperations是MongoTemplate所实现的接口，MongoOperations中存在很多文档操作方法 MongoOperations其实已经能满足很多需求了 如果还没有满足你的需求，接下来我就介绍一下，如何编写MongoDB Repository 编写MongoDB Repository1234567891011121314151617package springmvc.orders.db;import java.util.List;import org.springframework.data.mongodb.repository.MongoRepository;import springmvc.bean.Order;public interface OrderRepository extends MongoRepository&lt;Order, String&gt; &#123; List&lt;Order&gt; findByCustomer(String c); List&lt;Order&gt; findByCustomerLike(String c); List&lt;Order&gt; findByCustomerAndType(String c, String t); List&lt;Order&gt; findByCustomerLikeAndType(String c, String t);&#125; 看到这里，大家有没有发现package的地址就是我们刚才@EnableMongoRepositories(basePackages = “springmvc.orders.db”)的配置 MongoRepository接口有两个参数，第一个是带有@Document注解的对象类型，也就是该Repository要处理的类型。第二个参数是带有@Id注解的属性类型 OrderRepository继承了MongoRepository中很多自带的方法 方法 描述 long count() 返回指定Repository类型的文档数量 void delete(Iterable&lt;? extends T&gt;) 删除与指定对象关联的所有文档 void delete(T) 删除与指定对象关联的文档 void delete(ID) 根据ID删除某一个文档 void deleteAll(); 删除指定Repository类型的所有文档 boolean exists(Object) 如果存在与指定对象相关联的文档，则返回true boolean exists(ID) 如果存在与指定对象相关联的文档，则返回true ListfindAll() 返回指定Repository类型的所有文档 ListfindAll(Iterable) 返回指定文档ID对应的所有文档 ListfindAll(Pageable) 为指定Repository类型，返回分页且排序的文档列表 ListfindAll(Sort) 为指定Repository类型，返回排序后的所有文档列表 T findOne(ID) 为指定的ID返回单个文档 Save(terable) 保存指定Iterable中的所有文档 save() 为给定的对象保存一条文档 上面的我们定义的四个方法都是我们自定义的方法，其方法名存在很多意义，不能随便定义 1List&lt;Order&gt; findByCustomer(String c); find为查询动词，还可以是read、get、count等 Customer为断言，判断其行为 在断言中，会有一个或多个限制结果的条件。每个条件必须引用一个属性，并且还可以指定一种比较操作。如果省略比较操作符的话，那么这暗指是一种相等比较操作。不过，我们也可以选择其他的比较操作 类型 IsAfter、After、IsGreaterThan、GreaterThan IsGreaterThanEqual、GreaterThanEqual IsBefore、Before、IsLessThan、LessThan IsLessThanEqual、LessThanEqual IsBetween、Between IsNull、Null IsNotNull、NotNull IsIn、In IsNotIn、NotIn IsStartingWith、StartingWith、StartsWith IsEndingWith、EndingWith、EndsWith IsContaining、Containing、Contains IsLike、Like IsNotLike、NotLike IsTure、True IsFalse、False Is、Equals IsNot、Not other 类型 IgnoringCase、IgnoresCase、OrderBy、And、Or 指定查询12@Query("&#123;'customer':'Chuck Wagon','type':?0&#125;")List&lt;Order&gt; findChucksOrders(String t); @Query中给定的JSON将会与所有的Order文档进行匹配，并返回匹配的文档，这里的type属性映射成“？0”，这表明type属性应该与查询方法的第0个参数相等，如果有多个参数，则”?1”….. 混合自定义的功能12345678910package springmvc.orders.db;import java.util.List;import springmvc.bean.Order;public interface OrderOperations &#123; List&lt;Order&gt; findOrdersByType(String t);&#125; 1234567891011121314151617181920212223package springmvc.orders.db;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import springmvc.bean.Order;public class OrderRepositoryImpl implements OrderOperations &#123; @Autowired private MongoOperations mongo; //将混合实现注入MongoOperations @Override public List&lt;Order&gt; findOrdersByType(String t) &#123; String type =t.equals("Net")?"2":t; Criteria where=Criteria.where("type").is(type); Query query=Query.query(where); return mongo.find(query, Order.class); &#125;&#125; 12345678910111213141516171819202122package springmvc.orders.db;import java.util.List;import org.springframework.data.mongodb.repository.MongoRepository;import org.springframework.data.mongodb.repository.Query;import springmvc.bean.Order;//继承OrderOperations接口public interface OrderRepository extends MongoRepository&lt;Order, String&gt;,OrderOperations &#123; List&lt;Order&gt; findByCustomer(String c); List&lt;Order&gt; findByCustomerLike(String c); List&lt;Order&gt; findByCustomerAndType(String c, String t); List&lt;Order&gt; findByCustomerLikeAndType(String c, String t); @Query("&#123;'customer':'Chuck Wagon','type':?0&#125;") List&lt;Order&gt; findChucksOrders(String t); &#125; 123456789101112131415161718192021222324252627package springmvc.web;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import springmvc.bean.Order;import springmvc.orders.db.OrderRepository;@Controllerpublic class HomeController &#123; @Autowired MongoOperations mongo; @Autowired OrderRepository orderRepository; @RequestMapping(value = &#123; "/", "index" &#125;, method = RequestMethod.GET) public String index() &#123; List&lt;Order&gt; list=orderRepository.findOrdersByType("2"); System.out.println(list.size()); return "index"; &#125;&#125; 以上这些关联起来的关键点是OrderRepositoryImpl，这个名字前半部分与OrderRepository相同，只是添加了一个“Impl”后缀。如果想更改该后缀，可以在MongoConfig类中更改为自己理想的后缀 1@EnableMongoRepositories(basePackages = "springmvc.orders.db",repositoryImplementationPostfix="Stuff")]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIO全解析说明]]></title>
    <url>%2F2016%2F06%2F12%2Fnio20160612%2F</url>
    <content type="text"><![CDATA[Java NIO是一个用来替代标准Java IO API的新型数据传递方式，像现在分布式架构中会经常存在他的身影。其比传统的IO更加高效，非阻塞，异步，双向 NIO主体结构 Java NIO的主要构成核心就是Buffer、Channel和Selector这三个 对于Channel我想要提醒的是，Channel中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入 使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件 Channel所有的 IO 在NIO 中都从一个Channel 开始。Channel 有点象流 Channel的实现 FileChannel:从文件中读写数据 DatagramChannel:通过UDP读写网络中的数据 SocketChannel:通过TCP读写网络中的数据 ServerSocketChannel:监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel Scatter/Gather 分散（scatter）从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel 通过这样的方式可以方便数据的读取，当你想要获取整个数据的一部分的时候，通过这种方式可以很快的获取数据 1234ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写 transferFrom、transferTo实现两个Channel之间相互连接，数据传递 12345678910111213141516171819202122232425262728293031public static void trainforNio() &#123; RandomAccessFile fromFile=null; RandomAccessFile toFile=null; try &#123; fromFile = new RandomAccessFile("src/nio.txt", "rw"); // channel获取数据 FileChannel fromChannel = fromFile.getChannel(); toFile = new RandomAccessFile("src/toFile.txt", "rw"); FileChannel toChannel = toFile.getChannel(); System.out.println(toChannel.size()); //position处开始向目标文件写入数据,这里是toChannel long position = toChannel.size(); long count = fromChannel.size(); toChannel.transferFrom(fromChannel, position, count); System.out.println(toChannel.size()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (fromFile != null) &#123; fromFile.close(); &#125; if (toFile != null) &#123; toFile.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; transferFrom、transferTo作用是一样的，只是一个是tochannal调用，一个是fromchannnal调用 在实际的运用中可能存在源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数 在SoketChannel的实现中，SocketChannel只会传输此刻准备好的数据（可能不足count字节）。因此，SocketChannel可能不会将请求的所有数据(count个字节)全部传输到FileChannel中 看官一定要仔细看我栗子中的注释 BufferBuffer是一个缓存区，其会将Channel中的数据存储起来 Buffer的实现 ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer MappedByteBuffer capacity,position,limit在讲解该主题之前，首先要明白读模式和写模式，无论是Channel还是Buffer都存在这两种模式，要理解这两种模式，第一步要明确主题是哪一个，是Channel还是Buffer。举个栗子，主角是Channel，读模式的含义就是从Buffer中获取数据，写模式就是将数据写入Buffer，对于Buffer则是相反。搞清楚这一点，理解下面的就要相对清楚一点 capacity:作为一个内存块，其就代表了当前Buffer能最多暂存多少数据量，存储的数据类型则是根据上面的Buffer对象类型，一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据 position:代表当前数据读或写处于那个位置。读模式：被重置从0开始，最大值可能为capacity-1或者limit-1，写模式：被重置从0开始，最大值为limit-1 limit:最多能往Buffer里写多少数据，limit大小跟数据量大小和capacity有关，读模式：数据量&gt;capacity时，limit=capacity，数据量=capacity时，limit=capacity，数据量&lt;capacity时，limit&lt;capacity，写模式：limit&lt;=capacity 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;public class Method &#123; public static void nio() &#123; RandomAccessFile aFile = null; try &#123; aFile = new RandomAccessFile("src/nio.txt", "rw"); // channel获取数据 FileChannel fileChannel = aFile.getChannel(); // 初始化Buffer，设定Buffer每次可以存储数据量 // 创建的Buffer是1024byte的，如果实际数据本身就小于1024，那么limit就是实际数据大小 ByteBuffer buf = ByteBuffer.allocate(1024); // channel中的数据写入Buffer int bytesRead = fileChannel.read(buf); System.out.println(bytesRead); while (bytesRead != -1) &#123; // Buffer切换为读取模式 buf.flip(); // 读取数据 while (buf.hasRemaining()) &#123; System.out.print((char) buf.get()); &#125; // 清空Buffer区 buf.compact(); // 继续将数据写入缓存区 bytesRead = fileChannel.read(buf); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (aFile != null) &#123; aFile.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Method.nio(); Buffer读写数据步骤 写入数据到Buffer(fileChannel.read(buf)) 调用flip()方法(buf.flip()) 从Buffer中读取数据(buf.get()) 调用clear()方法或者compact()方法(buf.compact()) Buffer方法 flip():将Buffer读模式切换到写模式，并且将position制为0 clear():清空整个缓冲区 compact():只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面 allocate(1024):初始化Buffer，设定的值就决定capacity值的大小 rewind():将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等） mark()与reset():通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position equals():当满足下面三个条件时，两个Buffer才是相等 有相同的类型（byte、char、int等） Buffer中剩余的byte、char等的个数相等 Buffer中所有剩余的byte、char等都相同 只比较的是剩余的数据 compareTo():满足下列条件，则认为一个Buffer“小于”另一个Buffer 第一个不相等的元素小于另一个Buffer中对应的元素 所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少) SelectorSelector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便 大致流程当您调用一个选择器对象的 select( )方法时，相关的键会被更新，用来检查所有被注册到该选择器的通道。您可以获取一个键的集合，从而找到当时已经就绪的通道。通过遍历这些键，您可以选择出每个从上次您调用 select( )开始直到现在，已经就绪的通道 选择器(Selector)的特点12345678public abstract class Selector&#123;// This is a partial API listingpublic static Selector open( ) throws IOExceptionpublic abstract boolean isOpen( );//判断是openpublic abstract void close( ) throws IOException;//选择键设置无效public abstract SelectionProvider provider( );&#125; 选择器类管理着一个被注册的通道集合的信息和它们的就绪状态。通道是和选择器一起被注册的，并且使用选择器来更新通道的就绪状态。当这么做的时候，可以选择将被激发的线程挂起，直到有就绪的的通道 不能注册已经关闭的selectableChannel 通过调用一个自定义的 SelectorProvider对象的 openSelector( )方法来创建一个 Selector 实例也是可行的。您可以通过调用 provider( )方法来决定由哪个 SelectorProvider 对象来创建给定的 Selector 实例 通道(Channel)的特点1234567891011121314public abstract class SelectableChannelextends AbstractChannelimplements Channel&#123;// This is a partial API listingpublic abstract SelectionKey register (Selector sel, int ops)throws ClosedChannelException;public abstract SelectionKey register (Selector sel, int ops,Object att)throws ClosedChannelException;public abstract boolean isRegistered( );public abstract SelectionKey keyFor (Selector sel);public abstract int validOps( );&#125; 继承SelectableChannel 一个channel可以注册到多个selector中 一个selector中同一个channel只能有一个 通道被注册前，要非阻塞模式 支持Connect、Accept、Read、Write四种可选择操作事件，但并不是所有的SelectableChannel都存在以上四类，可以通过validOps()获取可以使用的操作事件集合 如果你对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来 任何一个通道和选择器的注册关系都被封装在一个 SelectionKey 对象中。 keyFor( )方法将返回与该通道和指定的选择器相关的键。如果通道被注册到指定的选择器上，那么相关的键将被返回。如果它们之间没有注册关系，那么将返回 null 选择键(SelectionKey)的特点123456789101112131415161718192021package java.nio.channels;public abstract class SelectionKey&#123;public static final int OP_READpublic static final int OP_WRITEpublic static final int OP_CONNECTpublic static final int OP_ACCEPTpublic abstract SelectableChannel channel( );public abstract Selector selector( );public abstract void cancel( );public abstract boolean isValid( );public abstract int interestOps( );public abstract void interestOps (int ops);public abstract int readyOps( );public final boolean isReadable( )public final boolean isWritable( )public final boolean isConnectable( )public final boolean isAcceptable( )public final Object attach (Object ob)public final Object attachment( )&#125; 封装了特定的通道与特定的选择器的注册关系 一个 SelectionKey 对象包含两个以整数形式进行编码的byte掩码：一个用于指示那些通道/选择器组合体所关心的操作(instrest 集合)，另一个表示通道准备好要执行的操作（ ready 集合） 当终结注册关系时 当应该终结这种关系的时候，可以调用 SelectionKey对象的 cancel( )方法。可以通过调用 isValid( )方法来检查它是否仍然表示一种有效的关系。当键被取消时，它将被放在相关的选择器的已取消的键的集合里。注册不会立即被取消，但键会立即失效。当再次调用 select( )方法时（或者一个正在进行的 select()调用结束时），已取消的键的集合中的被取消的键将被清理掉，并且相应的注销也将完成。通道会被注销，而新的SelectionKey 将被返回 当通道关闭时 当通道关闭时，所有相关的键会自动取消（记住，一个通道可以被注册到多个选择器上）。当选择器关闭时，所有被注册到该选择器的通道都将被注销，并且相关的键将立即被无效化（取消）。一旦键被无效化，调用它的与选择相关的方法就将抛出 CancelledKeyException interest 集合 当前的 interest 集合可以通过调用键对象的 interestOps( )方法来获取 最初，这应该是通道被注册时传进来的值。这个 interset 集合永远不会被选择器改变，但您可以通过调用 interestOps( )方法并传入一个新的byte掩码参数来改变它。 interest 集合也可以通过将通道注册到选择器上来改变（实际上使用一种迂回的方式调用 interestOps( )），就像 4.1.2 小节中描的那样。当相关的 Selector 上的 select( )操作正在进行时改变键的 interest 集合，不会影响那个正在进行的选择操作。所有更改将会在 select( )的下一个调用中体现出来 ready集合 可以通过调用键的 readyOps( )方法来获取相关的通道的已经就绪的操作。 ready 集合是 interest集合的子集，并且表示了 interest 集合中从上次调用 select( )以来已经就绪的那些操作 SelectionKey 类定义了四个便于使用的布尔方法来为您测试这些byte值： isReadable( )， isWritable( )， isConnectable( )， 和 isAcceptable( ) SelectionKey 对象包含的 ready 集合与最近一次选择器对所注册的通道所作的检查相同。而每个单独的通道的就绪状态会同时改变 附加的对象 可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下： 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 如果选择键的存续时间很长，但您附加的对象不应该存在那么长时间，请记得在完成后清理附件。否则，您附加的对象将不能被垃圾回收，您将会面临内存泄漏问题 总体上说， SelectionKey 对象是线程安全的，但知道修改 interest 集合的操作是通过 Selector 对象进行同步的是很重要的。这可能会导致 interestOps( )方法的调用会阻塞不确定长的一段时间。选择器所使用的锁策略（例如是否在整个选择过程中保持这些锁）是依赖于具体实现的。幸好，这种多元处理能力被特别地设计为可以使用单线程来管理多个通道。被多个线程使用的选择器也只会在系统特别复杂时产生问题。 选择过程123456789public abstract class Selector&#123;public abstract Set keys( );public abstract Set selectedKeys( );public abstract int select( ) throws IOException;public abstract int select (long timeout) throws IOException;public abstract int selectNow( ) throws IOException;public abstract void wakeup( );&#125; 已注册的键的集合 与选择器关联的已经注册的键的集合。并不是所有注册过的键都仍然有效。这个集合通过keys( )方法返回，并且可能是空的。这个已注册的键的集合不是可以直接修改的；试图这么做的话将引 java.lang.UnsupportedOperationException。 已选择的键的集合 已注册的键的集合的子集。这个集合的每个成员都是相关的通道被选择器（在前一个选择操作中）判断为已经准备好的，并且包含于键的 interest 集合中的操作。这个集合通过 selectedKeys( )方法返回（并有可能是空的） 不要将已选择的键的集合与 ready 集合弄混了。这是一个键的集合，每个键都关联一个已经准备好至少一种操作的通道。每个键都有一个内嵌的 ready 集合，指示了所关联的通道已经准备好的操作 键可以直接从这个集合中移除，但不能添加 已取消的键的集合 已注册的键的集合的子集，这个集合包含了 cancel( )方法被调用过的键（这个键已经被无效化），但它们还没有被注销。这个集合是选择器对象的私有成员，因而无法直接访问 在一个刚初始化的 Selector 对象中，这三个集合都是空的。 执行步骤 已取消的键的集合将会被检查。如果它是非空的，每个已取消的键的集合中的键将从另外两个集合中移除，并且相关的通道将被注销。这个步骤结束后，已取消的键的集合将是空的。 已注册的键的集合中的键的 interest 集合将被检查。在这个步骤中的检查执行过后，对interest 集合的改动不会影响剩余的检查过程。 a.如果通道的键还没有处于已选择的键的集合中，那么键的 ready 集合将被清空，然后表示操作系统发现的当前通道已经准备好的操作的比特掩码将被设置。 b.否则，也就是键在已选择的键的集合中。键的 ready 集合将被表示操作系统发现的当前已经准备好的操作的比特掩码更新。所有之前的已经不再是就绪状态的操作不会被清除。事实上，所有的比特位都不会被清理。由操作系统决定的 ready 集合是与之前的 ready 集合按位分离的，一旦键被放置于选择器的已选择的键的集合中，它的 ready 集合将是累积的。比特位只会被设置，不会被清理。 步骤 2 可能会花费很长时间，特别是所激发的线程处于休眠状态时。与该选择器相关的键可能会同时被取消。当步骤 2 结束时，步骤 1 将重新执行，以完成任意一个在选择进行的过程中，键已经被取消的通道的注销。 select 操作返回的值是 ready 集合在步骤 2 中被修改的键的数量，而不是已选择的键的集合中的通道的总数。返回值不是已准备好的通道的总数，而是从上一个 select( )调用之后进入就绪状态的通道的数量。之前的调用中就绪的，并且在本次调用中仍然就绪的通道不会被计入，而那些在前一次调用中已经就绪但已经不再处于就绪状态的通道也不会被计入。这些通道可能仍然在已选择的键的集合中，但不会被计入返回值中。返回值可能是 0。 为什么延迟注销 使用内部的已取消的键的集合来延迟注销，是一种防止线程在取消键时阻塞，并防止与正在进行的选择操作冲突的优化。注销通道是一个潜在的代价很高的操作，这可能需要重新分配资源（请记住，键是与通道相关的，并且可能与它们相关的通道对象之间有复杂的交互）。 三种select()方法仅仅在它们在所注册的通道当前都没有就绪时，是否阻塞的方面有所不同。 select():在没有通道就绪时将无限阻塞。一旦至少有一个已注册的通道就绪，选择器的选择键就会被更新，并且每个就绪的通道的 ready 集合也将被更新。返回值将会是已经确定就绪的通道的数目。正常情况下， 这些方法将返回一个零的值，因为直到一个通道就绪前它都会阻塞。 select(long timeout):如果在您提供的超时时间（以毫秒计算）内没有通道就绪时，它将返回 0。如果一个或者多个通道在时间限制终止前就绪，键的状态将会被更新，并且方法会在那时立即返回。将超时参数指定为 0 表示将无限期等待，那么它就在各个方面都等同于使用select() selectNow():执行就绪检查过程，但不阻塞。如果当前没有通道就绪，它将立即返回 0 停止选择过程wakeUp() 某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close() 用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。通道本身并不会关闭。 interrupt() 如果睡眠中的线程的 interrupt( )方法被调用，它的返回状态将被设置。如果被唤醒的线程之后将试图在通道上执行 I/O 操作，通道将立即关闭，然后线程将捕捉到一个异常。 例子服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;public class NIOServer &#123; // 通道管理器 private Selector selector; public void initServer(int port) throws Exception &#123; // 获得一个ServerSocket通道 ServerSocketChannel serverChannel = ServerSocketChannel.open(); // 设置通道为 非阻塞 serverChannel.configureBlocking(false); // 将该通道对于的serverSocket绑定到port端口 serverChannel.socket().bind(new InetSocketAddress(port)); // 获得一耳光通道管理器 this.selector = Selector.open(); // 将通道管理器和该通道绑定，并为该通道注册selectionKey.OP_ACCEPT事件 // 注册该事件后，当事件到达的时候，selector.select()会返回， // 如果事件没有到达selector.select()会一直阻塞 serverChannel.register(selector, SelectionKey.OP_ACCEPT); &#125; // 采用轮训的方式监听selector上是否有需要处理的事件，如果有，进行处理 public void listen() throws Exception &#123; System.out.println("start server"); // 轮询访问selector while (true) &#123; // 当注册事件到达时，方法返回，否则该方法会一直阻塞 selector.select(); // 获得selector中选中的相的迭代器，选中的相为注册的事件 Iterator ite = this.selector.selectedKeys().iterator(); while (ite.hasNext()) &#123; SelectionKey key = (SelectionKey) ite.next(); // 删除已选的key 以防重负处理 ite.remove(); // 客户端请求连接事件 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel) key.channel(); // 获得和客户端连接的通道 SocketChannel channel = server.accept(); // 设置成非阻塞 channel.configureBlocking(false); // 在这里可以发送消息给客户端 channel.write(ByteBuffer.wrap(new String("hello client").getBytes())); // 在客户端 连接成功之后，为了可以接收到客户端的信息，需要给通道设置读的权限 channel.register(this.selector, SelectionKey.OP_READ); // 获得了可读的事件 &#125; else if (key.isReadable()) &#123; read(key); &#125; &#125; &#125; &#125; // 处理 读取客户端发来的信息事件 private void read(SelectionKey key) throws Exception &#123; // 服务器可读消息，得到事件发生的socket通道 SocketChannel channel = (SocketChannel) key.channel(); // 穿件读取的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(10); channel.read(buffer); byte[] data = buffer.array(); String msg = new String(data).trim(); System.out.println("server receive from client: " + msg); ByteBuffer outBuffer = ByteBuffer.wrap(msg.getBytes()); channel.write(outBuffer); &#125; public static void main(String[] args) throws Throwable &#123; NIOServer server = new NIOServer(); server.initServer(8989); server.listen(); &#125;&#125; 客户端 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.util.Iterator;public class NIOClient &#123; // 通道管理器 private Selector selector; /** * * // 获得一个Socket通道，并对该通道做一些初始化的工作 * @param ip 连接的服务器的ip // * @param port * 连接的服务器的端口号 * @throws IOException */ public void initClient(String ip, int port) throws IOException &#123; // 获得一个Socket通道 SocketChannel channel = SocketChannel.open(); // 设置通道为非阻塞 channel.configureBlocking(false); // 获得一个通道管理器 this.selector = Selector.open(); // 客户端连接服务器,其实方法执行并没有实现连接，需要在listen()方法中调 // 用channel.finishConnect();才能完成连接 channel.connect(new InetSocketAddress(ip, port)); // 将通道管理器和该通道绑定，并为该通道注册SelectionKey.OP_CONNECT事件。 channel.register(selector, SelectionKey.OP_CONNECT); &#125; /** * * // 采用轮询的方式监听selector上是否有需要处理的事件，如果有，则进行处理 * @throws // IOException * @throws Exception */ @SuppressWarnings("unchecked") public void listen() throws Exception &#123; // 轮询访问selector while (true) &#123; // 选择一组可以进行I/O操作的事件，放在selector中,客户端的该方法不会阻塞， // 这里和服务端的方法不一样，查看api注释可以知道，当至少一个通道被选中时， // selector的wakeup方法被调用，方法返回，而对于客户端来说，通道一直是被选中的 selector.select(); // 获得selector中选中的项的迭代器 Iterator ite = this.selector.selectedKeys().iterator(); while (ite.hasNext()) &#123; SelectionKey key = (SelectionKey) ite.next(); // 删除已选的key,以防重复处理 ite.remove(); // 连接事件发生 if (key.isConnectable()) &#123; SocketChannel channel = (SocketChannel) key.channel(); // 如果正在连接，则完成连接 if (channel.isConnectionPending()) &#123; channel.finishConnect(); &#125; // 设置成非阻塞 channel.configureBlocking(false); // 在这里可以给服务端发送信息哦 channel.write(ByteBuffer.wrap(new String("hello server!").getBytes())); // 在和服务端连接成功之后，为了可以接收到服务端的信息，需要给通道设置读的权限。 channel.register(this.selector, SelectionKey.OP_READ); // 获得了可读的事件 &#125; else if (key.isReadable()) &#123; read(key); &#125; &#125; &#125; &#125; private void read(SelectionKey key) throws Exception &#123; SocketChannel channel = (SocketChannel) key.channel(); // 穿件读取的缓冲区 ByteBuffer buffer = ByteBuffer.allocate(10); channel.read(buffer); byte[] data = buffer.array(); String msg = new String(data).trim(); System.out.println("client receive msg from server:" + msg); ByteBuffer outBuffer = ByteBuffer.wrap(msg.getBytes()); channel.write(outBuffer); &#125; /** * * // 启动客户端测试 * @throws IOException * @throws Exception */ public static void main(String[] args) throws Exception &#123; NIOClient client = new NIOClient(); client.initClient("localhost", 8989); client.listen(); &#125;&#125; 参考资料Java NIO系列教程 Java NIO学习8(Selector）]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java反射获取类和对象信息全解析]]></title>
    <url>%2F2016%2F06%2F07%2Fjvm20160607%2F</url>
    <content type="text"><![CDATA[反射可以解决在编译时无法预知对象和类是属于那个类的，要根据程序运行时的信息才能知道该对象和类的信息的问题。 在两个人协作开发时，你只要知道对方的类名就可以进行初步的开发了。 获取类对象 Class.forName(String clazzName)静态方法 调用类的class属性，Person.class返回的就是Person的class对象（推荐使用） 调用某个对象的getClass()方法 具体使用还是要根据实际来选择，第一种方式是比较自由的，只要知道一个类名就可以了，其不会做该类是否存在的校验，第二种、第三种则会做校验 获取类的信息获取类构造器 Connstructor&lt;T&gt; getConstructor(Class&lt;?&gt;...parameterTypes):返回此Class对象对应类的带指定形参的public构造器 Constructor&lt;?&gt;[] getConstructors():返回此Class对象对应类的所有public构造器 Constructor&lt;T&gt;[] getDeclaredConstructor(Class&lt;?&gt;...parameterTypes):返回此class对象对应类的带指定参数的构造器，与构造器的访问权限无关 Constructor&lt;?&gt;[] getDeclaredConstructors():返回此class对象对应类的所有构造器，与构造器的访问权限无关 获取类成员方法 Method getMethod(String name,Class&lt;?&gt;...parameterTypes):返回此class对象对应类的带指定形参的public方法 Method[] getMethods():返回此class对象所表示的类的所有public方法 Method getDeclaredMethod(string name,Class&lt;?&gt;...parameterTypes):返回此class对象对应类的带指定形参的方法，与方法访问权限无关 Method[] getDeclaredMethods():返回此class对象对应类的全部方法，与方法的访问权限无关 获取类成员变量 Field getField(String name):返回此class对象对应类的指定名称的public成员变量 Field[] getFields():返回此class对象对应类的所有public成员变量 Field getDeclaredField(String name):返回此class对象对应类的指定名称的成员变量，与成员变量访问权限无关 Field[] getDeclaredFields():返回此class对象对应类的全部成员变量，与成员变量的访问权限无关 获取类注解 &lt;A extends Annotation&gt;A getAnnotation(Class&lt;A&gt;annotationClass):尝试获取该class对象对应类上村子的指定类型的Annotation，如果该类型注解不存在，则返回null &lt;A extends Annotation&gt;A getDeclaredAnnotation(Class&lt;A&gt;annotationClass):这是Java 8中新增的，该方法获取直接修饰该class对象对应类的指定类型的Annotation，如果不存在，则返回null Annotation[] getAnnotations():返回修饰该class对象对应类上存在的所有Annotation Annotation[] getDeclaredAnnotations():返回修饰该Class对象对应类上存在的所有Annotation &lt;A extends Annotation&gt;A[] getAnnotationByType(Class&lt;A&gt;annotationClass):该方法的功能与前面介绍的getAnnotation()方法基本相似，但由于Java8增加了重复注解功能，因此需要使用该方法获取修饰该类的指定类型的多个Annotation &lt;A extends Annotation&gt;A[] getDeclaredAnnotationByType(Class&lt;A&gt;annotationClass):该方法发功能与前面介绍的getDeclaredAnnotations()方法相似，也是因为Java8的重复注解的功能，需要使用该方法获取直接修饰该类的指定类型的多个Annotation 获取该类内部类 Class&lt;?&gt;[] getDeclaredClasses():返回该class队形对应类里包含的全部内部类 获取该类对象所在的外部类 Class&lt;?&gt; getDeclaringClass():返回该Class对象对应类所在的外部类 获取该类对象对应类所实现的接口 Class&lt;?&gt;[] getInterfaces():返回该Class对象对应类所实现的全部接口 获取该类对象对应类所继承的父类 Class&lt;? super T&gt; getSuperclass():返回该Class对象对应类的超类的Class对象 获取该类对象对应类的修饰符、所在包、类名等基本信息 int getModifiers():返回此类或接口的所有修饰符，修饰符由public、protected、private、final、static、abstract等对应的常量组成，返回的整数应使用Modifier工具类的方法来解码，才可以获取真是的修饰符 Package getPackage():获取该类的包 String getName():以字符串形式返回此CLass对象所表示的类的简称 判断该类是否为接口、枚举、注解类型 boolean isAnnotation():返回此class对象是否表示一个注解类型 boolean isAnnotationPresent(Class&lt;? extends Annotation&gt;annotationClass):判断此Class对象是否使用类Annotation修饰 boolean isAnonymousClass():返回此class对象是否是一个匿名类 boolean isArray():返回此class对象是否表示一个数组类 boolean isEnum():返回此class对象是否表示一个枚举 boolean isInterface():返回此class对象是否表示一个接口 boolean isInstance(Object obj):判断obj是否是此class对象的实例，该方法可以完全代替instanceof操作符 123public interface Colorable &#123; public void value();&#125; 12345678910public class ClassInfo &#123; public static void main(String[] args) throws NoSuchMethodException, SecurityException &#123; Class&lt;Colorable&gt; cls=Colorable.class; System.out.println(cls.getMethod("value")); System.out.println(cls.isAnnotation()); System.out.println(cls.isInterface()); &#125;&#125; 结果 123public abstract void com.em.Colorable.value()falsetrue Java8中新增的方法参数反射 int getParameterCount():获取该构造器或方法的形参个数 Parameter[] getParameters():获取该构造器或方法的所有形参 getModifiers():获取修饰该形参的修饰符 String getName():获取形参名 Type getParameterizedType():获取带泛型的形参类型 Class&lt;?&gt;getType():获取形参类型 boolean isNamePresent():该方法返回该类的class文件中是否包含了方法的形参名信息 boolean isVarArgs():该方法用于判断该参数是否为个数可变的形参 12345public class Test &#123; public void getInfo(String str,List&lt;String&gt;list)&#123; System.out.println("成功"); &#125;&#125; 12345678910111213141516public class ClassInfo &#123; public static void main(String[] args) throws NoSuchMethodException, SecurityException &#123; Class&lt;Test&gt; cls=Test.class; Method med=cls.getMethod("getInfo", String.class,List.class); System.out.println(med.getParameterCount()); Parameter[] params=med.getParameters(); System.out.println(params.length); for(Parameter par:params)&#123; System.out.println(par.getName()); System.out.println(par.getType()); System.out.println(par.getParameterizedType()); &#125; &#125;&#125; 结果 1234567822arg0class java.lang.Stringclass java.lang.Stringarg1interface java.util.Listjava.util.List&lt;java.lang.String&gt; 反射生成对象 使用Class对象的newInstance()方法创建Class对象的实例，该方法要求要有默认构造器（比较常用） 先使用Class对象获取指定的Constructor对象，在调用Constructor对象的newInstance()方法来创建该Class对象对应类的实例 反射调用方法 Object invoke(Object obj,Object...args):该方法中的obj是执行该方法的主调，后面的args是执行该方法时传入该方法的实参 123456789public class Test &#123; public Test(String str) &#123; System.out.println(str); &#125; public void getInfo(String str)&#123; System.out.println(str); &#125;&#125; 1234567891011public class ClassInfo &#123; public static void main(String[] args) throws Exception &#123; Class&lt;Test&gt; cls=Test.class; Constructor&lt;Test&gt;construct=cls.getConstructor(String.class); Test test=construct.newInstance("初始化"); Method med=cls.getMethod("getInfo", String.class); med.invoke(test, "调用方法成功"); &#125;&#125; 结果 12初始化调用方法成功 接下来看官仔细看下面的栗子 12345678910public class Test &#123; public Test(String str) &#123; System.out.println(str); &#125; //私有方法 private void getInfo(String str)&#123; System.out.println(str); &#125;&#125; 1234567891011121314public class ClassInfo &#123; public static void main(String[] args) throws Exception &#123; Class&lt;Test&gt; cls=Test.class; Constructor&lt;Test&gt;construct=cls.getConstructor(String.class); Test test=construct.newInstance("初始化"); //为啥使用这个方法呢？ Method med=cls.getDeclaredMethod("getInfo", String.class); //为啥使用这个方法呢？ med.setAccessible(true); med.invoke(test, "调用方法成功"); &#125;&#125; 结果 12初始化调用方法成功 setAccessible(boolean flag):将值设为true，指示该Method在使用是应该取消Java语言的访问权限检查 访问成员变量值 getXxx(Object obj):获取obj对象的该成员变量的值。此处的Xxx对应8种基本类型，如果该成员变量的类型是引用类型的，则去掉Xxx部分 setXxx(Object obj,Xxx val):将obj对象的该成员变量设置为val值。此处的Xxx对应8中基本类型，如果该成员变量的类型是引用类型，则取消set后面的Xxx 以上两个方法可以方法所有的成员变量，包括private的私有成员变量 1234567891011121314151617public class Test &#123; private int num; public Test(String str) &#123; System.out.println(str); &#125; private void getInfo(String str)&#123; System.out.println(str); &#125; public int getNum() &#123; return num; &#125; public void setNum(int num) &#123; this.num = num; &#125; &#125; 12345678910111213141516public class ClassInfo &#123; public static void main(String[] args) throws Exception &#123; Class&lt;Test&gt; cls=Test.class; Constructor&lt;Test&gt;construct=cls.getConstructor(String.class); Test test=construct.newInstance("初始化"); Method med=cls.getDeclaredMethod("getInfo", String.class); med.setAccessible(true); med.invoke(test, "调用方法成功"); Field fld=cls.getDeclaredField("num"); fld.setAccessible(true); fld.setInt(test, 12); System.out.println(fld.getInt(test)); &#125;&#125; 结果 123初始化调用方法成功12 操作数组java.lang.reflect包下有一个Array类，其可以动态创建数组 static Object newInstance(Class&lt;?&gt;componentType,int...length):创建一个具有指定的元素类型、指定维度的新数组 static xxx getXxx(Object array,int index):返回array数组中第index个元素。其中xxx是各种基本数据类型，如果数组元素是引用类型，则该方法变为get() static void setXxx(Object array,int index,xxx val):将array数组中低index 个元素的值设为val，其中xxx是各种基本数据类型，如果数组元素是引用类型，则该方法变为set() 12345678910public class ArrayInfo &#123; public static void main(String[] args) &#123; Object arrays=Array.newInstance(String.class, 3); Array.set(arrays, 0, "第一个"); Array.set(arrays, 1, "第二个"); Array.set(arrays, 2, "第三个"); System.out.println(Array.get(arrays, 2)); &#125;&#125;]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java代理设计模式详解]]></title>
    <url>%2F2016%2F06%2F01%2Fjvm20160601%2F</url>
    <content type="text"><![CDATA[代理即通过代理类，找到适合你的实现类。相当于现实生活中的中介的角色，你想租房子，这个时候你又不想自己找房子，那你可以找中介，通过中介找到合适自己的房子，同时你也可以让中介帮你签合同等其他事宜。代理存在静态代理和动态代理两种 静态代理123public interface Sourceable &#123; public void method(); &#125; 1234567public class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125; &#125; 123456789101112131415161718192021public class Proxy implements Sourceable &#123; private Source source; public Proxy()&#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; //新增的处理，个性化处理 before(); source.method(); atfer(); &#125; private void atfer() &#123; System.out.println("after proxy!"); &#125; private void before() &#123; System.out.println("before proxy!"); &#125; &#125; 测试类 12345678public class ProxyTest &#123; public static void main(String[] args) &#123; Sourceable source = new Proxy(); source.method(); &#125; &#125; 动态代理JDK动态代理、AOP动态代理 Proxy提供了用于创建动态代理类和代理对象的静态方法，它也是所有动态代理类的父类 创建动态代理类 static Class&lt;?&gt;getProxyClass(ClassLoader loader,Class&lt;?&gt;…interfaces):创建一个动态代理类锁对应的Class对象，该代理类将实现interface所指定的多个接口，第一个ClassLoader 参数指生成动态代理类的类加载器。 创建动态代理对象 static Object newProxyInstance(ClassLoader loader,Class&lt;?&gt;…interfaces,InvocationHadnler h):直接创建一个动态代理对象，该代理对象的实现类实现了interfaces指定的系列接口，执行代理对象的每个方法时都会被替换执行InvocationHadnler对象的invoke方法 123public interface Colorable &#123; public void value();&#125; 123456public class RedColor implements Colorable&#123; @Override public void value() &#123; System.out.println("--------------red-------------"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class ColorableProxy implements InvocationHandler &#123; private Colorable colorable; private Colorable proxy; public ColorableProxy(Colorable colorable) &#123; this.colorable = colorable; this.proxy = (Colorable) Proxy.newProxyInstance( Colorable.class.getClassLoader(), new Class&lt;?&gt;[] &#123; Colorable.class &#125;, this); &#125; public Colorable getProxy() &#123; return proxy; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName(); System.out.println("===========starting invoke function:" + methodName + "=========="); Object result = method.invoke(colorable, args); System.out.println("=========== invoke function:" + methodName + " success=========="); return result; &#125; public static void main(String[] args) &#123; Colorable proxy = new ColorableProxy(new RedColor()).getProxy(); //真正调用invoke方法是在这一步才被激发的，可以debug试一下 proxy.value(); &#125;&#125; 结果 123===========starting invoke function:value==========--------------red-------------=========== invoke function:value success========== proxy：代表动态代理对象 method：代表正在执行的方法 args:代表调用目标方法时传入的实参 AOP动态代理 123public interface Colorable &#123; public void value();&#125; 123456public class RedColor implements Colorable&#123; @Override public void value() &#123; System.out.println("--------------red-------------"); &#125;&#125; 123456public class ToolUtility &#123; public void method()&#123; System.out.println("运行工具方法"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class ColorableAOProxy implements InvocationHandler &#123; private Colorable colorable; private Colorable proxy; public ColorableAOProxy(Colorable colorable) &#123; this.colorable = colorable; this.proxy = (Colorable) Proxy.newProxyInstance( Colorable.class.getClassLoader(), new Class&lt;?&gt;[] &#123; Colorable.class &#125;, this); &#125; public Colorable getProxy() &#123; return proxy; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; ToolUtility tool = new ToolUtility(); tool.method(); String methodName = method.getName(); System.out.println("===========starting invoke function:" + methodName + "=========="); Object result = method.invoke(colorable, args); System.out.println("=========== invoke function:" + methodName + " success=========="); return result; &#125; public static void main(String[] args) &#123; Colorable proxy = new ColorableAOProxy(new RedColor()).getProxy(); // 真正调用invoke方法是在这一步才被激发的，可以debug试一下 proxy.value(); &#125;&#125; 结果 1234运行工具方法===========starting invoke function:value==========--------------red-------------=========== invoke function:value success==========]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA类加载机制全解析]]></title>
    <url>%2F2016%2F05%2F31%2Fjvm20160531%2F</url>
    <content type="text"><![CDATA[当程序使用某个类时，如果该类还没被初始化，加载到内存中，则系统会通过加载、连接、初始化三个过程来对该类进行初始化。该过程就被称为类的初始化 类加载指将类的class文件读入内存，并为之创建一个java.lang.Class的对象 类文件来源 从本地文件系统加载的class文件 从JAR包加载class文件 从网络加载class文件 把一个Java源文件动态编译，并执行加载 类加载器通常无须等到“首次使用”该类时才加载该类，JVM允许系统预先加载某些类 类加载器类加载器就是负责加载所有的类，将其载入内存中，生成一个java.lang.Class实例。一旦一个类被加载到JVM中之后，就不会再次载入了。 根类加载器（Bootstrap ClassLoader）：其负责加载Java的核心类，比如String、System这些类 拓展类加载器（Extension ClassLoader）：其负责加载JRE的拓展类库 系统类加载器（System ClassLoader）：其负责加载CLASSPATH环境变量所指定的JAR包和类路径 用户类加载器：用户自定义的加载器，以类加载器为父类 类加载器之间的父子关系并不是继承关系，是类加载器实例之间的关系 1234567891011public static void main(String[] args) throws IOException &#123; ClassLoader systemLoader = ClassLoader.getSystemClassLoader(); System.out.println("系统类加载"); Enumeration&lt;URL&gt; em1 = systemLoader.getResources(""); while (em1.hasMoreElements()) &#123; System.out.println(em1.nextElement()); &#125; ClassLoader extensionLader = systemLoader.getParent(); System.out.println("拓展类加载器" + extensionLader); System.out.println("拓展类加载器的父" + extensionLader.getParent());&#125; 结果 1234系统类加载file:/E:/gaode/em/bin/拓展类加载器sun.misc.Launcher$ExtClassLoader@6d06d69c拓展类加载器的父null 为什么根类加载器为NULL? 根类加载器并不是Java实现的，而且由于程序通常须访问根加载器，因此访问扩展类加载器的父类加载器时返回NULL JVM类加载机制 全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 URLClassLoader类URLClassLoader为ClassLoader的一个实现类，该类也是系统类加载器和拓展类加载器的父类（继承关系）。它既可以从本地文件系统获取二进制文件来加载类，也可以远程主机获取二进制文件来加载类。 两个构造器 URLClassLoader(URL[] urls):使用默认的父类加载器创建一个ClassLoader对象，该对象将从urls所指定的路径来查询并加载类 URLClassLoader(URL[] urls,ClassLoader parent):使用指定的父类加载器创建一个ClassLoader对象，其他功能与前一个构造器相同 12345678910111213141516171819202122232425262728293031323334353637import java.net.MalformedURLException;import java.net.URL;import java.net.URLClassLoader;import java.sql.Connection;import java.sql.SQLException;import java.util.Properties;import com.mysql.jdbc.Driver;public class GetMysql &#123; private static Connection conn; public static Connection getConn(String url,String user,String pass) throws MalformedURLException, InstantiationException, IllegalAccessException, ClassNotFoundException, SQLException&#123; if(conn==null)&#123; URL[]urls=&#123;new URL("file:mysql-connector-java-5.1.18.jar")&#125;; URLClassLoader myClassLoader=new URLClassLoader(urls); Driver driver=(Driver) myClassLoader.loadClass("com.mysql.jdbc.Driver").newInstance(); Properties pros=new Properties(); pros.setProperty("user", user); pros.setProperty("password", pass); conn=driver.connect(url, pros); &#125; return conn; &#125; public static method1 getConn() throws MalformedURLException, InstantiationException, IllegalAccessException, ClassNotFoundException, SQLException&#123; URL[]urls=&#123;new URL("file:com.em")&#125;; URLClassLoader myClassLoader=new URLClassLoader(urls); method1 driver=(method1) myClassLoader.loadClass("com.em.method1").newInstance(); return driver; &#125; public static void main(String[] args) throws MalformedURLException, InstantiationException, IllegalAccessException, ClassNotFoundException, SQLException &#123; System.out.println(getConn("jdbc:mysql://10.10.16.11:3306/auto?useUnicode=true&amp;characterEncoding=utf8&amp;allowMultiQueries=true", "jiji", "jiji")); System.out.println(getConn()); &#125;&#125; 获得URLClassLoader对象后，调用loanClass()方法来加载指定的类 自定义类加载器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.lang.reflect.Method;public class CompileClassLoader extends ClassLoader&#123; // 读取一个文件的内容 @SuppressWarnings("resource") private byte[] getBytes(String filename) throws IOException &#123; File file = new File(filename); long len = file.length(); byte[] raw = new byte[(int) len]; FileInputStream fin = new FileInputStream(file); // 一次读取class文件的全部二进制数据 int r = fin.read(raw); if (r != len) throw new IOException("无法读取全部文件" + r + "!=" + len); fin.close(); return raw; &#125; // 定义编译指定java文件的方法 private boolean compile(String javaFile) throws IOException &#123; System.out.println("CompileClassLoader:正在编译" + javaFile + "…….."); // 调用系统的javac命令 Process p = Runtime.getRuntime().exec("javac" + javaFile); try &#123; // 其它线程都等待这个线程完成 p.waitFor(); &#125; catch (InterruptedException ie) &#123; System.out.println(ie); &#125; // 获取javac 的线程的退出值 int ret = p.exitValue(); // 返回编译是否成功 return ret == 0; &#125; // 重写Classloader的findCLass方法 protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class clazz = null; // 将包路径中的.替换成斜线/ String fileStub = name.replace(".", "/"); String javaFilename = fileStub + ".java"; String classFilename = fileStub + ".class"; File javaFile = new File(javaFilename); File classFile = new File(classFilename); // 当指定Java源文件存在，且class文件不存在，或者Java源文件的修改时间比class文件//修改时间晚时，重新编译 if (javaFile.exists() &amp;&amp; (!classFile.exists()) || javaFile.lastModified() &gt; classFile.lastModified()) &#123; try &#123; // 如果编译失败，或该Class文件不存在 if (!compile(javaFilename) || !classFile.exists()) &#123; throw new ClassNotFoundException("ClassNotFoundException:" + javaFilename); &#125; &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; // 如果class文件存在，系统负责将该文件转化成class对象 if (classFile.exists()) &#123; try &#123; // 将class文件的二进制数据读入数组 byte[] raw = getBytes(classFilename); // 调用Classloader的defineClass方法将二进制数据转换成class对象 clazz = defineClass(name, raw, 0, raw.length); &#125; catch (IOException ie) &#123; ie.printStackTrace(); &#125; &#125; // 如果claszz为null,表明加载失败，则抛出异常 if (clazz == null) &#123; throw new ClassNotFoundException(name); &#125; return clazz; &#125; // 定义一个主方法 public static void main(String[] args) throws Exception &#123; // 如果运行该程序时没有参数，即没有目标类 if (args.length &lt; 1) &#123; System.out.println("缺少运行的目标类，请按如下格式运行java源文件："); System.out.println("java CompileClassLoader ClassName"); &#125; // 第一个参数是需要运行的类 String progClass = args[0]; // 剩下的参数将作为运行目标类时的参数，所以将这些参数复制到一个新数组中 String progargs[] = new String[args.length - 1]; System.arraycopy(args, 1, progargs, 0, progargs.length); CompileClassLoader cl = new CompileClassLoader(); // 加载需要运行的类 Class&lt;?&gt; clazz = cl.loadClass(progClass); // 获取需要运行的类的主方法 Method main = clazz.getMethod("main", (new String[0]).getClass()); Object argsArray[] = &#123; progargs &#125;; main.invoke(null, argsArray); &#125;&#125; JVM中除了根类加载器之外的所有类的加载器都是ClassLoader子类的实例，通过重写ClassLoader中的方法，实现自定义的类加载器 loadClass(String name,boolean resolve):为ClassLoader的入口点，根据指定名称来加载类，系统就是调用ClassLoader的该方法来获取制定累对应的Class对象 findClass(String name):根据指定名称来查找类 推荐使用findClass方法 类的链接当类被加载后，系统会为之生成一个Class对象，接着将会进入连接阶段，链接阶段负责把类的二进制数据合并到JRE中 三个阶段 验证：检验被加载的类是否有正确的内部结构，并和其他类协调一致 准备：负责为类的类变量分配内存。并设置默认初始值 解析：将类的二进制数据中的符号引用替换成直接引用 类的初始化JVM负责对类进行初始化，主要对类变量进行初始化 在Java中对类变量进行初始值设定有两种方式：①声明类变量是指定初始值②使用静态代码块为类变量指定初始值 JVM初始化步骤 假如这个类还没有被加载和连接，则程序先加载并连接该类 假如该类的直接父类还没有被初始化，则先初始化其直接父类 假如类中有初始化语句，则系统依次执行这些初始化语句 类初始化时机 创建类实例。也就是new的方式 调用某个类的类方法 访问某个类或接口的类变量，或为该类变量赋值 使用反射方式强制创建某个类或接口对应的java.lang.Class对象 初始化某个类的子类，则其父类也会被初始化 直接使用java.exe命令来运行某个主类 类加载机制（类加载过程和类加载器）]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象与内存控制]]></title>
    <url>%2F2016%2F05%2F26%2Fjvm20160526%2F</url>
    <content type="text"><![CDATA[JVM的垃圾回收机制是由一条后台线程执行的，其本身也是非常消耗内存的，因此，滥用创建对象，会导致性能大大下降，对内存的分配的了解就显得尤为重要 变量分类局部变量 形参：存在于方法签名中定义的局部变量，有方法调用者为其赋值，随着方法的结束而消亡 方法内部变量：在方法内部定义的局部变量，必须在方法内对其进行显示初始化，随着方法的结束而消亡 代码块内部的局部变量：在代码块内定义的局部变量，必须在代码块内对其显式初始化，随着代码块结束而消亡 成员变量 实例变量：非静态的成员变量，随着对象的产生，进行初始化等操作，对象结束变量也就消亡 类变量：静态的成员变量，带有static修饰符，随着类初始化产生，随着类消失而消失 在创建变量的时候，一定要合法的前向引用。其含义就是先定义的变量不能引用后定义的变量，反之则可以 变量的内存分配在同一个JVM中每一个类只会存在一个Class对象，因此JVM只要分配一块内存空间给类变量就可以了，而实例变量则每次创建对象都要为其分配一块内存，几个实例就要创建几块内存空间 实例变量的初始化时机 定义变量的时候 代码块中 构造器中 定义实例变量时指定的初始值、初始化块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将被提取到构造器中，也就是说在编译后，初始化都会被放在构造器中按先后顺序进行初始化赋值 类变量初始化时机同一个JVM中，类变量只能初始化一次 定义变量的时候 静态代码块 父类构造器在创建Java对象的时候，都会先去执行该类的父类对象的非静态代码块和构造器，最后才是该类的非静态代码块和构造器 所谓的隐式调用和显式调用，其实就是有没有用super去调用父类的构造器的区别。 如果父类还没被初始化过，则会最先对类变量进行初始化 访问子类对象的实例变量子类的方法可以访问父类的实例变量，这是因为子类继承父类就会获得父类的成员变量和方法，但父类的方法不能访问子类的实例变量，因为父类无法知道哪个子类继承他 而且子父类中的成员变量（类变量和实例变量）是相互独立的，父类中的成员变量不会被子类中同名的变量覆盖 123456789101112131415161718192021222324class Base &#123; static int count = 2;&#125;class Mid extends Base &#123; static int count = 20; &#125;public class Sub extends Mid &#123; static int count = 200; public static void main(String[] args) &#123; // 创建一个Sub对象 Sub s = new Sub(); // 将Sub对象向上转型后赋为Mid、Base类型的变量 Mid s2m = s; Base s2b = s; // 分别通过3个变量来访问count实例变量 System.out.println(s.count); System.out.println(s2m.count); System.out.println(s2b.count); &#125;&#125; 结果 200202 当创建Sub的时候，会初始化Base、Mid和Sub三个对象，同时也就存在三个count变量了，也就是说有三块内存保存着这三个对象和count变量，以s2m变量为例，s2m拥有的地址是Sub对象的堆地址，但s2m变量类是是Mid，则会去寻找Mid下的变量值 也就说成员变量的值取决于声明该变量声明时是所用的类型 访问子类对象的方法子类可以重写父类的方法，子类也可以通过super的方式调用父类的方法，在多态的情况下，子类重写的方法会覆盖掉父类的方法 12345678910111213141516171819202122232425262728class Base&#123; int count =2; public void display()&#123; System.out.println(this.count); &#125;&#125;class Derived extends Base&#123; int count =20; @Override public void display() &#123; System.out.println(this.count); &#125;&#125;public class FieldAndMethod &#123; public static void main(String[] args) &#123; Base b=new Base(); System.out.println(b.count); b.display(); Derived d=new Derived(); System.out.println(d.count); d.display(); Base db=new Derived(); System.out.println(db.count); db.display(); Base b2d=d; System.out.println(b2d.count); &#125;&#125; 结果 2220202202 一切在你执行这段代码之后，你就会明白一切了。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java枚举全解析]]></title>
    <url>%2F2016%2F05%2F25%2Fenum20150525%2F</url>
    <content type="text"><![CDATA[enum关键字与class和interface地位相同，其一样有成员变量、方法、可以实现一个或多个接口，也可以有构造器 枚举类与普通类的区别 枚举类的父类是java.lang.Enum类 非抽象枚举类默认使用final修饰，不能存在子类 枚举类的构造器只能使用private修饰符，默认就是private 枚举类的所有实例必须在枚举类的第一行显式列出，否则这个枚举类永远不能产生实例，列出的实例，系统会自动添加public static final修饰 抽象的枚举类，系统默认使用abstract修饰，而不用final修饰 1234567package com.em;public enum EmFestival &#123; //会产生如下四个实例 SPRING,SUMMER,FALL,WINTER;&#125; 枚举值就代表可能会产生的实例 123456789101112131415161718192021package com.em;public class FestivalTest &#123; public static String jude(EmFestival v)&#123; switch (v) &#123; case SPRING: return "春天"; case SUMMER: return "夏天"; case FALL: return "秋天"; case WINTER: return "冬天"; &#125; return null; &#125; public static void main(String[] args) &#123; System.out.println(jude(EmFestival.SPRING)); &#125;&#125; 为什么使用枚举类？ 存在命名空间，可以将其他的静态常量区分开 打印输出的意义明确，不想普通常量那样，都用数字表示含义，无法直接表达含义 枚举类方法 compare(E o)：用于与指定的枚举对象比较顺序，同一个枚举实例只能与相同类型的枚举实例进行比较。如果该枚举对象位于指定枚举对象之后，则返回正整数 name()：返回此枚举实例的名称 ordinal()：返回枚举值在枚举类中的索引值（从0开始） toString()：与name方法一致，推荐使用toString valueOf()：一个静态方法，用于返回指定枚举类中指定名称的枚举值 123456789101112131415package com.em;public enum EmFestival &#123; //会产生如下四个实例 SPRING("春天"),SUMMER("夏天"),FALL("秋天"),WINTER("冬天"); private final String fest; private EmFestival(String fest)&#123; this.fest=fest; &#125; public String getFest() &#123; return fest; &#125; &#125; 实现接口的枚举类枚举类实现接口，与正常类没有什么区别 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.em;import java.io.FileDescriptor;import java.io.IOException;import sun.nio.ch.SelChImpl;import sun.nio.ch.SelectionKeyImpl;public enum EmFestival implements SelChImpl&#123;// 会产生如下四个实例 SPRING("春天"),SUMMER("夏天"),FALL("秋天"),WINTER("冬天"); @Override public boolean isOpen() &#123; // TODO Auto-generated method stub return false; &#125; @Override public void close() throws IOException &#123; // TODO Auto-generated method stub &#125; @Override public FileDescriptor getFD() &#123; // TODO Auto-generated method stub return null; &#125; @Override public int getFDVal() &#123; // TODO Auto-generated method stub return 0; &#125; @Override public void kill() throws IOException &#123; // TODO Auto-generated method stub &#125; @Override public void translateAndSetInterestOps(int arg0, SelectionKeyImpl arg1) &#123; // TODO Auto-generated method stub &#125; @Override public boolean translateAndSetReadyOps(int arg0, SelectionKeyImpl arg1) &#123; // TODO Auto-generated method stub return false; &#125; @Override public boolean translateAndUpdateReadyOps(int arg0, SelectionKeyImpl arg1) &#123; // TODO Auto-generated method stub return false; &#125; @Override public int validOps() &#123; // TODO Auto-generated method stub return 0; &#125; private final String fest; private EmFestival(String fest)&#123; this.fest=fest; &#125; public String getFest() &#123; return fest; &#125; &#125; 包含抽象方法的枚举类枚举类里定义抽象方法时不能使用abstract关键字将枚举类定义成抽象类（因为系统自动会添加abastract），但因为枚举类需要显式创建枚举值，而不是作为父类，所以定义每个枚举值时必须为抽象方法提供实现，否则出现编译错误 1234567891011121314151617181920212223242526272829303132333435363738394041package com.em;public enum EmFestival &#123; // 会产生如下四个实例 SPRING("春天") &#123; @Override public String getFestival() &#123; return "春天"; &#125; &#125;, SUMMER("夏天") &#123; @Override public String getFestival() &#123; return "夏天"; &#125; &#125;, FALL("秋天") &#123; @Override public String getFestival() &#123; return "秋天"; &#125; &#125;, WINTER("冬天") &#123; @Override public String getFestival() &#123; return "冬天"; &#125; &#125;; public abstract String getFestival(); private final String fest; private EmFestival(String fest) &#123; this.fest = fest; &#125; public String getFest() &#123; return fest; &#125;&#125;]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java泛型全解析]]></title>
    <url>%2F2016%2F05%2F25%2Fgeneric20160525%2F</url>
    <content type="text"><![CDATA[把一个对象放进集合中之后，集合就会忘记这个对象的数据类型，当再次取出该对象时，该对象的编译类型就变成Object类型了 为了解决上面的问题，就引出了泛型这一个概念 泛型接口和类12345678910111213141516171819202122public class Fruit&lt;T&gt; &#123; private T info; public Fruit(T info) &#123; this.info = info; &#125; public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public static void main(String[] args) &#123; Fruit&lt;String&gt; lt = new Fruit&lt;String&gt;("苹果"); System.out.println(lt.getInfo()); Fruit&lt;Integer&gt; intg = new Fruit&lt;Integer&gt;(1); System.out.println(intg.getInfo()); &#125;&#125; 可以灵活的封装，并且同时有可以限定类型，泛型的接口和类更像是一种通用的模型，模型内部的类型由使用者自己限定 无论为泛型的类型形参传入哪一种类型的实参，对于Java来说，他们依然被当成同一个类处理，在内存中也只占用一块内存空间，因此在静态方法、静态初始化块或者静态变量的声明和初始化中不允许使用类型形参 泛型接口和类的子类123456789101112public class Apple extends Fruit&lt;String&gt;&#123; public Apple(String info) &#123; super(info); &#125; @Override public String getInfo() &#123; return super.getInfo(); &#125;&#125; 使用泛型接口或类的时候，虽然可以不加&lt;&gt;部分，但是推荐加上，不然还要强制类型转换等麻烦操作 重写父类方法或者实现接口的时候，返回值一定要跟父类（接口）一致 设定类型形参的上限12345678910111213141516171819202122public class Fruit&lt;T extends String &amp; java.io.Serializable&gt; &#123; private T info; public Fruit(T info) &#123; this.info = info; &#125; public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public static void main(String[] args) &#123; Fruit&lt;String&gt; lt = new Fruit&lt;&gt;("苹果"); System.out.println(lt.getInfo()); Fruit&lt;?&gt; intg = new Fruit&lt;&gt;("1"); System.out.println(intg.getInfo()); &#125;&#125; 可以有多个限定条件，存在多个限定条件的时候，使用&amp;连接 至多一个父类上限，多个接口上限 接口上限要在类上线后面 类型通配符使用类型通配符的类是各种该类的泛型的父类 12345678910111213141516171819202122public class Fruit&lt;T&gt; &#123; private T info; public Fruit(T info) &#123; this.info = info; &#125; public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public static void main(String[] args) &#123; Fruit&lt;String&gt; lt = new Fruit&lt;&gt;("苹果"); System.out.println(lt.getInfo()); Fruit&lt;?&gt; intg = new Fruit&lt;&gt;(1); System.out.println(intg.getInfo()); &#125;&#125; 在Java 7以后可以使用菱形语法，在构造器后不需要完整的泛型信息 “?”用于操作具体的某个泛型类的时候，还未确定最终使用时，采用的对象类型，就用问号作为占位的含义 通配符的上限设定1Fruit&lt;? extends String&gt; intg = new Fruit&lt;&gt;("1"); 使用extends的方式限定?必须是String类型或者是其子类类型 通配符的下限设定123static &lt;T,B&gt; void getCollection(Fruit&lt;? super T&gt;b)&#123; System.out.println(b);&#125; 一定要是T或者T的父类 泛型方法12345修饰符&lt;T,S&gt;返回值类型 方法名(形参列表)&#123; 方法体&#125; 123static &lt;T,B&gt; void getCollection(B[]b,Collection&lt;T&gt; c)&#123; &#125; 多个类型形参之间用逗号分隔 所有类型形参声明放在修饰符和方法返回类型之间 方法中定义的类型形参只能在该方法里使用，而接口或类中定义的类型形参可以在整个接口、类中使用 方法中的泛型参数无需显式传入实际类型参数 123456789101112131415161718192021public class Fruit&lt;T extends String &amp; java.io.Serializable&gt; &#123; private T info; public Fruit(T info) &#123; this.info = info; &#125; static &lt;T,B&gt; void getCollection(B[]b)&#123; System.out.println(b); &#125; public T getInfo() &#123; return info; &#125; public void setInfo(T info) &#123; this.info = info; &#125; public static void main(String[] args) &#123; Fruit.getCollection(new String[]&#123;"1"&#125;); &#125;&#125; 存在泛型构造器，不能使用”菱形“语法 123public &lt;T&gt; Fruit(T info) &#123; System.out.println(info);&#125; 泛型方法与类型通配符的区别 泛型方法允许类型形参被用来表示方法的一个或多个参数之间的类型依赖关系，或者方法返回值与参数之间的类型依赖关系，如果没有这样的类型依赖关系，就不应该使用泛型方法 类型通配符既可以在方法签名中定义形参的类型，也可以用于定义变量类型，但泛型方法中的类型形参必须在对应方法中显式声明 擦除和转换擦除 当把一个具有泛型信息的对象赋值给另一个没有泛型信息的变量时，尖括号中的泛型信息就会被擦除扔掉 转换 当把一个没有泛型信息的对象赋值给另一个泛型信息的变量时，不会发生报错，会自动转换]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息推送原理]]></title>
    <url>%2F2016%2F05%2F20%2Fsendmessage20160520%2F</url>
    <content type="text"><![CDATA[消息模式归根结底，企业应用系统就是对数据的处理，而对于一个拥有多个子系统的企业应用系统而言，它的基础支撑无疑就是对消息的处理。与对象不同，消息本质上是一种数据结构（当然，对象也可以看做是一种特殊的消息），它包含消费者与服务双方都能识别的数据，这些数据需要在不同的进程（机器）之间进行传递，并可能会被多个完全不同的客户端消费。消息传递相较文件传递与远程过程调用（RPC）而言，似乎更胜一筹，因为它具有更好的平台无关性，并能够很好地支持并发与异步调用。 对于Web Service与RESTful而言，则可以看做是消息传递技术的一种衍生或封装。 消息传递流程发起请求将消息序列化传递→接受请求将消息反序列化→返回结果给请求端将消息序列化传递 按照网络通信原理，需要实现这个需要做的就是将请求转换成流，通过传输协议传输至远端，远端计算机在接收到请求的流后进行处理，处理完毕后将结果转化为流，并通过传输协议返回给调用端。 处理消息的方式一种方式是广播机制，这时消息通道中的消息在出列的同时，还需要复制消息对象，将消息传递给多个订阅者。 另一种方式则属于抢占机制，它遵循同步方式，在同一时间只能有一个订阅者能够处理该消息。 消息通道（Message Channel）模式 平台无关性 生产者与消费者只要遵守消息通道的数据传递格式、处理消息的机制与时机就可以了 支持并发 消息是以队列的形式存在，先进先出，可以很好的保证消息的顺畅 异步调用 生产者与消费者之间是通过消息通道（Message Channel）连接，是低耦合的。 特点 生产者和消费者必须知道通道资源的位置，否则无法知道从哪里发送（接受）消息（考虑引用Lookup服务进行查找） 以队列形式存在，先进先出 可以存在多个生产者和消费者 关键字解释Lookup服务 Lookup服务是用来查找通道资源，可以将与通道相关的信息存储到配置文件中，Lookup服务首先通过读取配置文件来获得通道。在JMS中就可以通过JNDI来获取消息通道Queue。 发布者-订阅者（Publisher-Subscriber）模式发布者-订阅者（Publisher-Subscriber）模型两种模型的选择：拉模型与推模型 拉模型是由消息的消费者发起的，主动权把握在消费者手中，它会根据自己的情况对生产者发起调用。 在基于消息的分布式系统中，拉模型的消费者通常以Batch Job的形式，根据事先设定的时间间隔，定期侦听通道的情况。一旦发现有消息传递进来，就会转而将消息传递给真正的处理器（也可以看做是消费者）处理消息，执行相关的业务。 推模型的主动权常常掌握在生产者手中，消费者被动地等待生产者发出的通知，这就要求生产者必须了解消费者的相关信息。 对于推模型而言，消费者无需了解生产者。在生产者通知消费者时，传递的往往是消息（或事件），而非生产者自身。同时，生产者还可以根据不同的情况，注册不同的消费者，又或者在封装的通知逻辑中，根据不同的状态变化，通知不同的消费者。 特点 广播机制，这时消息通道中的消息在出列的同时，还需要复制消息对象，将消息传递给多个订阅者 点对点P2P模型 特点 属于抢占机制，它遵循同步方式，在同一时间只能有一个订阅者能够处理该消息。 消息路由（Message Router）模式 无论是Message Channel模式，还是Publisher-Subscriber模式，队列在其中都扮演了举足轻重的角色。然而，在企业应用系统中，当系统变得越来越复杂时，对性能的要求也会越来越高，此时对于系统而言，可能就需要支持同时部署多个队列，并可能要求分布式部署不同的队列。这些队列可以根据定义接收不同的消息，例如订单处理的消息，日志信息，查询任务消息等。这时，对于消息的生产者和消费者而言，并不适宜承担决定消息传递路径的职责。事实上，根据S单一职责原则，这种职责分配也是不合理的，它既不利于业务逻辑的重用，也会造成生产者、消费者与消息队列之间的耦合，从而影响系统的扩展。 既然这三种对象（组件）都不宜承担这样的职责，就有必要引入一个新的对象专门负责传递路径选择的功能，这就是所谓的Message Router模式。 通过消息路由，我们可以配置路由规则指定消息传递的路径，以及指定具体的消费者消费对应的生产者。例如指定路由的关键字，并由它来绑定具体的队列与指定的生产者（或消费者）。路由的支持提供了消息传递与处理的灵活性，也有利于提高整个系统的消息处理能力。同时，路由对象有效地封装了寻找与匹配消息路径的逻辑，就好似一个调停者（Meditator），负责协调消息、队列与路径寻址之间关系。 应用级协议为了应用的方便，业界推出了很多基于此原理之上的应用级的协议，使得大家可以不用去直接操作这么底层的东西，通常应用级的远程通信协议会提供： 为了避免直接做流操作这么麻烦，提供一种更加易用或贴合语言的标准传输格式； 网络通信机制的实现，就是替你完成了将传输格式转化为流，通过某种传输协议传输至远端计算机，远端计算机在接收到流后转化为传输格式，并进行存储或以某种方式通知远端计算机。 在学习应用级的远程通信协议时，我们可以带着这几个问题进行学习： 传输的标准格式是什么？ 怎么样将请求转化为传输的流？ 怎么接收和处理流？ 传输协议是？ 不过应用级的远程通信协议并不会在传输协议上做什么多大的改进，主要是在流操作方面，让应用层生成流和处理流的这个过程更加的贴合所使用的语言或标准，至于传输协议则通常都是可选的，在java领域中知名的有：RMI、XML-RPC、Binary-RPC、SOAP、CORBA、JMS、HTTP，来具体的看看这些远程通信的应用级协议。 RMI（远程方法调用） RMI是个典型的为java定制的远程通信协议，我们都知道，在single vm中，我们可以通过直接调用java object instance来实现通信，那么在远程通信时，如果也能按照这种方式当然是最好了，这种远程通信的机制成为RPC（Remote Procedure Call），RMI正是朝着这个目标而诞生的。 RMI 采用stubs 和 skeletons 来进行远程对象(remote object)的通讯。stub 充当远程对象的客户端代理，有着和远程对象相同的远程接口，远程对象的调用实际是通过调用该对象的客户端代理对象stub来完成的，通过该机制RMI就好比它是本地工作，采用tcp/ip协议，客户端直接调用服务端上的一些方法。优点是强类型，编译期可检查错误，缺点是只能基于JAVA语言，客户机与服务器紧耦合。 来看下基于RMI的一次完整的远程通信过程的原理： 客户端发起请求，请求转交至RMI客户端的stub类； stub类将请求的接口、方法、参数等信息进行序列化； 基于socket将序列化后的流传输至服务器端； 服务器端接收到流后转发至相应的skelton类； skelton类将请求的信息反序列化后调用实际的处理类； 处理类处理完毕后将结果返回给skelton类； Skelton类将结果序列化，通过socket将流传送给客户端的stub； stub在接收到流后反序列化，将反序列化后的Java Object返回给调用者。 根据原理来回答下之前学习应用级协议带着的几个问题： 传输的标准格式是什么？是Java ObjectStream。 怎么样将请求转化为传输的流？基于Java串行化机制将请求的java object信息转化为流。 怎么接收和处理流？根据采用的协议启动相应的监听端口，当有流进入后基于Java串行化机制将流进行反序列化，并根据RMI协议获取到相应的处理对象信息，进行调用并处理，处理完毕后的结果同样基于java串行化机制进行返回。 传输协议是？Socket。 XML-RPC RPC使用C/S方式，采用http协议，发送请求到服务器，等待服务器返回结果。这个请求包括一个参数集和一个文本集，通常形成“classname.methodname”形式。优点是跨语言跨平台，C端、S端有更大的独立性，缺点是不支持对象，无法在编译器检查错误，只能在运行期检查。 XML-RPC也是一种和RMI类似的远程调用的协议，它和RMI的不同之处在于它以标准的xml格式来定义请求的信息(请求的对象、方法、参数等)，这样的好处是什么呢，就是在跨语言通讯的时候也可以使用。 来看下XML-RPC协议的一次远程通信过程： 客户端发起请求，按照XML-RPC协议将请求信息进行填充； 填充完毕后将xml转化为流，通过传输协议进行传输； 接收到在接收到流后转换为xml，按照XML-RPC协议获取请求的信息并进行处理； 处理完毕后将结果按照XML-RPC协议写入xml中并返回。 同样来回答问题： 传输的标准格式是？标准格式的XML。 怎么样将请求转化为传输的流？将XML转化为流。 怎么接收和处理流？通过监听的端口获取到请求的流，转化为XML，并根据协议获取请求的信息，进行处理并将结果写入XML中返回。 传输协议是？Http。 Binary-RPC Binary-RPC看名字就知道和XML-RPC是差不多的了，不同之处仅在于传输的标准格式由XML转为了二进制的格式。 同样来回答问题： 传输的标准格式是？标准格式的二进制文件。 怎么样将请求转化为传输的流？将二进制格式文件转化为流。 怎么接收和处理流？通过监听的端口获取到请求的流，转化为二进制文件，根据协议获取请求的信息，进行处理并将结果写入XML中返回。 传输协议是？Http。 SOAP SOAP原意为Simple Object Access Protocol，是一个用于分布式环境的、轻量级的、基于XML进行信息交换的通信协议，可以认为SOAP是XML RPC的高级版，两者的原理完全相同，都是http+XML，不同的仅在于两者定义的XML规范不同，SOAP也是Webservice采用的服务调用协议标准，因此在此就不多加阐述了。 Web Service提供的服务是基于web容器的，底层使用http协议，类似一个远程的服务提供者，比如天气预报服务，对各地客户端提供天气预报，是一种请求应答的机制，是跨系统跨平台的。就是通过一个servlet，提供服务出去。 首先客户端从服务器获得WebService的WSDL，同时在客户端生成一个代理类(Proxy Class)，这个代理类负责与WebService服务器进行Request和Response。当一个数据（XML格式的）被封装成SOAP格式的数据流发送到服务器端的时候，就会生成一个进程对象并且把接收到这个Request的SOAP包进行解析，然后对事物进行处理，处理结束以后再对这个计算结果进行SOAP包装，然后把这个包作为一个Response发送给客户端的代理类(Proxy Class)，同样地，这个代理类也对这个SOAP包进行解析处理，继而进行后续操作。这就是WebService的一个运行过程。 Web Service大体上分为5个层次: Http传输信道；XML的数据格式；SOAP封装格式；WSDL的描述方式；UDDI UDDI是一种目录服务，企业可以使用它对Webservices进行注册和搜索； JMS JMS是实现java领域远程通信的一种手段和方法，基于JMS实现远程通信时和RPC是不同的，虽然可以做到RPC的效果，但因为不是从协议级别定义的，因此我们不认为JMS是个RPC协议，但它确实是个远程通信协议，在其他的语言体系中也存在着类似JMS的东西，可以统一的将这类机制称为消息机制，而消息机制呢，通常是高并发、分布式领域推荐的一种通信机制，这里的主要一个问题是容错。 JMS是Java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。JMS支持两种消息模型：Point-to-Point（P2P）和Publish/Subscribe（Pub/Sub），即点对点和发布订阅模型。 来看JMS中的一次远程通信的过程： 客户端将请求转化为符合JMS规定的Message； 通过JMS API将Message放入JMS Queue或Topic中； 如为JMS Queue，则发送中相应的目标Queue中，如为Topic，则发送给订阅了此Topic的JMS Queue。 处理端则通过轮训JMS Queue，来获取消息，接收到消息后根据JMS协议来解析Message并处理。 同样来回答问题： 传输的标准格式是？JMS规定的Message。 怎么样将请求转化为传输的流？将参数信息放入Message中即可。 怎么接收和处理流？轮训JMS Queue来接收Message，接收到后进行处理，处理完毕后仍然是以Message的方式放入Queue中发送或Multicast。 传输协议是？不限。 基于JMS也是常用的实现远程异步调用的方法之一。 之间的区别RPC与RMI RPC跨语言，而RMI只支持Java。 RMI调用远程对象方法，允许方法返回Java对象以及基本数据类型，而RPC不支持对象的概念，传送到RPC服务的消息由外部数据表示 (External Data Representation, XDR) 语言表示，这种语言抽象了字节序类和数据类型结构之间的差异。只有由 XDR 定义的数据类型才能被传递，可以说 RMI 是面向对象方式的Java RPC。 在方法调用上，RMI中，远程接口使每个远程方法都具有方法签名。如果一个方法在服务器上执行，但是没有相匹配的签名被添加到这个远程接口上，那么这个新方法就不能被RMI客户方所调用。在RPC中，当一个请求到达RPC服务器时，这个请求就包含了一个参数集和一个文本值，通常形成“classname.methodname”的形式。这就向RPC服务器表明，被请求的方法在为 “classname”的类中，名叫“methodname”。然后RPC服务器就去搜索与之相匹配的类和方法，并把它作为那种方法参数类型的输入。这里的参数类型是与RPC请求中的类型是匹配的。一旦匹配成功，这个方法就被调用了，其结果被编码后返回客户方。 RPC本身没有规范,但基本的工作机制是一样的，即：serialization/deserialization+stub+skeleton，宽泛的讲，只要能实现远程调用，都是RPC，如:rmi .net-remoting ws/soap/rest hessian xmlrpc thrift potocolbuffer。 在Java里提供了完整的sockets通讯接口，但sockets要求客户端和服务端必须进行应用级协议的编码交换数据，采用sockets是非常麻烦的。一个代替Sockets的协议是RPC(Remote Procedure Call), 它抽象出了通讯接口用于过程调用，使得编程者调用一个远程过程和调用本地过程同样方便。RPC 系统采用XDR来编码远程调用的参数和返回值。但RPC并不支持对象，所以，面向对象的远程调用RMI(Remote Method Invocation)成为必然选择。采用RMI，调用远程对象和调用本地对象同样方便。RMI 采用JRMP(Java Remote Method Protocol)通讯协议，是构建在TCP/IP协议上的一种远程调用方法。 JMS与RMI 采用JMS服务，对象是在物理上被异步从网络的某个JVM 上直接移动到另一个JVM 上（是消息通知机制），而RMI对象是绑定在本地JVM 中，只有函数参数和返回值是通过网络传送的（是请求应答机制）。 RMI一般都是同步的，也就是说，当client调用Server的一个方法的时候，需要等到对方的返回，才能继续执行client端，这个过程调用本地方法感觉上是一样的，这也是RMI的一个特点。JMS 一般只是一个点发出一个Message到Message Server,发出之后一般不会关心谁用了这个message。所以，一般RMI的应用是紧耦合，JMS的应用相对来说是松散耦合应用。 Webservice与RMI RMI是在tcp协议上传递可序列化的java对象，只能用在java虚拟机上，绑定语言，客户端和服务端都必须是java。webservice没有这个限制，webservice是在http协议上传递xml文本文件，与语言和平台无关。 Webservice与JMS Webservice专注于远程服务调用，jms专注于信息交换。 大多数情况下Webservice是两系统间的直接交互（Consumer Producer），而大多数情况下jms是三方系统交互（Consumer Producer）。当然，JMS也可以实现request-response模式的通信，只要Consumer或Producer其中一方兼任broker即可。 JMS可以做到异步调用完全隔离了客户端和服务提供者，能够抵御流量洪峰；WebService服务通常为同步调用，需要有复杂的对象转换，相比SOAP，现在JSON，rest都是很好的http架构方案； JMS是java平台上的消息规范。一般jms消息不是一个xml，而是一个java对象，很明显，jms没考虑异构系统，说白了，JMS就没考虑非java的东西。但是好在现在大多数的jms provider（就是JMS的各种实现产品）都解决了异构问题。相比WebService的跨平台各有千秋吧。 可选实现技术目前java领域可用于实现远程通讯的框架或library，知名的有：JBoss-Remoting、Spring-Remoting、Hessian、Burlap、XFire(Axis)、ActiveMQ、Mina、Mule、EJB3等等，来对每种做个简单的介绍和评价，其实呢，要做分布式服务框架，这些东西都是要有非常深刻的了解的，因为分布式服务框架其实是包含了解决分布式领域以及应用层面领域两方面问题的。 当然，你也可以自己根据远程网络通信原理(transport protocol+Net IO)去实现自己的通讯框架或library。 那么在了解这些远程通讯的框架或library时，会带着什么问题去学习呢？ 是基于什么协议实现的？ 怎么发起请求？ 怎么将请求转化为符合协议的格式的？ 使用什么传输协议传输？ 响应端基于什么机制来接收请求？ 怎么将流还原为传输格式的？ 处理完毕后怎么回应？ Spring-Remoting Spring-remoting是Spring提供java领域的远程通讯框架，基于此框架，同样也可以很简单的将普通的spring bean以某种远程协议的方式来发布，同样也可以配置spring bean为远程调用的bean。 是基于什么协议实现的？作为一个远程通讯的框架，Spring通过集成多种远程通讯的library，从而实现了对多种协议的支持，例如rmi、http+io、xml-rpc、binary-rpc等。 怎么发起请求？在Spring中，由于其对于远程调用的bean采用的是proxy实现，发起请求完全是通过服务接口调用的方式。 怎么将请求转化为符合协议的格式的？Spring按照协议方式将请求的对象信息转化为流，例如Spring Http Invoker是基于Spring自己定义的一个协议来实现的，传输协议上采用的为http，请求信息是基于java串行化机制转化为流进行传输。 使用什么传输协议传输？支持多种传输协议，例如rmi、http等等。 响应端基于什么机制来接收请求？响应端遵循协议方式来接收请求，对于使用者而言，则只需通过spring的配置方式将普通的spring bean配置为响应端或者说提供服务端。 怎么将流还原为传输格式的？按照协议方式来进行还原。 处理完毕后怎么回应？处理完毕后直接返回即可，spring-remoting将根据协议方式来做相应的序列化。 Hessian Hessian是由caucho提供的一个基于binary-RPC实现的远程通讯library。 是基于什么协议实现的？基于Binary-RPC协议实现。 怎么发起请求？需通过Hessian本身提供的API来发起请求。 怎么将请求转化为符合协议的格式的？Hessian通过其自定义的串行化机制将请求信息进行序列化，产生二进制流。 使用什么传输协议传输？Hessian基于Http协议进行传输。 响应端基于什么机制来接收请求？响应端根据Hessian提供的API来接收请求。 怎么将流还原为传输格式的？Hessian根据其私有的串行化机制来将请求信息进行反序列化，传递给使用者时已是相应的请求信息对象了。 处理完毕后怎么回应？处理完毕后直接返回，hessian将结果对象进行序列化，传输至调用端。 Burlap Burlap也是有caucho提供，它和hessian的不同在于，它是基于XML-RPC协议的。 是基于什么协议实现的？基于XML-RPC协议实现。 怎么发起请求？根据Burlap提供的API。 怎么将请求转化为符合协议的格式的？将请求信息转化为符合协议的XML格式，转化为流进行传输。 使用什么传输协议传输？Http协议。 响应端基于什么机制来接收请求？监听Http请求。 怎么将流还原为传输格式的？根据XML-RPC协议进行还原。 处理完毕后怎么回应？返回结果写入XML中，由Burlap返回至调用端。 XFire、Axis XFire、Axis是Webservice的实现框架，WebService可算是一个完整的SOA架构实现标准了，因此采用XFire、Axis这些也就意味着是采用webservice方式了。 是基于什么协议实现的？基于SOAP协议。 怎么发起请求？获取到远端service的proxy后直接调用。 怎么将请求转化为符合协议的格式的？将请求信息转化为遵循SOAP协议的XML格式，由框架转化为流进行传输。 使用什么传输协议传输？Http协议。 响应端基于什么机制来接收请求？监听Http请求。 怎么将流还原为传输格式的？根据SOAP协议进行还原。 处理完毕后怎么回应？返回结果写入XML中，由框架返回至调用端。 ActiveMQ ActiveMQ是JMS的实现，基于JMS这类消息机制实现远程通讯是一种不错的选择，毕竟消息机制本身的功能使得基于它可以很容易的去实现同步/异步/单向调用等，而且消息机制从容错角度上来说也是个不错的选择，这是Erlang能够做到容错的重要基础。 是基于什么协议实现的？基于JMS协议。 怎么发起请求？遵循JMS API发起请求。 怎么将请求转化为符合协议的格式的？不太清楚，猜想应该是二进制流。 使用什么传输协议传输？支持多种传输协议，例如socket、http等等。 响应端基于什么机制来接收请求？监听符合协议的端口。 怎么将流还原为传输格式的？同问题3。 处理完毕后怎么回应？遵循JMS API生成消息，并写入JMS Queue中。 Mina Mina是Apache提供的通讯框架，在之前一直没有提到网络IO这块，之前提及的框架或library基本都是基于BIO的，而Mina是采用NIO的，NIO在并发量增长时对比BIO而言会有明显的性能提升，而java性能的提升，与其NIO这块与OS的紧密结合是有不小的关系的。 是基于什么协议实现的？基于纯粹的Socket+NIO。 怎么发起请求？通过Mina提供的Client API。 怎么将请求转化为符合协议的格式的？Mina遵循java串行化机制对请求对象进行序列化。 使用什么传输协议传输？支持多种传输协议，例如socket、http等等。 响应端基于什么机制来接收请求？以NIO的方式监听协议端口。 怎么将流还原为传输格式的？遵循java串行化机制对请求对象进行反序列化。 处理完毕后怎么回应？遵循Mina API进行返回。 MINA是NIO方式的，因此支持异步调用是毫无悬念的。 RPC框架的发展与现状RPC（Remote Procedure Call）是一种远程调用协议，简单地说就是能使应用像调用本地方法一样的调用远程的过程或服务，可以应用在分布式服务、分布式计算、远程服务调用等许多场景。说起 RPC 大家并不陌生，业界有很多开源的优秀 RPC 框架，例如 Dubbo、Thrift、gRPC、Hprose 等等。下面先简单介绍一下 RPC 与常用远程调用方式的特点，以及一些优秀的开源 RPC 框架。 RPC 与其它远程调用方式比较，RPC 与 HTTP、RMI、Web Service 都能完成远程调用，但是实现方式和侧重点各有不同。 RPC与HTTP HTTP（HyperText Transfer Protocol）是应用层通信协议，使用标准语义访问指定资源（图片、接口等），网络中的中转服务器能识别协议内容。HTTP 协议是一种资源访问协议，通过 HTTP 协议可以完成远程请求并返回请求结果。 HTTP 的优点是简单、易用、可理解性强且语言无关，在远程服务调用中包括微博有着广泛应用。HTTP 的缺点是协议头较重，一般请求到具体服务器的链路较长，可能会有 DNS 解析、Nginx 代理等。 RPC 是一种协议规范，可以把 HTTP 看作是一种 RPC 的实现，也可以把 HTTP 作为 RPC 的传输协议来应用。RPC 服务的自动化程度比较高，能够实现强大的服务治理功能，和语言结合更友好，性能也十分优秀。与 HTTP 相比，RPC 的缺点就是相对复杂，学习成本稍高。 RPC与RMI RMI（Remote Method Invocation）是指 Java 语言中的远程方法调用，RMI 中的每个方法都具有方法签名，RMI 客户端和服务器端通过方法签名进行远程方法调用。RMI 只能在 Java 语言中使用，可以把 RMI 看作面向对象的 Java RPC。 RPC与Web Service Web Service 是一种基于 Web 进行服务发布、查询、调用的架构方式，重点在于服务的管理与使用。Web Service 一般通过 WSDL 描述服务，使用 SOAP通过 HTTP 调用服务。 RPC 是一种远程访问协议，而 Web Service 是一种体系结构，Web Service 也可以通过 RPC 来进行服务调用，因此 Web Service 更适合同一个 RPC 框架进行比较。当 RPC 框架提供了服务的发现与管理，并使用 HTTP 作为传输协议时，其实就是 Web Service。 相对 Web Service，RPC 框架可以对服务进行更细粒度的治理，包括流量控制、SLA 管理等，在微服务化、分布式计算方面有更大的优势。 RPC 可基于 HTTP 或 TCP 协议，Web Service 就是基于 HTTP 协议的 RPC，它具有良好的跨平台性，但其性能却不如基于 TCP 协议的 RPC。会两方面会直接影响 RPC 的性能，一是传输方式，二是序列化。 众所周知，TCP 是传输层协议，HTTP 是应用层协议，而传输层较应用层更加底层，在数据传输方面，越底层越快，因此，在一般情况下，TCP 一定比 HTTP 快。]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java定时任务]]></title>
    <url>%2F2016%2F05%2F20%2Ftask20160520%2F</url>
    <content type="text"><![CDATA[在java中一个完整定时任务需要由Timer、TimerTask两个类来配合完成。 API中是这样定义他们的，Timer：一种工具，线程用其安排以后在后台线程中执行的任务。可安排任务执行一次，或者定期重复执行。由TimerTask：Timer 安排为一次执行或重复执行的任务。我们可以这样理解Timer是一种定时器工具，用来在一个后台线程计划执行指定任务，而TimerTask一个抽象类，它的子类代表一个可以被Timer计划的任务。 Timer类在工具类Timer中，提供了四个构造方法，每个构造方法都启动了计时器线程，同时Timer类可以保证多个线程可以共享单个Timer对象而无需进行外部同步，所以Timer类是线程安全的。但是由于每一个Timer对象对应的是单个后台线程，用于顺序执行所有的计时器任务，一般情况下我们的线程任务执行所消耗的时间应该非常短，但是由于特殊情况导致某个定时器任务执行的时间太长，那么他就会“独占”计时器的任务执行线程，其后的所有线程都必须等待它执行完，这就会延迟后续任务的执行，使这些任务堆积在一起，具体情况我们后面分析。 当程序初始化完成Timer后，定时任务就会按照我们设定的时间去执行，Timer提供了schedule方法，该方法有多中重载方式来适应不同的情况，如下： schedule(TimerTask task, Date time)：安排在指定的时间执行指定的任务。 schedule(TimerTask task, Date firstTime, long period) ：安排指定的任务在指定的时间开始进行重复的固定延迟执行。 schedule(TimerTask task, long delay) ：安排在指定延迟后执行指定的任务。 schedule(TimerTask task, long delay, long period) ：安排指定的任务从指定的延迟后开始进行重复的固定延迟执行。 同时也重载了scheduleAtFixedRate方法，scheduleAtFixedRate方法与schedule相同，只不过他们的侧重点不同，区别后面分析。 scheduleAtFixedRate(TimerTask task, Date firstTime, long period)：安排指定的任务在指定的时间开始进行重复的固定速率执行。 scheduleAtFixedRate(TimerTask task, long delay, long period)：安排指定的任务在指定的延迟后开始进行重复的固定速率执行。 TimerTaskTimerTask类是一个抽象类，由Timer 安排为一次执行或重复执行的任务。它有一个抽象方法run()方法，该方法用于执行相应计时器任务要执行的操作。因此每一个具体的任务类都必须继承TimerTask，然后重写run()方法。 另外它还有两个非抽象的方法： boolean cancel()：取消此计时器任务。 long scheduledExecutionTime()：返回此任务最近实际执行的安排执行时间。 实例指定延迟时间执行定时任务 12345678910111213141516public class TimerTest01 &#123; Timer timer; public TimerTest01(int time)&#123; timer = new Timer(); timer.schedule(new TimerTaskTest01(), time * 1000); &#125; public static void main(String[] args) &#123; System.out.println("timer begin...."); new TimerTest01(3); &#125;&#125;public class TimerTaskTest01 extends TimerTask&#123; public void run() &#123; System.out.println("Time's up!!!!"); &#125;&#125; 结果 timer begin….3秒后打印：Time’s up!!!! 在指定时间执行定时任务 1234567891011121314151617181920212223242526public class TimerTest02 &#123; Timer timer; public TimerTest02()&#123; Date time = getTime(); System.out.println("指定时间time=" + time); timer = new Timer(); timer.schedule(new TimerTaskTest02(), time); &#125; public Date getTime()&#123; Calendar calendar = Calendar.getInstance(); calendar.set(Calendar.HOUR_OF_DAY, 11); calendar.set(Calendar.MINUTE, 39); calendar.set(Calendar.SECOND, 00); Date time = calendar.getTime(); return time; &#125; public static void main(String[] args) &#123; new TimerTest02(); &#125;&#125;public class TimerTaskTest02 extends TimerTask&#123; @Override public void run() &#123; System.out.println("指定时间执行线程任务..."); &#125;&#125; 当时间到达11:39:00时就会执行该线程任务，当然大于该时间也会执行！！执行结果为： 指定时间time=Tue Jun 10 11:39:00 CST 2014指定时间执行线程任务… 在延迟指定时间后以指定的间隔时间循环执行定时任务 1234567891011121314151617public class TimerTest03 &#123; Timer timer; public TimerTest03()&#123; timer = new Timer(); timer.schedule(new TimerTaskTest03(), 1000, 2000); &#125; public static void main(String[] args) &#123; new TimerTest03(); &#125;&#125;public class TimerTaskTest03 extends TimerTask&#123; @Override public void run() &#123; Date date = new Date(this.scheduledExecutionTime()); System.out.println("本次执行该线程的时间为：" + date); &#125;&#125; 结果 本次执行该线程的时间为：Tue Jun 10 21:19:47 CST 2014本次执行该线程的时间为：Tue Jun 10 21:19:49 CST 2014本次执行该线程的时间为：Tue Jun 10 21:19:51 CST 2014本次执行该线程的时间为：Tue Jun 10 21:19:53 CST 2014本次执行该线程的时间为：Tue Jun 10 21:19:55 CST 2014本次执行该线程的时间为：Tue Jun 10 21:19:57 CST 2014…………….. 对于这个线程任务,如果我们不将该任务停止,他会一直运行下去。 对于上面三个实例，LZ只是简单的演示了一下，同时也没有讲解scheduleAtFixedRate方法的例子，其实该方法与schedule方法一样！ 分析schedule和scheduleAtFixedRate1、schedule(TimerTask task, Date time)、schedule(TimerTask task, long delay) 对于这两个方法而言，如果指定的计划执行时间scheduledExecutionTime&lt;= systemCurrentTime，则task会被立即执行。scheduledExecutionTime不会因为某一个task的过度执行而改变。 2、schedule(TimerTask task, Date firstTime, long period)、schedule(TimerTask task, long delay, long period) 这两个方法与上面两个就有点儿不同的，前面提过Timer的计时器任务会因为前一个任务执行时间较长而延时。在这两个方法中，每一次执行的task的计划时间会随着前一个task的实际时间而发生改变，也就是scheduledExecutionTime(n+1)=realExecutionTime(n)+periodTime。也就是说如果第n个task由于某种情况导致这次的执行时间过程，最后导致systemCurrentTime&gt;= scheduledExecutionTime(n+1)，这是第n+1个task并不会因为到时了而执行，他会等待第n个task执行完之后再执行，那么这样势必会导致n+2个的执行实现scheduledExecutionTime放生改变即scheduledExecutionTime(n+2) = realExecutionTime(n+1)+periodTime。所以这两个方法更加注重保存间隔时间的稳定。 3、scheduleAtFixedRate(TimerTask task, Date firstTime, long period)、scheduleAtFixedRate(TimerTask task, long delay, long period) 在前面也提过scheduleAtFixedRate与schedule方法的侧重点不同，schedule方法侧重保存间隔时间的稳定，而scheduleAtFixedRate方法更加侧重于保持执行频率的稳定。为什么这么说，原因如下。在schedule方法中会因为前一个任务的延迟而导致其后面的定时任务延时，而scheduleAtFixedRate方法则不会，如果第n个task执行时间过长导致systemCurrentTime&gt;= scheduledExecutionTime(n+1)，则不会做任何等待他会立即执行第n+1个task，所以scheduleAtFixedRate方法执行时间的计算方法不同于schedule，而是scheduledExecutionTime(n)=firstExecuteTime +n*periodTime，该计算方法永远保持不变。所以scheduleAtFixedRate更加侧重于保持执行频率的稳定。 Timer的缺陷Timer的缺陷 Timer计时器可以定时（指定时间执行任务）、延迟（延迟5秒执行任务）、周期性地执行任务（每隔个1秒执行任务），但是，Timer存在一些缺陷。首先Timer对调度的支持是基于绝对时间的，而不是相对时间，所以它对系统时间的改变非常敏感。其次Timer线程是不会捕获异常的，如果TimerTask抛出的了未检查异常则会导致Timer线程终止，同时Timer也不会重新恢复线程的执行，他会错误的���为整个Timer线程都会取消。同时，已经被安排单尚未执行的TimerTask也不会再执行了，新的任务也不能被调度。故如果TimerTask抛出未检查的异常，Timer将会产生无法预料的行为。 1、Timer管理时间延迟缺陷 前面Timer在执行定时任务时只会创建一个线程任务，如果存在多个线程，若其中某个线程因为某种原因而导致线程任务执行时间过长，超过了两个任务的间隔时间，会发生一些缺陷： 12345678910111213141516171819202122232425262728293031323334public class TimerTest04 &#123; private Timer timer; public long start; public TimerTest04()&#123; this.timer = new Timer(); start = System.currentTimeMillis(); &#125; public void timerOne()&#123; timer.schedule(new TimerTask() &#123; public void run() &#123; System.out.println("timerOne invoked ,the time:" + (System.currentTimeMillis() - start)); try &#123; Thread.sleep(4000); //线程休眠3000 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, 1000); &#125; public void timerTwo()&#123; timer.schedule(new TimerTask() &#123; public void run() &#123; System.out.println("timerOne invoked ,the time:" + (System.currentTimeMillis() - start)); &#125; &#125;, 3000); &#125; public static void main(String[] args) throws Exception &#123; TimerTest04 test = new TimerTest04(); test.timerOne(); test.timerTwo(); &#125;&#125; 按照我们正常思路，timerTwo应该是在3s后执行，其结果应该是： timerOne invoked ,the time:1001timerOne invoked ,the time:3001但是事与愿违，timerOne由于sleep(4000)，休眠了4S，同时Timer内部是一个线程，导致timeOne所需的时间超过了间隔时间，结果： timerOne invoked ,the time:1000timerOne invoked ,the time:5000 2、Timer抛出异常缺陷 如果TimerTask抛出RuntimeException，Timer会终止所有任务的运行。如下： 12345678910111213141516171819202122232425public class TimerTest04 &#123; private Timer timer; public TimerTest04()&#123; this.timer = new Timer(); &#125; public void timerOne()&#123; timer.schedule(new TimerTask() &#123; public void run() &#123; throw new RuntimeException(); &#125; &#125;, 1000); &#125; public void timerTwo()&#123; timer.schedule(new TimerTask() &#123; public void run() &#123; System.out.println("我会不会执行呢？？"); &#125; &#125;, 1000); &#125; public static void main(String[] args) &#123; TimerTest04 test = new TimerTest04(); test.timerOne(); test.timerTwo(); &#125;&#125; 运行结果：timerOne抛出异常，导致timerTwo任务终止。 Exception in thread “Timer-0” java.lang.RuntimeException at com.chenssy.timer.TimerTest04$1.run(TimerTest04.java:25) at java.util.TimerThread.mainLoop(Timer.java:555) at java.util.TimerThread.run(Timer.java:505) 对于Timer的缺陷，我们可以考虑 ScheduledThreadPoolExecutor 来替代。Timer是基于绝对时间的，对系统时间比较敏感，而ScheduledThreadPoolExecutor 则是基于相对时间；Timer是内部是单一线程，而ScheduledThreadPoolExecutor内部是个线程池，所以可以支持多个任务并发执行。 用ScheduledExecutorService替代Timer 解决问题一： 123456789101112131415161718192021222324252627282930313233public class ScheduledExecutorTest &#123; private ScheduledExecutorService scheduExec; public long start; ScheduledExecutorTest()&#123; this.scheduExec = Executors.newScheduledThreadPool(2); this.start = System.currentTimeMillis(); &#125; public void timerOne()&#123; scheduExec.schedule(new Runnable() &#123; public void run() &#123; System.out.println("timerOne,the time:" + (System.currentTimeMillis() - start)); try &#123; Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,1000,TimeUnit.MILLISECONDS); &#125; public void timerTwo()&#123; scheduExec.schedule(new Runnable() &#123; public void run() &#123; System.out.println("timerTwo,the time:" + (System.currentTimeMillis() - start)); &#125; &#125;,2000,TimeUnit.MILLISECONDS); &#125; public static void main(String[] args) &#123; ScheduledExecutorTest test = new ScheduledExecutorTest(); test.timerOne(); test.timerTwo(); &#125;&#125; 结果： timerOne,the time:1003timerTwo,the time:2005 123456789101112131415161718192021222324252627public class ScheduledExecutorTest &#123; private ScheduledExecutorService scheduExec; public long start; ScheduledExecutorTest()&#123; this.scheduExec = Executors.newScheduledThreadPool(2); this.start = System.currentTimeMillis(); &#125; public void timerOne()&#123; scheduExec.schedule(new Runnable() &#123; public void run() &#123; throw new RuntimeException(); &#125; &#125;,1000,TimeUnit.MILLISECONDS); &#125; public void timerTwo()&#123; scheduExec.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; System.out.println("timerTwo invoked ....."); &#125; &#125;,2000,500,TimeUnit.MILLISECONDS); &#125; public static void main(String[] args) &#123; ScheduledExecutorTest test = new ScheduledExecutorTest(); test.timerOne(); test.timerTwo(); &#125;&#125; 结果： timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked …..timerTwo invoked ………………………..]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ+Spring工程创建详解（附工程文件）]]></title>
    <url>%2F2016%2F05%2F19%2Factivemq20160519%2F</url>
    <content type="text"><![CDATA[ActiveMQ是Apache所提供的一个开源的消息系统，完全采用Java来实现，因此，它能很好地支持J2EE提出的JMS（Java Message Service,即Java消息服务）规范。JMS是一组Java应用程序接口，它提供消息的创建、发送、读取等一系列服务。JMS提供了一组公共应用程序接口和响应的语法，类似于Java数据库的统一访问接口JDBC,它是一种与厂商无关的API，使得Java程序能够与不同厂商的消息组件很好地进行通信。 JMS支持两种消息发送和接收模型。一种称为P2P(Ponit to Point)模型，即采用点对点的方式发送消息。P2P模型是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息，队列的存在使得消息的异步传输称为可能，P2P模型在点对点的情况下进行消息传递时采用。 另一种称为Pub/Sub(Publish/Subscribe，即发布-订阅)模型，发布-订阅模型定义了如何向一个内容节点发布和订阅消息，这个内容节点称为topic(主题)。主题可以认为是消息传递的中介，消息发布这将消息发布到某个主题，而消息订阅者则从主题订阅消息。主题使得消息的订阅者与消息的发布者互相保持独立，不需要进行接触即可保证消息的传递，发布-订阅模型在消息的一对多广播时采用。 ActiveMQ的安装下载最新的安装包apache-activemq-5.13.2-bin.tar.gz 下载之后解压： tar -zvxf apache-activemq-5.13.2-bin.tar.gz ActiveMQ目录内容有： bin目录包含ActiveMQ的启动脚本 conf目录包含ActiveMQ的所有配置文件 data目录包含日志文件和持久性消息数据 example: ActiveMQ的示例 lib: ActiveMQ运行所需要的lib webapps: ActiveMQ的web控制台和一些相关的demo ActiveMQ的默认服务端口为61616，这个可以在conf/activemq.xml配置文件中修改： 1234&lt;transportConnectors&gt; &lt;!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB --&gt; &lt;transportConnector name="openwire" uri="tcp://0.0.0.0:61616?maximumConnections=1000&amp;amp;wireFormat.maxFrameSize=104857600"/&gt;&lt;/transportConnectors&gt; JMS的规范流程 获得JMS connection factory. 通过我们提供特定环境的连接信息来构造factory。 利用factory构造JMS connection 启动connection 通过connection创建JMS session. 指定JMS destination. 创建JMS producer或者创建JMS message并提供destination. 创建JMS consumer或注册JMS message listener. 发送和接收JMS message. 关闭所有JMS资源，包括connection, session, producer, consumer等。 案例（整合Spring）pom.xml 123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.11.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;4.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; Queue类型消息 spring配置文件 123456789101112131415161718192021222324252627282930&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;!-- ActiveMQ服务的地址和端口--&gt; &lt;property name="brokerURL" value="failover:(tcp://192.168.147.131:61616)" /&gt; &lt;/bean&gt; &lt;!-- 定义消息队列（Queue） --&gt; &lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;testSpringQueue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt;&lt;!-- 定义消息发布（Pub/Sub） --&gt;&lt;!-- &lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; --&gt;&lt;!-- &lt;constructor-arg&gt; --&gt;&lt;!-- &lt;value&gt;topic&lt;/value&gt; --&gt;&lt;!-- &lt;/constructor-arg&gt; --&gt;&lt;!-- &lt;/bean&gt; --&gt; &lt;!-- 配置JMS模板（Queue），Spring提供的JMS工具类，它发送、接收消息。 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="queueDestination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;/bean&gt;&lt;/beans&gt; 推送代码 123456789101112131415161718192021222324252627282930313233343536373839package com.mq.spring.queue;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import javax.jms.*;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;"classpath:spring-mq-queue.xml"&#125;)public class QueueSender &#123; @Resource private JmsTemplate jmsTemplate; @Test public void send()&#123; sendMqMessage(null,"spring activemq queue type message !"); &#125; /** * 说明:发送的时候如果这里没有显示的指定destination.将用spring xml中配置的destination * @param destination * @param message */ public void sendMqMessage(Destination destination, final String message)&#123; if(null == destination)&#123; destination = jmsTemplate.getDefaultDestination(); &#125; jmsTemplate.send(destination, new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(message); &#125; &#125;); System.out.println("spring send message..."); &#125; public void setJmsTemplate(JmsTemplate jmsTemplate) &#123; this.jmsTemplate = jmsTemplate; &#125;&#125; 消费代码 123456789101112131415161718192021222324252627282930313233343536package com.mq.spring.queue;import org.junit.Test;import javax.jms.*;import org.junit.runner.RunWith;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import javax.jms.Message;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;"classpath:spring-mq-queue.xml"&#125;)public class QueueReceiver &#123; @Resource private JmsTemplate jmsTemplate; @Test public void receiveMqMessage()&#123; Destination destination = jmsTemplate.getDefaultDestination(); receive(destination); &#125; /** * 接受消息 */ public void receive(Destination destination) &#123; TextMessage tm = (TextMessage) jmsTemplate.receive(destination); try &#123; System.out.println("从队列" + destination.toString() + "收到了消息：\t" + tm.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; public void setJmsTemplate(JmsTemplate jmsTemplate) &#123; this.jmsTemplate = jmsTemplate; &#125;&#125; 说明:上面的生产者和消费者使用同一套配置文件,使用独立的程序去接收消息,spring jms也提供了消息监听处理.接下来我们换成监听式消费 配置文件 12345678910111213141516171819202122232425&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="failover:(tcp://192.168.147.131:61616)" /&gt; &lt;/bean&gt; &lt;!-- 定义消息队列（Queue） --&gt; &lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;testSpringQueue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置消息队列监听者（Queue） --&gt; &lt;bean id="consumerMessageListener" class="com.mq.spring.queue.ConsumerMessageListener" /&gt; &lt;!-- 消息监听容器（Queue），配置连接工厂，监听的队列是testSpringQueue，监听器是上面定义的监听器 --&gt; &lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="queueDestination" /&gt; &lt;property name="messageListener" ref="consumerMessageListener" /&gt; &lt;/bean&gt;&lt;/beans&gt; 监听器代码 1234567891011public class ConsumerMessageListener implements MessageListener&#123; @Override public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println("ConsumerMessageListener收到了文本消息：\t" + tm.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这样我们的消息消费就可以在监听器中处理消费了.生产的代码不变,修改发送者的消息体内容,执行生产程序 Topic类型消息 在使用 Spring JMS的时候，主题（ Topic）和队列消息的主要差异体现在JmsTemplate中 “pubSubDomain”是否设置为 True。如果为 True，则是 Topic；如果是false或者默认，则是 queue。 1&lt;property name="pubSubDomain" value="true" /&gt; topic类型消费配置文件说明 123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;!-- 配置JMS连接工厂 --&gt; &lt;bean id="connectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="failover:(tcp://192.168.147.131:61616)" /&gt; &lt;/bean&gt; &lt;!-- 定义消息Destination --&gt; &lt;bean id="destination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;!-- 设置消息队列的名字 --&gt; &lt;constructor-arg&gt; &lt;value&gt;testSpringTopic&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- 配置JMS模板（Queue），Spring提供的JMS工具类，它发送、接收消息。 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="defaultDestination" ref="destination" /&gt; &lt;property name="receiveTimeout" value="10000" /&gt; &lt;/bean&gt; &lt;!-- 配置消息消费监听者 --&gt; &lt;bean id="consumerMessageListener" class="com.mq.spring.topic.ConsumerMessageListener" /&gt; &lt;!-- 消息监听容器，配置连接工厂,监听器是上面定义的监听器 --&gt; &lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="destination" /&gt; &lt;!--主题（Topic）和队列消息的主要差异体现在JmsTemplate中"pubSubDomain"是否设置为True。如果为True，则是Topic；如果是false或者默认，则是queue--&gt; &lt;property name="pubSubDomain" value="true" /&gt; &lt;property name="messageListener" ref="consumerMessageListener" /&gt; &lt;/bean&gt;&lt;/beans&gt; 生产者代码 123456789101112131415161718192021222324252627282930313233343536373839404142package com.mq.spring.topic;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.jms.core.JmsTemplate;import org.springframework.jms.core.MessageCreator;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.Session;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;"classpath:spring-mq-topic.xml"&#125;)public class TopicSender &#123; @Resource private JmsTemplate jmsTemplate; @Test public void send()&#123; sendMqMessage(null,"spring activemq topic type message[with listener] !"); &#125; /** * 说明:发送的时候如果这里没有显示的指定destination.将用spring xml中配置的destination * @param destination * @param message */ public void sendMqMessage(Destination destination, final String message)&#123; if(null == destination)&#123; destination = jmsTemplate.getDefaultDestination(); &#125; jmsTemplate.send(destination, new MessageCreator() &#123; public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage(message); &#125; &#125;); System.out.println("spring send message..."); &#125; public void setJmsTemplate(JmsTemplate jmsTemplate) &#123; this.jmsTemplate = jmsTemplate; &#125;&#125; 监听器代码 1234567891011public class ConsumerMessageListener implements MessageListener&#123; @Override public void onMessage(Message message) &#123; TextMessage tm = (TextMessage) message; try &#123; System.out.println("ConsumerMessageListener收到了文本消息：\t" + tm.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考资料 http://www.importnew.com/19767.html http://blog.csdn.net/u013256816/article/details/51161548 http://www.tuicool.com/articles/FVf26b http://haohaoxuexi.iteye.com/blog/1893038 Spring+ActiveMQ+Mysql 配置JMS ​]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lambda让编程变得简单]]></title>
    <url>%2F2016%2F05%2F18%2Flambda20160518%2F</url>
    <content type="text"><![CDATA[Lambda相当于就是一个匿名方法，其在代替匿名内部类创建对象的时候，Lambda表达式代码块会代替实现抽象方法的方法体 Lambda表达式的目标类型必须是“函数式接口(FunctionalInterface)”。函数式接口只能包含一个抽象方法接口。函数式接口可以包含多个默认方法、类方法、但只能一个抽象方法 函数式接口 有且只有一个没有default关键字修饰的接口 可以多个default关键字的抽象方法、类方法、变量 语法组成元素 形参列表。形参可以省略数据类型。若只有一个参数，可以将圆括号省略 箭头(-&gt;) 代码块。如果只有一条语句，Lambda表达式可以省略花括号。代码块只能有一条return语句，并且可以省略return关键字 12345678910public class Lam &#123; @FunctionalInterface interface Try&#123; Integer ss(); &#125; public static void main(String[] args) &#123; Try i=()-&gt;"sa".indexOf("a"); System.out.println(i.ss()); &#125;&#125; Lambda的代码块中就是用来实现接口的抽象方法，而且有且只能有一个。若有default关键字的抽象方法是不属于函数式接口(FunctionalInterface) 存在@FunctionalInterface注解的一定是函数式接口，没有@FunctionalInterface不一定就不是函数式接口 1234567891011121314151617public class Lam &#123; @FunctionalInterface interface Try &#123; String s = "1"; Integer ss(); default Integer bb() &#123; return 0; &#125; &#125; public static void main(String[] args) &#123; Try i = () -&gt; "sa".indexOf("a"); System.out.println(i.s); System.out.println(i.ss()); &#125;&#125; 方法引用与构造器引用 种类 示例 说明 对应的Lambda表达式 引用类方法 类名::类方法 函数式接口中实现的方法的全部参数都传递给类方法作为参数 (a,b,…)-&gt;类名.类方法(a,b,…) 特定对象的实例方法 对象::实例方法 函数式接口中实现的方法的全部参数都传递给类方法作为参数 (a,b,…)-&gt;对象.实例方法(a,b,…) 引用某类对象的实例方法 类名::实例方法 函数式接口中实现的方法第一个参数作为调用者，之后的参数都作为方法的参数 (a,b,…)-&gt;a.实例方法(b,…) 引用构造方法 类名::new 函数式接口中实现的方法的全部参数都传递给类方法作为参数 (a,b,…)-&gt;类名.new(a,b,…) 使用::这种方式，抽象方法肯定是存在参数的 其实上面三类可以归为一类来看待，这是形式不同而已 12345678910111213141516public class Lam &#123; @FunctionalInterface interface Try &#123; String s = "1"; Integer ss(String s); default Integer sb() &#123; return 0; &#125; &#125; public static void main(String[] args) &#123; Try i = "sa"::indexOf; System.out.println(i.s); System.out.println(i.ss("a")); &#125; 引用构造方法 12345678910111213141516public class Lam &#123; @FunctionalInterface interface Try &#123; String s = "1"; String ss(String s); default Integer sb() &#123; return 0; &#125; &#125; public static void main(String[] args) &#123; Try i = String::new; System.out.println(i.s); System.out.println(i.ss("a")); &#125; 等价于 12345678910111213141516171819202122public class Lam &#123; @FunctionalInterface interface Try &#123; String s = "1"; String ss(String s); default Integer sb() &#123; return 0; &#125; &#125; public static void main(String[] args) &#123; Try i = new Try()&#123; @Override public String ss(String s) &#123; return new String(s); &#125; &#125;; System.out.println(i.s); System.out.println(i.ss("a")); &#125; Lambda表达式与匿名函数的异同点同 Lambda表达式与匿名内部类一样，都可以直接访问“effectivly final”的局部变量，以及外部类的成员变量（包括实例变量和类变量） Lambda表达式创建的对象与匿名内部类生成的对象一样，都可以直接调用从接口中继承的默认方法 异 匿名内部类可以为任意接口创建实例——不管接口包含多少抽象方法，只要匿名内部类实现所有的抽象方法即可；但Lambda表达式只能为函数式接口创建实例 匿名内部类可以为抽象类甚至普通类创建实例；但Lambda表达式只能为函数式接口创建实例 匿名内部类实现的抽象方法的方法体运行调用接口中定义的默认方法；但Lambda表达式的代码块不允许调用接口中定义的默认方法]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo+SpringMVC工程创建详解（附工程文件）]]></title>
    <url>%2F2016%2F05%2F17%2Fdubbo20160517%2F</url>
    <content type="text"><![CDATA[Dubbo出现的目的是为了应对现在高并发，高数据量请求的问题。目前的垂直应用架构已经无法满足现在大数据的冲击，SOA就应运而生，而Dubbo在国内使用的还是比较多，稳定性也比较不错。 架构 节点角色说明： Provider: 暴露服务的服务提供方。Consumer: 调用远程服务的服务消费方。Registry: 服务注册与发现的注册中心。Monitor: 统计服务的调用次调和调用时间的监控中心。Container: 服务运行容器。 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 (1) 连通性： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 (2) 健状性： 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 (3) 伸缩性： 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 (4) 升级性： 当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力： 可以得出，注册中心若宕机，只要没有注册新的服务，服务提供者和消费者还是可以根据本地缓存进行沟通。并且注册中心不是请求转发中心，所以压力是比较小的。 搭建Dubbo工程1、ZooKeeper的配置安装 http://blog.csdn.net/u013142781/article/details/50395650 2、Dubbo-admin配置安装 http://blog.csdn.net/u013142781/article/details/50396621 3、工程API工程创建 工程使用Maven+SpringMVC的方式构建 ITestService.java 12345package com.gege.service;public interface ITestService &#123; public String getName();&#125; DubboInit.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.gege.tool;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import org.apache.commons.lang.exception.ExceptionUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class DubboInit extends HttpServlet &#123; /** * */ private static final long serialVersionUID = 8279515033200832L; private Logger logger = (Logger) LoggerFactory.getLogger(DubboInit.class); /** * 启动dubbo容器 */ public void init() throws ServletException &#123; try &#123; startApplicationContext(); &#125; catch (Exception e) &#123; e.printStackTrace(); logger.error(ExceptionUtils.getFullStackTrace(e)); &#125; &#125; public static ApplicationContext applicationContext = null; /** * 启动spring容器 * @return */ public static ApplicationContext startApplicationContext() &#123; if (applicationContext == null) &#123; applicationContext = new ClassPathXmlApplicationContext("classpath*:applicationContext.xml"); &#125; return applicationContext; &#125;;&#125; pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spring.version&gt;3.2.8.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;utf8&lt;/encoding&gt; &lt;fileNameMapping&gt;no-version&lt;/fileNameMapping&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 4、Dubbo服务提供者工程创建与配置 工程使用Maven+SpringMVC的方式构建 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;provider&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spring.version&gt;3.2.8.RELEASE&lt;/spring.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- 添加dubbo依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- 添加zk客户端依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;taglibs&lt;/groupId&gt; &lt;artifactId&gt;standard&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;resources&gt; &lt;resource&gt; //静态资源文件扫描地址 &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;local&lt;/id&gt; &lt;properties&gt; //这是zookeeper的地址，要根据实际情况进行配置，默认zookeeper://127.0.0.1:2181 &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; //dubbo暴露的端口配置 &lt;dubbo.gege.port&gt;29014&lt;/dubbo.gege.port&gt; //工程的服务集合，$&#123;user.name&#125;表示当前PC的UserName &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; //dubbo服务版本号 &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.port&gt;29014&lt;/dubbo.gege.port&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.port&gt;29014&lt;/dubbo.gege.port&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;product&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.port&gt;29014&lt;/dubbo.gege.port&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; TestServiceImpl.java 123456789101112package com.gege.service.impl;import org.springframework.stereotype.Service;import com.gege.service.ITestService;@Servicepublic class TestServiceImpl implements ITestService &#123; public String getName() &#123; return "gege"; &#125;&#125; applicationContext.xml 1234567891011121314151617181920212223242526272829303132333435&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.0.xsd"&gt; &lt;task:annotation-driven /&gt; //注解扫描 &lt;context:component-scan base-package="com.gege"/&gt; &lt;context:annotation-config /&gt; &lt;bean class="org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor" /&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath*:statics.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;import resource="dubbo-provider.xml"/&gt; &lt;/beans&gt; dubbo-provider.xml 12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;dubbo:application name="provider" /&gt; &lt;dubbo:registry protocol="dubbo" address="$&#123;dubbo.gege.address&#125;" port="$&#123;dubbo.gege.port&#125;" /&gt; &lt;dubbo:protocol name="dubbo" port="$&#123;dubbo.gege.port&#125;" /&gt; &lt;dubbo:monitor protocol="registry" /&gt; &lt;dubbo:service ref="testServiceImpl" interface="com.gege.service.ITestService" group="$&#123;dubbo.gege.group&#125;" version="$&#123;dubbo.gege.version&#125;"/&gt; &lt;/beans&gt; log4j.properties 1234567891011121314151617181920212223log4j.rootLogger=debug, stdout, Rlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout#Pattern to output the caller's file name and line number.log4j.appender.stdout.layout.ConversionPattern=[%d] %t (%F:%L) %-5p - %m%nlog4j.appender.R.Encoding=UTF-8log4j.appender.R=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.R.DatePattern='_'yyyy-MM-dd_HH'.log'log4j.appender.R.File=$&#123;catalina.home&#125;/logs/1.0_order_providerlog4j.appender.R.ImmediateFlush=truelog4j.appender.R.MaxFileSize=10000KB# Keep one backup filelog4j.appender.R.MaxBackupIndex=20log4j.appender.R.layout=org.apache.log4j.PatternLayoutlog4j.appender.R.layout.ConversionPattern=[%d] %t %c (%F:%L) %-5p - %m%nlog4j.category.com.cheguo=infolog4j.category.org.springframework=infolog4j.category.freemarker=info statics.properties 1234567dubbo.gege.address=$&#123;dubbo.gege.address&#125;dubbo.gege.port=$&#123;dubbo.gege.port&#125;dubbo.gege.version=$&#123;dubbo.gege.version&#125;dubbo.gege.group=$&#123;dubbo.gege.group&#125;dubbo.gege.check=falsedubbo.gege.retries=0dubbo.gege.timeout=10000 web.xml 123456789101112&lt;!DOCTYPE web-app PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN" "http://java.sun.com/dtd/web-app_2_3.dtd" &gt;&lt;web-app&gt; &lt;display-name&gt;provider&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;dubboService&lt;/servlet-name&gt; &lt;servlet-class&gt;com.gege.tool.DubboInit&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt;&lt;/web-app&gt; 5、Dubbo服务消费者工程创建与配置 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;consumer&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spring.version&gt;3.2.8.RELEASE&lt;/spring.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;provider&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.gege&lt;/groupId&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.14&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;taglibs&lt;/groupId&gt; &lt;artifactId&gt;standard&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;project.basedir&#125;/src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;*.*&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;local&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;product&lt;/id&gt; &lt;properties&gt; &lt;dubbo.gege.address&gt;zookeeper://127.0.0.1:2181&lt;/dubbo.gege.address&gt; &lt;dubbo.gege.group&gt;$&#123;user.name&#125;&lt;/dubbo.gege.group&gt; &lt;dubbo.gege.version&gt;1.0.0&lt;/dubbo.gege.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; applicationContext.xml 12345678910111213141516171819202122232425262728293031323334&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.0.xsd"&gt; &lt;task:annotation-driven /&gt; &lt;context:component-scan base-package="com.gege"/&gt; &lt;context:annotation-config /&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath*:statics.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;import resource="dubbo-customer.xml"/&gt;&lt;/beans&gt; dubbo-customer.xml 123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;dubbo:application name="consumer"&gt;&lt;/dubbo:application&gt; &lt;!-- 使用zookeeper注册中心暴露服务地址 --&gt; &lt;dubbo:registry id="gege_center" protocol="dubbo" address="$&#123;dubbo.gege.address&#125;" /&gt; &lt;dubbo:provider group="$&#123;dubbo.gege.group&#125;" version="$&#123;dubbo.gege.version&#125;" registry="gege_center"/&gt; &lt;dubbo:consumer group="$&#123;dubbo.gege.group&#125;" version="$&#123;dubbo.gege.version&#125;" registry="gege_center" check="false" /&gt; &lt;!-- 声明需要引用的服务接口 --&gt; &lt;dubbo:reference id="testService" interface="com.gege.service.ITestService" retries="$&#123;dubbo.gege.retries&#125;" timeout="$&#123;dubbo.gege.timeout&#125;"/&gt;&lt;/beans&gt; log4j.properties 同上 statics.properties 12345dubbo.gege.address=$&#123;dubbo.gege.address&#125;dubbo.gege.group=$&#123;dubbo.gege.group&#125;dubbo.gege.version=$&#123;dubbo.gege.version&#125;dubbo.gege.retries=0dubbo.gege.timeout=10000 ConsumerServiceTest.java 1234567891011121314151617181920212223package com.gege.service;import java.io.IOException;import org.apache.log4j.Logger;import org.springframework.context.support.ClassPathXmlApplicationContext;public class ConsumerServiceTest &#123; Logger logger=Logger.getLogger(ConsumerServiceTest.class); public static void main(String[] args) &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( new String[] &#123; "applicationContext.xml" &#125;); context.start(); ITestService testService = (ITestService) context.getBean("testService"); System.out.println(testService.getName()); try &#123; System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 6、运行测试 运行zookeeper，双击zkServer.cmd 运行dubbo-admin，双击Tomcat7w.exe，点击start 在eclipse中用tomcat运行服务提供者，可能存在超时，配置tomcat的timeout配置就可以了 在eclipse中用tomcat运行服务消费者 消费者工程的ConsumerServiceTest下，右键 就可以查看是否成功 正常结果： gege 更多内容可以关注微信公众号，或者访问AppZone网站 以上です(Ending) ありがどう(Thank You)]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringAOP面向切面详解（带实例）]]></title>
    <url>%2F2016%2F05%2F13%2Faop20160513%2F</url>
    <content type="text"><![CDATA[了解AOP的相关术语1.通知(Advice): 通知定义了切面是什么，以及何时使用。描述了切面要完成的工作和何时需要执行这个工作。 2.连接点(Joinpoint): 程序能够应用通知的一个“时机”，这些“时机”就是连接点，例如方法被调用时、异常被抛出时等等。 3.切入点(Pointcut): 通知定义了切面要发生的“故事”和时间，那么切入点就定义了“故事”发生的地点，例如某个类或方法的名称，Spring中允许我们方便的用正则表达式来指定（切面在哪个方法的前或后做出的那个方法点） 4.切面(Aspect): 通知和切入点共同组成了切面：时间、地点和要发生的“故事”，事务管理是J2EE应用中一个很好的横切关注点例子，切面用Spring的Advisor或拦截器实现 5.引入(Introduction): 引入允许我们向现有的类添加新的方法和属性(Spring提供了一个方法注入的功能） 6.目标(Target): 即被通知的对象，如果没有AOP,那么它的逻辑将要交叉别的事务逻辑，有了AOP之后它可以只关注自己要做的事（AOP让他做爱做的事） 7.代理(proxy): 应用通知的对象，详细内容参见设计模式里面的代理模式 8.织入(Weaving): 把切面应用到目标对象来创建新的代理对象的过程，织入一般发生在如下几个时机: (1)编译时：当一个类文件被编译时进行织入，这需要特殊的编译器才可以做的到，例如AspectJ的织入编译器 (2)类加载时：使用特殊的ClassLoader在目标类被加载到程序之前增强类的字节代码 (3)运行时：切面在运行的某个时刻被织入,SpringAOP就是以这种方式织入切面的，原理应该是使用了JDK的动态代理技术 存在的实现方式 1.经典的基于代理的AOP2.@AspectJ注解驱动的切面3.纯POJO切面4.注入式AspectJ切面 实现步骤 1.创建通知：实现这几个接口，把其中的方法实现了2.定义切点和通知者：在Spring配制文件中配置这些信息3.使用ProxyFactoryBean来生成代理 范例例子我是基于maven和Spring注解的方式，用POJO实现 工程的主要文件的配置 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;imooc&lt;/groupId&gt; &lt;artifactId&gt;springmaven&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springmaven Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;finalName&gt;SSHMJ-FRANK&lt;/finalName&gt; &lt;/properties&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;mav&lt;/id&gt; &lt;name&gt;sss&lt;/name&gt; &lt;url&gt;http://mvnrepository.com&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-asm&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-expression&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-instrument&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-instrument-tomcat&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-pool&lt;/groupId&gt; &lt;artifactId&gt;commons-pool&lt;/artifactId&gt; &lt;version&gt;1.5.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;/groupId&gt; &lt;artifactId&gt;commons-collections&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-oxm&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc-portlet&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-struts&lt;/artifactId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-httpclient&lt;/groupId&gt; &lt;artifactId&gt;commons-httpclient&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ognl&lt;/groupId&gt; &lt;artifactId&gt;ognl&lt;/artifactId&gt; &lt;version&gt;2.6.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-site-plugin&lt;/artifactId&gt; &lt;version&gt;3.4&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;finalName&gt;springmaven&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; 1234567891011121314151617181920212223242526272829```xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; version=&quot;3.0&quot;&gt;&lt;!-- 工程名 --&gt; &lt;display-name&gt;springmaven&lt;/display-name&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- spring配置文件的配置 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt;&lt;!-- springmvc的配置环境 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;url-pattern&gt;*.html&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; applicationContext.xml 123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.0.xsd"&gt; &lt;context:component-scan base-package="com.service" /&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;aop:config proxy-target-class="true"&gt; &lt;aop:aspect ref="preDo"&gt; &lt;aop:pointcut expression="execution(* com.service.ToDo.toEat(..))" id="register" /&gt; &lt;aop:before method="toPre" pointcut-ref="register" /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; spring-servlet.xml 1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.0.xsd"&gt; &lt;!-- don't handle the static resource --&gt; &lt;mvc:default-servlet-handler /&gt; &lt;!-- if you use annotation you must configure following setting --&gt; &lt;mvc:annotation-driven /&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver" id="internalResourceViewResolver"&gt; &lt;property name="viewClass" value="org.springframework.web.servlet.view.JstlView"&gt;&lt;/property&gt;&lt;!-- 前缀 --&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt;&lt;!-- 后缀 --&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt;&lt;/beans&gt; IToDo.java 切入点(Pointcut)接口 123456package com.service.imp;public interface IToDo &#123; public String toEat();&#125; ToDo.java 123456789101112131415package com.service;import org.springframework.stereotype.Service;import com.service.imp.IToDo;@Servicepublic class ToDo implements IToDo &#123; @Override public String toEat() &#123; System.out.println("吃苹果"); return "吃苹果"; &#125;&#125; IPreDo.java 123456package com.service.imp;public interface IPreDo &#123; public String toPre();&#125; application.java 用于测试 1234567891011121314package springmaven;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.service.imp.IToDo;public class application &#123; public static void main(String[] args) &#123; ApplicationContext appCtx = new ClassPathXmlApplicationContext("applicationContext.xml"); IToDo tdo = (IToDo)appCtx.getBean("toDo"); tdo.toEat(); &#125;&#125; 工程图片 返回的结果 12洗手吃苹果 主要的配置讲解12345678&lt;aop:aspectj-autoproxy /&gt;&lt;aop:config proxy-target-class="true"&gt; &lt;aop:aspect ref="preDo"&gt; &lt;aop:pointcut expression="execution(* com.service.ToDo.toEat(..))" id="register" /&gt; &lt;aop:before method="toPre" pointcut-ref="register" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; &lt;aop:aspectj-autoproxy /&gt;:会自动为spring容器中那些配置@aspectJ切面的bean创建代理，织入切面，我这里没有使用注解的方式，使用了xml配置的方式。 &lt;aop:config&gt;:就是用来配置aspectJ切面 proxy-target-class:设置代理模式。当poxy-target-class=&quot;true&quot;时，表示使用CGLib动态代理技术织入增强。设置为false时，表示使用jdk动态代理织入增强，如果目标类没有声明接口，则spring将自动使用CGLib动态代理。 &lt;aop:aspect ref=&quot;preDo&quot;&gt;:设置切面，ref是切面Bean的id名 &lt;aop:pointcut expression=&quot;execution(* com.service.ToDo.toEat(..))&quot; id=&quot;register&quot; /&gt;:这里设置切入点，expression设置切面植入的切入点的方法地址 &lt;aop:before method=&quot;toPre&quot; pointcut-ref=&quot;register&quot; &gt;:在执行切入点方法之前执行切面方法，method为切面中的执行方法，pointcut-ref与切点的id一致就可以了 &lt;aop:advisor&gt; 定义一个AOP通知者&lt;aop:after&gt; 后通知&lt;aop:after-returning&gt; 返回后通知&lt;aop:after-throwing&gt; 抛出后通知&lt;aop:around&gt; 周围通知&lt;aop:aspect&gt;定义一个切面&lt;aop:before&gt;前通知&lt;aop:config&gt;顶级配置元素，类似于这种东西&lt;aop:pointcut&gt;定义一个切点 参考资料：Spring实现AOP的4种方式 proxy-target-class]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo让博客梦变的简单]]></title>
    <url>%2F2016%2F05%2F12%2Fhexo20160512%2F</url>
    <content type="text"><![CDATA[之前写过一篇关于octopress创建个人技术博客的文章，很多朋友说步骤好复杂啊（心塞）！这次，为了满足这群朋友，这里我要教大家的是用Hexo建个人技术博客。 1、安装GIT和Node.JS这一步我不想展开太多，因为真的很简单，关于GIT的配置我在之前的文章中已经提到过了，看官可以查看历史文章 2、安装Hexo在桌面下使用GIT Bash输入下面代码，实现Hexo的安装 1$ sudo npm install hexo-cli -g 初始化自己的个人博客 使用GIT Bash切换到你希望安装个人博客的文件夹下 1$ hexo init username.github.io username就是Github中的用户名，详细查看 3、主题的安装在完成上一步后，其实就可以运行查看页面了，这一步让你实现安装自己喜欢的主题，看官可以在Github网站中输入hexo关键字，可以搜索到很多相关主题，挑选一个就可以了 我这里使用NEXT这个主题进行讲解，其他的方式也是一样的。 使用GIT Bash切换到你刚才个人博客安装目录 12$ cd username.github.io$ git clone https://github.com/iissnan/hexo-theme-next themes/next 完成上面步骤，会在theme的目录下多了一个next文件，说明主题下载成功了，但现在还没有配置完成 在username.github.io文件下有一个_config.yml文件，编辑它 将theme的value设为自己的主题名，这里是next——完成主题的配置 将deploy的value进行配置——完成主题发布地址配置 1234567891011# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:username/username.github.io.git branch: master 4、创建文章在username.github.io/source/_posts目录下创建你自己的文章，文章格式为Markdown的.md格式 1234---title: Hexo让博客梦变的简单date: 2016-05-12--- title:文章名，date:发布的日期 5、发布到Github安装自动部署发布工具 使用GIT Bash切换到username.github.io目录下 1$ npm install hexo-deployer-git --save 编译 1$ hexo generate 本地测试 1$ hexo s -p 4000 -p:设置端口号，默认为4000，使用默认端口可以省略 发布到Github 1$ hexo d 这里特别要提醒一下不要忘记在username.github.io文件下_config.yml文件，配置deploy 6、备份我们常常可能会遇到系统重装或者换电脑编辑的场景，备份变得尤为重要，这里介绍一个hexo-git-backup备份工具 大家可以根据自己的实际情况，参照README.md说明进行配置 7、获取备份，实现还原 操作步骤1，2 将Github的备份clone到本地 12$ git@github.com:username/username.github.io.git$ git checkout branchname branchname:备份分支名 8、帮助文档hexo配置next配置 以上です(Ending) ありがどう(Thank You)]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven的pom文件那些事]]></title>
    <url>%2F2016%2F05%2F05%2Fpom20160505%2F</url>
    <content type="text"><![CDATA[pom.xml文件1234&lt;groupId&gt;org.son.nexus&lt;/groupId&gt;//项目隶属的实际项目名&lt;artifactId&gt;nexus-indexer&lt;/artifactId&gt;//实际项目中的一个maven项目模块名称&lt;version&gt;2.0.0&lt;/version&gt;//版本号&lt;packaging&gt;jar&lt;/packaging&gt;//当前maven项目打包的方式，默认为jar 以上的配置会获得一个-.格式的包//nexus-indexer-2.0.0.jar jdk7//用于定义构建输出的一些附属的构建。classifier是不能直接定义的，只能存在附加的构件（插件）时，才能（必须）添加。在包名上也会有所提现（nexus-indexer-2.0.0-javadoc.jar） jar包在仓库中的路径规律：groupId/artifactId/version/groupId-version 依赖仓库groupId、artifactId、version为基本坐标 type为依赖类型，默认为jar scope为依赖范围 compile：编译，测试，运行三种classpath都有效 test：只对测试有效 provided：编译和测试有效 runtime：测试和运行有效 system：系统依赖，依赖的包是从计算机本地导入，与systemPath配合使用 ​ &lt;systemPath&gt;${java.home}/lib/rt.jar&lt;/systemPath&gt; ​ &lt;scope&gt;system&lt;/scope&gt; 传递性依赖maven默认就是支持的。 什么是传递性依赖？ 比如：现在要导入A.jar包，但A.jar有依赖于B.jar，在maven中就会默认导入B.jar，而不需要手动去导入。 scope最好设为compile 传递性依赖存在一个问题：工程中存在两个jar分别依赖的是B.jar的不同版本jar包，就会出现错误。 optional：可选依赖，true 不会传递，false会传递（默认） 123456&lt;dependency&gt;&lt;groupId&gt;mysql&lt;/groupId&gt;&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;version&gt;5.1.10&lt;/version&gt;&lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 当其他项目C依赖B项目时，mysql-connector-java的依赖不会发生传递给C 优点： 减少冲突，不用的功能不传递 一个jar应该只有 一个职责原则 exclusions排除依赖用于替换某个 依赖中的依赖包作用 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-B&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt;//可以有多个 &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-C&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-B&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 该例子的意思：不想使用project-B中版本的project-C依赖包，而将其环卫1.1.0的project-C包 归类依赖场景：存在一群版本号一致的依赖，想通过一个统一的配置，将其统一配置和修改 123&lt;properties&gt; &lt;spring&gt;2.5.6&lt;/spring&gt;&lt;/properties&gt; 调用方式 1&lt;version&gt;$&#123;spring&#125;&lt;/version&gt; 部署至远程仓库修改工程中的pom文件 12345678910111213141516&lt;project&gt; .... &lt;distributionManagement&gt; &lt;repository&gt; //发布版本构件的仓库 &lt;id&gt;&lt;/id&gt; //远程仓库的唯一标识 &lt;name&gt;&lt;/name&gt; //自定义 &lt;url&gt;&lt;/url&gt; //该仓库地址 &lt;/repository&gt; &lt;snapshotRepository&gt;//快照版本的设置 &lt;id&gt;&lt;/id&gt; &lt;name&gt;&lt;/name&gt; &lt;url&gt;&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; ....&lt;/project&gt; 如果需要认证，则在setting.xml文件中增加一个元素进行配置 在配置完成后，执行mvn clean deploy 就会根据当前版本类型，发布到相应的仓库中 快照版本 快照不是正式版本，当设为带有SNAPSHOT的version时，即为快照版本，每次向仓库部署是，会自动打上时间戳，其他引用快照相同版本时会自动下载更新 例如：2.1-SNAPSHOT时，则会下载2.1-20091214之类的构件 在maven本地仓库的groupId/artifactId/version文件路径下会存在maven-metadata.xml文件，会记录版本信息 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;metadata&gt; &lt;groupId&gt;com.android.support&lt;/groupId&gt; &lt;artifactId&gt;multidex&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;versioning&gt; &lt;versions&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;version&gt;1.0.1&lt;/version&gt; &lt;/versions&gt; &lt;lastUpdated&gt;20141209014044&lt;/lastUpdated&gt; &lt;/versioning&gt;&lt;/metadata&gt; maven生命周期maven的生命周期是抽象的，其实际行为都是由插件来完成的。 maven的生命周期是不做任何实际工作，实际任务都由插件完成。 maven具有三个独立的生命周期：clean、defeat、site 1、clean生命周期：清理项目，包含三个phase。 1）pre-clean：执行清理前需要完成的工作 2）clean：清理上一次构建生成的文件 3）post-clean：执行清理后需要完成的工作 2、default生命周期：构建项目，重要的phase如下。 1）validate：验证工程是否正确，所有需要的资源是否可用。2）compile：编译项目的源代码。3）test：使用合适的单元测试框架来测试已编译的源代码。这些测试不需要已打包和布署。4）Package：把已编译的代码打包成可发布的格式，比如jar。5）integration-test：如有需要，将包处理和发布到一个能够进行集成测试的环境。6）verify：运行所有检查，验证包是否有效且达到质量标准。7）install：把包安装到maven本地仓库，可以被其他工程作为依赖来使用。8）Deploy：在集成或者发布环境下执行，将最终版本的包拷贝到远程的repository，使得其他的开发者或者工程可以共享。 3、site生命周期：建立和发布项目站点，phase如下 1）pre-site：生成项目站点之前需要完成的工作 2）site：生成项目站点文档 3）post-site：生成项目站点之后需要完成的工作 4）site-deploy：将项目站点发布到服务器 命令行输入：$mvn clean 其实调用的是clean生命周期的clean阶段，执行了pre-clean和clean 而这些命令其实是由插件提供功能的。 自定义绑定插件123456789101112131415161718&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;//maven官方，官方的可以省略该标签 &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;//插件 &lt;version&gt;2.1.1&lt;/version&gt;//没有version时，会默认下载最新的release版本 &lt;executions&gt;//配置执行n个任务 &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt;//可以任意 &lt;phase&gt;verify&lt;/phase&gt;//绑定到verify生命周期，在此时才会起作用 &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt;//启用该插件的jar-no-fork功能 &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 因为一个插件可能存在多个功能，但我们并不一定所有的功能都需要，所以设定goal标签，表示我们要实现的功能。 插件配置命令行配置：$mvn install -Dt 插件相关参数 //适用于当每次运行时，这个插件的配置都会变的情况 pom中全局配置： 123456789101112&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; //jdk1.7 &lt;target&gt;1.7&lt;/target&gt; //编译后jdk1.7 &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 聚合（多模块）意义：一次构建所有想要构建的项目 1234567891011&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.son.nexus&lt;/groupId&gt; &lt;artifactId&gt;nexus-indexer&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt;//本身也是一个maven工程 &lt;modules&gt; &lt;module&gt;account-email&lt;/module&gt;//想要构建的项目，这里写的是当前pom文件下的相对路径地址 &lt;module&gt;account-persilist&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 聚合pom文件的packaging标签一定要是pom，其工程就只是一个pom文件，没有其他的实现代码 一般来说模块处的目录名应与其artifactId一致 聚合模块与其他模块的目录结构并非一定要父子关系 继承父pom 12345678910111213141516171819202122232425&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.son.nexus&lt;/groupId&gt; &lt;artifactId&gt;nexus-indexer&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt;//本身也是一个maven工程 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-B&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-C&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.juv&lt;/groupId&gt; &lt;artifactId&gt;project-B&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 父pom的packaging也是pom 子pom 12345678910111213&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.son.nexus&lt;/groupId&gt; &lt;artifactId&gt;nexus-B&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;parent&gt; &lt;groupId&gt;org.son.nexus&lt;/groupId&gt; &lt;artifactId&gt;nexus-C&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt;//相对路径 &lt;/parent&gt;&lt;/project&gt; 子pom的packaging则不一定要是pom，但一定有parent标签 子类的groupId和version也可以继承与父pom文件 标签作用：当子类不需要父pom中的某些依赖的时，就可以使用。 父pom 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 被标签所包裹的依赖是不会主动被加载进入子pom中，只有子pom中显式再次声明的时候才会被依赖 子pom 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 可以省略version等其他配置，因为父pom中已经配置过了 若想获取父pom中所有的dependencyManagement中的构件配置，则在子pom中如下配置 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;artifactId&gt;account-parent&lt;/artifactId&gt; &lt;groupId&gt;com.juvenxu.accout&lt;/groupId&gt; &lt;version&gt;3.1.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 配置父pom的一些信息 插件管理父pom中的插件不想在子类中默认使用 跟依赖管理一样，被包裹的插件配置不会被子pom主动获取，只有当子pom中声明了该插件的groupId和artifactId后，才会被继承 一个pom文件既可以是聚合也可以是父pom 反应堆在聚合构建时，构件模块的先后顺序的排列。 按书写的先后顺序进行构建 加料区如果同一个项目中存在多个模块相互依赖时候，version和groupId可以使用\${project.groupId}（当前模块的groupId）和${project.verison}，这样就不用不断的更改了。 标签用来确定最终包名 以上です(Ending) ありがどう(Thank You)]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超越sublime的Visual Studio Code]]></title>
    <url>%2F2016%2F04%2F20%2Ffist%2F</url>
    <content type="text"><![CDATA[在 Build 2015 大会上，微软除了发布了 新的 Windows 10 系统外，最大的惊喜莫过于宣布推出免费跨平台的 Visual Studio Code 编辑器了！经过一年的更新换代，现在迎来1.0正式版。 其功能非常强大，甚至可以说比Sublime还要强大。功能列表： 语法高亮 代码补全 括号自动匹配 代码对比 Diff(我的最爱) GIT 支持插件安装 自定义热键 代码片段 代码调试 资源管理 支持单屏同时显示多个页面（CTRL+C利器） 支持图片的浏览 而且支持中文（良心啊） 支持主题更换 代码Fomat 以上的功能，作为程序员的你肯定就会有冲动去下载一个试试了！ 如果你是Linux的用户，别担心，它支持三大系统。 Visual Studio Code在使用过后，完全觉得的就是一个IDE，真的它强大，而且软件是秒开的。 它值得你拥有！ 以上です(Ending) ありがどう(Thank You)]]></content>
      <tags>
        <tag>it</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何建高逼格个人网站]]></title>
    <url>%2F2016%2F04%2F20%2Foctopress%2F</url>
    <content type="text"><![CDATA[从大学开始我就希望能有一个自己的个人网站，觉得那样真的很酷，就自学了HTML和Java编程，从此踏上了码农搬砖的不归路。。。。现如今，建一个网站的成本真的是太低了，特别是有了GitHub以后，不懂代码的孩子都可以自己建站了。好，废话就放到最后说。 一、Git安装配置1、安装Git软件 2、在Github上注册一个帐号 地址：Github 3、在桌面打开Git，进行设置 12git config --global user.name "username"//github的帐号名git config --global user.email "username@163.com"//github的邮箱地址 4、生成ssh密钥 1ssh-keygen -t rsa -C "username@163.com"//github邮箱地址 接着会提醒你输入名字和密码，可以为空会在C盘的该电脑用户下面生成一个.ssh文件，其中的id_rsa和id_rsa.pub，id_rsa要好好保存，id_rsa.pub用来在github网站做配置用 5、配置github 使用Add SSH key，tittle可以随意输，将id_rsa.pub中的密钥保存到这里设置完成后，可以在本地输入 1ssh -T git@github.com 其会将github中的公钥与本地的私钥进行匹配，成功则会返回成功信息 二、安装Ruby1、在安装Ruby时一定要勾选Add RubyExcutables to your Path，否则自己要配置环境变量2、查看是否安装成功 1ruby -v 三、安装devkit四、将Ruby与devkit关联起来1、在devkit安装目录下 1ruby dk.rb init 会生成一个config.yml成功的情况下在该文件下面会有一行是关于ruby的安装路径的数据如果失败也没事，只要通过手动输入就行了 2、在devkit安装目录下 1ruby dk.rb install 五、安装配置octopress1、克隆octopress，切换到自己要安装的文件下 1git clone git://github.com/imathis/octopress.git octopress 2、在octopress的文件根目录下 1gen sources -a http://gems.ruby-china.org/ 一个国内的软件源移除自带的软件源，因为在国内会被墙 1gem sources -r http://rubygems.org 查看软件源 1gem sources -l 3、修改octopress文件下面的Gemfile文件中的source地址，也改为http://gems.ruby-china.org/如果上面的命令存在执行错误，可以使用windows自带的CMD命令行去执行 4、在octopress下 1gem install bundle 过程有点长，会有successful接着执行 1bundle install 过程有点长最后 1rake install 会生成source和public文件，source是源代码的文件，而public是生成的文件 5、编译octopress 在octopress文件下 1rake generator 该指令会编译修改的内容，生成好的文件会在public文件下 6、运行 在octopress文件下 1rake preview 会使用4000端口，开一个服务使用localhost:4000访问 六、部署到Github网站上去1、在Github中创建一个username.github.io的仓库，username为用户自己的Github帐号名 2、在octopress文件下 1rake setup_github_pages 会提示你输入github中的git地址 会创建_deploy文件，并且跟Github绑定好了如果上面的指令不起作用，可以使用下面的方式 1rake setup_github_pages[github中username.github.io仓库的ssh地址] 3、使用rake generator 4、使用rake deploy，将本地编译好的文件上传到Github中 七、将source目录代码上传到Github分支123git add .git commit -m 'your message'//注释git push origin source 相关软件下载地址：软件 学习网站：jekyll 以上です(Ending) ありがどう(Thank You)]]></content>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
</search>
